
<html>
<head>
<link href="../lg.css" rel="stylesheet" type="text/css" media="screen, projection"  />
<link rel="shortcut icon" href="../favicon.ico" />
<title>Screen scraping with Perl LG #108</title>

<style type="text/css" media="screen, projection">
<!--

-->
</style>

<link rel="alternate" type="application/rss+xml" title="LG RSS" href="lg.rss" />
<link rel="alternate" type="application/rdf+xml" title="LG RDF" href="lg.rdf" />
<link rel="alternate" type="application/atom+xml" title="LG Atom" href="lg.atom.xml" />

</head>

<body>


<img src="../gx/2003/newlogo-blank-200-gold2.jpg" id="logo" alt="Linux Gazette"/>
<p id="fun">...making Linux just a little more fun!</p>


<div class="content articlecontent">

<div id="previousnexttop">
<A HREF="oregan.html" >&lt;-- prev</A> | <A HREF="park.html" >next --&gt;</A>
</div>



<h1>Screen scraping with Perl</h1>
<p id="by"><b>By <A HREF="../authors/oregan.html">Jimmy O'Regan</A></b></p>

<p>
<p>
Screen scraping is a relatively well-known idea, but for those who are not
familiar with it, the term refers to the process of extracting data from a 
website. This may involve sending form information, navigating through the
site, etc., but the part I'm most interested in is processing the HTML to
extract the information I'm looking for. 

<p>
As I mentioned in my <a href="oregan.html">article about outliners</a>, I've
been organising myself recently, and as part of that process of organisation
I've been writing several screen scrapers to reduce the amount of browsing
I do: repeatedly visiting news sites to see if they have been updated is a 
waste of anyone's time, and in these times of <a 
href="../105/oregan2.html">feed readers</a>, it's even less tolerable. 

<p>
<a href="http://liferea.sourceforge.net">Liferea</a>, my feed reader of 
choice, has a facility to read a feed generated by a command, and I have been
taking advantage of this facility. As well as reducing the amount of time
I spend reading the news from various sources, this also allows me to keep
track of websites I wouldn't normally remember to read.

<h3>Perl</h3>

<p>
In my article about feed readers I mentioned <a 
href="http://rssscraper.rubyforge.org/">RSSscraper</a>, a Ruby-based 
framework for writing screen scrapers. As much as I like RSSscraper, I've
been writing my screen scrapers in Perl. Ruby looks like a nice language,
but I find Perl's regexes easier to use, and <a 
href="http://www.cpan.org">CPAN</a> is filled with convenient modules to do
just about everything you can think of (and many more things you'd probably
never think of).

<p>
Most of my screen scrapers use regexes, mainly because Perl's regexes were
haunting me: there was a something I just wasn't grasping, and I wanted to
push past it (and I have, and now I can't remember what the block was :).
There are much better ways to write screen scrapers: Perl has modules like
WWW::Mechanize, HTML::TokeParser, etc., that make screen scraping easier.

<h3>Using regexes</h3>

<p>
First of all, here's a list of scrapers:

<ul>
<li><a href="misc/oregan/bill-bailey.pl.txt">bill-bailey.pl.txt</a>:
<a href="http://www.bill-bailey.co.uk/">Bill Bailey</a> is a British comedian, 
whose <a href="http://www.bill-bailey.co.uk/blog/index.php">blog</a> lacks
any sort of feed.</li>
<li><a href="misc/oregan/mmoore.pl.txt">mmoore.pl.txt</a>: <a 
href="http://www.michaelmoore.com/">Michael Moore</a> doesn't really need an 
introduction. His <a 
href="http://www.michaelmoore.com/words/diary/index.php">blog</a> is also
feedless.</li>
<li><a href="misc/oregan/sun-bizarre.pl.txt">sun-bizarre.pl.txt</a>,
<a href="misc/oregan/sun-viral.pl.txt">sun-viral.pl.txt</a>:
I read <a href="http://www.thesun.co.uk">The Sun</a>. There, I admitted it. I 
work in a factory, and it's good to keep up with the news that everyone else 
reads, but mostly it's because I like looking at pictures of scantily clad 
women. [shrug]. I also have <a 
href="misc/oregan/sun-pic.pl.txt">sun-pic.pl.txt</a>, which allows me to
bypass The Sun's annoying popups.</li>
<li><a href="misc/oregan/telsa.pl.txt">telsa.pl.txt</a>: <a 
href="http://www.linux.org.uk/~telsa/">Telsa</a>, wife of Alan Cox, keeps one
of the most interesting <a 
href="http://www.linux.org.uk/~telsa/Diary/diary.html">diaries</a> on the 
'net</li>
<li><a href="misc/oregan/tp.pl.txt">tp.pl.txt</a>: Grabs a list of Terry
Pratchett's Usenet posts from <a href="http://groups.google.com">Google
Groups</a> (Google does provide an API, but Google Groups isn't currently
available).</li>
<li><a href="misc/oregan/uf.pl.txt">uf.pl.txt</a>: Grabs the latest 
<a href="http://www.userfriendly.org">Userfriendly</a> comic strip.</li>
</ul>

<p>
Most of the scrapers work in exactly the same way: fetch the page using 
LWP::Simple, split the page into sections, and extract the blog entry from 
each section. <tt>sun-pic.pl</tt> is a dirty hackish attempt to bypass
popups, and The Sun's horrible site's tendency to crash Mozilla. It's
called with the address of the page, grabs the images from the popups, 
and puts them in a specific directory. It's not meant to be useful to anyone 
else, other than as an example of a quick and dirty script that's different
from the other examples here. If you're interested, read the comments in
the script.

<p>
I'll use Telsa's diary as an example, because the page layout is clear, 
and the script I wrote is one of the better examples (I'd learned to use
the <code>/x</code> modifier for clarity in regexes by then).

<pre>
   &lt;dt&gt;&lt;a name="2004-10-25"&gt;&lt;strong&gt;October 25th&lt;/strong&gt;&lt;/a&gt;&lt;/dt&gt;
   &lt;dd&gt;
    &lt;p&gt;
      <em>[Content]</em>
    &lt;/p&gt;
   &lt;/dd&gt;
</pre>

<p>... and so on. 

<p> Each entry starts with <code>&lt;dt&gt;</code>, so I use that as the 
point at which to split. From each entry, I want to grab the anchor name,
the title (between the <code>&lt;strong&gt;</code> tags), and everything
that follows, until the <code>&lt;/dd&gt;</code> tag.

<p> The script looks like this:

<pre class="code">
#!/usr/bin/perl -w

use strict;
use XML::RSS;
use LWP::Simple;
use HTML::Entities;

my $rss = new XML::RSS (version =&gt; '1.0');
my $url = &quot;http://www.linux.org.uk/~telsa/Diary/diary.html&quot;;
my $page = get($url);

$rss-&gt;channel(title       =&gt; &quot;The more accurate diary. Really.&quot;,
              link        =&gt; $url,
              description =&gt; &quot;Telsa's diary of life with a hacker:&quot; 
	      		     . &quot; the current ramblings&quot;);

foreach (split ('&lt;dt&gt;', $page))
{
	if (/&lt;a\sname=&quot;
             ([^&quot;]*)     # Anchor name
             &quot;&gt;
             &lt;strong&gt;
             ([^&gt;]*)     # Post title
             &lt;\/strong&gt;&lt;\/a&gt;&lt;\/dt&gt;\s*&lt;dd&gt;
             (.*)        # Body of post
             &lt;\/dd&gt;/six)
	{
		$rss-&gt;add_item(title       =&gt; $2,
			       link        =&gt; &quot;$url#$1&quot;,
		       	       description =&gt; encode_entities($3));
	}
}

print $rss-&gt;as_string;
</pre>

<p>
Most of the scrapers follow this general recipe, but the Michael Moore 
and Terry Pratchett scrapers have two important differences.

<p> Michael Moore's blog, unlike most blogs, has the links for each item on a
separate part of the page from the content that's being scraped, so I have
a function to scrape the content again for the link: 

<pre class="code">
sub findurl ($$)
{
	my $title = shift;
	my $pagein = shift;
	if ($pagein =~ /&lt;a href=&quot;(index.php\?id=[^&quot;]*)&quot;&gt;$title&lt;\/a&gt;/i)
	{
		return &quot;http://www.michaelmoore.com/words/diary/$1&quot;;
	}
}
</pre>

<p> It's important to have a unique URL for each item in a feed, because most 
feed readers use the link as a key, and will only display one entry for each
link. 

<p> The Terry Pratchett scraper is also different, in that instead of using
LWP::Simple, it uses LWP::Agent. Google wouldn't accept a request from my
script, so I used LWP::Agent to masquerade as a browser:

<pre class="code">
my $ie=&quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)&quot;;

my $ua = LWP::UserAgent-&gt;new;
$ua-&gt;agent($ie);
my $url = &quot;http://groups.google.com/groups?safe=images&amp;as_uauthors=Terry%20Pratchett&amp;lr=lang_en&amp;hl=en&quot;;
my $response = $ua-&gt;get ($url);

if ($response-&gt;is_success) 
{
	<em>[scrape as usual]</em>
}
else 
{
	die $response-&gt;status_line;
}
</pre>

<p> (The content of the page is held in <code>$response-&gt;content</code>).

<p> One thing I'm still looking at is getting the news from <a 
href="http://www.thesun.co.uk/section/0,,2,00.html">The Sun</a>. The problem
with this page is that it has some of the worst abuses of HTML I've ever seen.
This snippet uses HTML::TableExtract to extract most of the headlines. 

<pre class="code">
use LWP::Simple;
use HTML::TableExtract;

my $html_string = get (&quot;http://www.thesun.co.uk/section/0,,2,00.html&quot;);
my $te = new HTML::TableExtract(depth =&gt; 6);
$te-&gt;parse($html_string);
foreach $ts ($te-&gt;table_states) 
{
	print &quot;Table found at &quot;, join(',', $ts-&gt;coords), &quot;:\n&quot;;
	foreach $row ($ts-&gt;rows) 
	{
		print join(',', @$row), &quot;\n&quot;;
	}
}
</pre>

<p> HTML::TableExtract is a nice module that lets you extract the text 
content of any table. The "depth" option allows you to select a depth of 
tables within other tables (the page grabbed by this script has most of its 
headlines at a depth of 6 tables within tables, but there are others at a 
depth of 7 -- I think I'll come back to that one). You can also specify a
"count" option to tell it which table to extract from, or a "headers" option,
which makes the module look for columns with those headers.

<p> Lastly, I'd like to take a look at HTML::TokeParser::Simple. If I had 
known about this module when I started writing screen scrapers, they would
be a lot easier to understand, and more resiliant to change. The scraper for
Telsa's diary, for example, will break if the <code>&lt;a&gt;</code> tag
has a <code>href</code> attribute as well as a <code>name</code> attribute.

<p> HTML::TokeParser::Simple is, as the name implies, a simplified version
of HTML::TokeParser, which allows you to look for certain tags within a
file. HTML::TokeParser::Simple gives a number of methods with a prefix of
either "is_" or "return_" that tell you if a tag is a certain type or returns
it, respectively. HTML::TokeParser::Simple also inherits from 
HTML::TokeParser, so it has full access to HTML::TokeParser's methods.

<p> The Telsa scraper using HTML::TokeParser::Simple looks like this (<a 
href="misc/oregan/telsa-2.pl.txt">text version</a>):

<pre class="code">
#!/usr/bin/perl -w

use strict;
use XML::RSS;
use LWP::Simple;
use HTML::Entities;
use HTML::TokeParser::Simple;

my $rss = new XML::RSS (version =&gt; '1.0');
my $url = &quot;http://www.linux.org.uk/~telsa/Diary/diary.html&quot;;
my $page = get($url);
my $stream = HTML::TokeParser::Simple-&gt;new(\$page);
my $tag;

$rss-&gt;channel(title       =&gt; &quot;The more accurate diary. Really.&quot;,
              link        =&gt; $url,
              description =&gt; &quot;Telsa's diary of life with a hacker:&quot; 
	      		     . &quot; the current ramblings&quot;);

while ($tag = $stream-&gt;get_token)
{
	next unless $tag-&gt;is_start_tag ('a');
	next unless $tag-&gt;return_attr(&quot;name&quot;) ne &quot;&quot;;
	my $link = $tag-&gt;return_attr(&quot;name&quot;);
	$tag = $stream-&gt;get_token;
	next unless $tag-&gt;is_start_tag ('strong');
	$tag = $stream-&gt;get_token;
	my $title = $tag-&gt;as_is;
	$tag = $stream-&gt;get_token;
	next unless $tag-&gt;is_end_tag ('/strong');
	$tag = $stream-&gt;get_token;
	next unless $tag-&gt;is_end_tag ('/a');
	$tag = $stream-&gt;get_token;
	next unless $tag-&gt;is_end_tag ('/dt');
	$tag = $stream-&gt;get_token;
	#We've got whitespace; on to the next tag
	$tag = $stream-&gt;get_token;
	next unless $tag-&gt;is_start_tag ('dd');
	my $content = &quot;&quot;;
	$tag = $stream-&gt;get_token;
	until ($tag-&gt;is_end_tag('/dd'))
	{
		$content .= $tag-&gt;as_is;
		$tag = $stream-&gt;get_token;
		next;
	}
	$rss-&gt;add_item(title       =&gt; $title,
		       link        =&gt; &quot;$url#$link&quot;,
	       	       description =&gt; encode_entities($content));
}

print $rss-&gt;as_string;
</pre>

<p> This is more verbose than necessary, but does the same thing as the
regex version. A better version would use HTML::TokeParser's 
<code>get_tag</code> method (<a href="misc/oregan/telsa-3.pl.txt">text</a>):

<pre class="code">
#!/usr/bin/perl -w

use strict;
use XML::RSS;
use LWP::Simple;
use HTML::Entities;
use HTML::TokeParser::Simple;

my $rss = new XML::RSS (version =&gt; '1.0');
my $url = &quot;http://www.linux.org.uk/~telsa/Diary/diary.html&quot;;
my $page = get($url);
my $stream = HTML::TokeParser::Simple-&gt;new(\$page);
my $tag;

$rss-&gt;channel(title       =&gt; &quot;The more accurate diary. Really.&quot;,
              link        =&gt; $url,
              description =&gt; &quot;Telsa's diary of life with a hacker:&quot; 
	      		     . &quot; the current ramblings&quot;);

while ($tag = $stream-&gt;get_tag('a'))
{
	next unless $tag-&gt;return_attr(&quot;name&quot;) ne &quot;&quot;;
	my $link = $tag-&gt;return_attr(&quot;name&quot;);
	$tag = $stream-&gt;get_tag ('strong');
	$tag = $stream-&gt;get_token;
	my $title = $tag-&gt;as_is;
	$tag = $stream-&gt;get_tag ('dd');
	my $content = &quot;&quot;;
	$tag = $stream-&gt;get_token;
	until ($tag-&gt;is_end_tag('/dd'))
	{
		$content .= $tag-&gt;as_is;
		$tag = $stream-&gt;get_token;
		next;
	}
	$rss-&gt;add_item(title       =&gt; $title,
		       link        =&gt; &quot;$url#$link&quot;,
	       	       description =&gt; encode_entities($content));
}

print $rss-&gt;as_string;
</pre>

<p> There are plenty of other modules for manipulating HTML: a <a 
href="http://search.cpan.org">CPAN search</a> gave me 7417 results! 

<p> If you're hungry for more, I recommend reading these articles from
<a href="http://perl.com/">Perl.com</a>: <a 
href="http://www.perl.com/pub/a/2001/11/15/creatingrss.html">Create RSS 
channels from HTML news sites</a> and <a 
href="http://www.perl.com/pub/a/2003/01/22/mechanize.html">Screen-scraping 
with WWW::Mechanize</a>. As a parting shot, I've also included a <a 
href="misc/oregan/moz2del.pl.txt">script</a> that generates <a 
href="http://del.icio.us">del.icio.us</a>-like XML from a Mozilla bookmark 
file: watch out for next month's Linux Gazette to find out what it's for!

</p>


<!-- *** BEGIN author bio *** -->
<P>&nbsp;
<P>
<!-- *** BEGIN bio *** -->
<hr>
<P>
<img ALIGN="LEFT" ALT="[BIO]" SRC="../gx/2004/authors/oregan.jpg" class="bio">
<em>
<!-- ../110/lg_laundrette.html#nottag.15 :) -->
Jimmy is a single father of one, who enjoys long walks... Oh, right.

<p> Jimmy has been using computers from the tender age of seven, when his father 
inherited an Amstrad PCW8256. After a few brief flirtations with an Atari ST
and numerous versions of DOS and Windows, Jimmy was introduced to Linux in 1998
and hasn't looked back.

<p> In his spare time, Jimmy likes to play guitar and read: not at the same time, 
but the picks make handy bookmarks.
</em>
<br CLEAR="all">
<!-- *** END bio *** -->

<!-- *** END author bio *** -->

<div id="articlefooter">

<p>
Copyright &copy; 2004, Jimmy O'Regan. Released under the <a
href="http://linuxgazette.net/copying.html">Open Publication license</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>

<p>
Published in Issue 108 of Linux Gazette, November 2004
</p>

</div>


<div id="previousnextbottom">
<A HREF="oregan.html" >&lt;-- prev</A> | <A HREF="park.html" >next --&gt;</A>
</div>


</div>






<div id="navigation">

<a href="../index.html">Home</a>
<a href="../faq/index.html">FAQ</a>
<a href="../lg_index.html">Site Map</a>
<a href="../mirrors.html">Mirrors</a>
<a href="../mirrors.html">Translations</a>
<a href="../search.html">Search</a>
<a href="../archives.html">Archives</a>
<a href="../authors/index.html">Authors</a>
<a href="../contact.html">Contact Us</a>

</div>



<div id="breadcrumbs">

<a href="../index.html">Home</a> &gt; 
<a href="index.html">November 2004 (#108)</a> &gt; 
Article

</div>





<img src="../gx/2003/sit3-shine.7-2.gif" id="tux" alt="Tux"/>




</body>
</html>

