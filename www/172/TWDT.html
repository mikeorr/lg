<!DOCTYPE html
	PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
	 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="utf-8" xml:lang="utf-8">
<head>
<title>
Linux Gazette : March 2010 (#172) 
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link href="../lg.css" rel="stylesheet" type="text/css" media="screen, projection" />

<style type="text/css" media="screen, projection">
<!--

.twdtarticle {
	width: 84%;
}

.twdtarticle h1 {
	font-size:19px;
	text-align:center;
}

.lgcontent {
        width: 84%;
        margin-top: 30px;
}

-->
</style>

</head>

<body id="twdtbody">

<a href="../">
<img src="../gx/2003/newlogo-blank-200-gold2.jpg" alt="Linux Gazette" id="twdtlogo"/>
</a>
<p id="fun">...making Linux just a little more fun!</p>

<div id="navigation">

<a href="../index.html">Home</a>
<a href="../faq/index.html">FAQ</a>
<a href="../lg_index.html">Site Map</a>
<a href="../mirrors.html">Mirrors</a>
<a href="../mirrors.html">Translations</a>
<a href="../search.html">Search</a>
<a href="../archives.html">Archives</a>
<a href="../authors/index.html">Authors</a>
<a href="http://lists.linuxgazette.net/listinfo.cgi">Mailing Lists</a>
<a href="../jobs.html">Join Us!</a>
<a href="../contact.html">Contact Us</a>
</div>

<div id="breadcrumbs1">

<a href="../index.html">Home</a> &gt;
<a href="index.html">March 2010 (#172)</a> &gt;
TWDT

</div>


<div class="content lgcontent">

<h2>March 2010 (#172):</h2>

<ul>

	<li><a href="#lg_mail">Mailbag</a>

	<li><a href="#lg_talkback">Talkback</a>

	<li><a href="#lg_tips">2-Cent Tips</a>

	<li><a href="#lg_bytes">News Bytes</a>, by <i>Deividson Luiz Okopnik and Howard Dyckoff</i></li>

	<li><a href="#brownss">Recovering FAT Directory Stubs with SleuthKit</a>, by <i>Silas Brown</i></li>

	<li><a href="#crosby">The Next Generation of Linux Games - GLtron and Armagetron Advanced</a>, by <i>Dafydd Crosby</i></li>

	<li><a href="#dyckoff">Away Mission - Recommended for March</a>, by <i>Howard Dyckoff</i></li>

	<li><a href="#grebler">Tunnel Tales 1</a>, by <i>Henry Grebler</i></li>

	<li><a href="#peterson">Logging to a Database with Rsyslog</a>, by <i>Ron Peterson</i></li>

	<li><a href="#silva">Running Quake 3 and 4 on Fedora 12 (64-bit)</a>, by <i>Anderson Silva</i></li>

	<li><a href="#collinge">HelpDex</a>, by <i>Shane Collinge</i></li>

	<li><a href="#doomed">Doomed to Obscurity</a>, by <i>Pete Trbovich</i></li>

	<li><a href="#lg_backpage">The Backpage</a>, by <i>Ben Okopnik</i></li>

</ul>

</div>



<br />


<div class="content lgcontent">

<a name="lg_mail"></a>
<h1>Mailbag</h1>

</b>
</p>

<p>
<h3>This month's answers created by:</h3><strong>[  Ben Okopnik, Ren&eacute; Pfeiffer, Thomas Adam  ]</strong>
<br />...and you, our readers!<br /><hr width="50%" align="center" size="3" /><h1>Our Mailbag</h1>
<hr />

<!-- Thread anchor: mbox selective deletion --><a name='mbox_selective_deletion'></a>
<h3>mbox selective deletion</h3>
<p>
<b><p>
Sam Bisbee [sbisbee at computervip.com]

</p>
</b><br />
<b>Fri, 5 Feb 2010 19:28:52 -0500</b>
</p>

<p>
Hey gang,
</p>

<p>
Here's the deal: I'm trying to delete a message from an mbox with Bash. I have
the message number that I got by filtering with `frm` (the message is
identified by a header that holds a unique SHA crypt). You've problem guessed
by now, but mailutils is fair game.
</p>

<p>
I don't want to convert from mbox to maildir in /tmp on each run, because it's
reasonable that the script would be run every minute. Also, I don't want to put
users through that pain with a large mbox.
</p>

<p>
Also, I really don't want to write a "delete by message number" program in C
using the libmailutils program, but I will resort to it if needed.
</p>

<p>
I saw <a href='http://www.argon.org/~roderick/mbox-purge.html'>http://www.argon.org/~roderick/mbox-purge.html</a>, but would like to have
"common" packages as dependencies only.
</p>

<p>
Is there some arg that I missed in `mail`? Should I just try and roll
mbox-purge in? All ideas, tricks and release management included, are welcome.
</p>

<p>
Cheers,
</p>

<pre>-- 
Sam Bisbee
</pre>

<p>

</p>

<p><b>[  <a name="mb-mbox_selective_deletion"></a> <a href="misc/lg/mbox_selective_deletion.html">Thread continues here (10 messages/30.08kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: Vocabulary lookup from the command line? --><a name='vocabulary_lookup_from_the_command_line'></a>
<h3>Vocabulary lookup from the command line?</h3>
<p>
<b><p>
Ren&eacute; Pfeiffer [lynx at luchs.at]

</p>
</b><br />
<b>Thu, 4 Feb 2010 01:08:04 +0100</b>
</p>

<p>
Hello, TAG!
</p>

<p>
I am trying to brush up my French - again. So I bought some comics in
French and I try to read them. Does anyone know of a command line tool
or even a "vocabulary shell" that allows me to quickly look up words,
prreferrably French words with Germen or English translations?
</p>

<p>
I can always hack a Perl script that queries dict.leo.org or similar
services, but a simple lookup tool should be around already.
</p>

<p>
Salut,
Ren&eacute;.
</p>

<p>

</p>

<p><b>[  <a name="mb-vocabulary_lookup_from_the_command_line"></a> <a href="misc/lg/vocabulary_lookup_from_the_command_line.html">Thread continues here (7 messages/11.42kB)</a>  ]</b></p>
<hr />


<br clear="all" />

<script type='text/javascript'>
digg_url = 'http://linuxgazette.net/172/lg_mail.html';
digg_title = 'Mailbag';
digg_bodytext = '<p>In this month\'s Linux Gazette, \'Mailbag\' covers the following topics:<br>mbox selective deletion<br>Vocabulary lookup from the command line?<br></p>';
digg_topic = 'linux_unix';
</script>
<script src="http://digg.com/tools/diggthis.js" type="text/javascript"></script>

</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:172/lg_mail.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Published in Issue 172 of Linux Gazette, March 2010
</p>

</div>
</div>


<div class="content lgcontent">

<a name="lg_talkback"></a>
<h1>Talkback</h1>

</b>
</p>

<p>

<!-- Thread anchor: Talkback:168/lg_tips.html --><a name='talkback_168_lg_tips'></a>
<h3>Talkback:168/lg_tips.html</h3>
<p><b>[ In reference to "<a href='../168/lg_tips.html'>2-Cent Tips</a>" in LG#168 ]</b></p><p>
<b><p>
Amit Saha [amitsaha.in at gmail.com]

</p>
</b><br />
<b>Mon, 1 Feb 2010 20:43:28 +0530</b>
</p>

<p>
Hell Bob McCall:
</p>

<p>
On Mon, Feb 1, 2010 at 8:27 PM, Bob McCall &lt;rcmcll@gmail.com&gt; wrote:
</p>

<pre>
&gt; Amit:
&gt; I saw your two cent tip on gnuplot. Have you seen this?
&gt;
&gt; <a href='http://ndevilla.free.fr/gnuplot/'>http://ndevilla.free.fr/gnuplot/</a>
&gt;
&gt; best,
&gt; Bob
</pre>


<p>
Nice! Thanks for the pointer, I am CCing TAG.
</p>

<p>
Best,
Amit
</p>



<pre>-- 
Journal: <a href='http://amitksaha.wordpress.com'>http://amitksaha.wordpress.com</a>,
µ-blog: <a href='http://twitter.com/amitsaha'>http://twitter.com/amitsaha</a>
 
Freenode: cornucopic in #scheme, #lisp, #math,#linux, #python
</pre>


</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:172/lg_talkback.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Published in Issue 172 of Linux Gazette, March 2010
</p>

</div>
</div>


<div class="content lgcontent">

<a name="lg_tips"></a>
<h1>2-Cent Tips</h1>

</b>
</p>

<p>

<!-- Thread anchor: Two-cent Tip: Retrieving directory contents --><a name='two_cent_tip__retrieving_directory_contents'></a>
<h3>Two-cent Tip: Retrieving directory contents</h3>
<p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]

</p>
</b><br />
<b>Wed, 3 Feb 2010 22:35:08 -0500</b>
</p>

<p>
During a recent email discussion regarding pulling down the LG archives
with 'wget', I discovered (perhaps mistakenly; if so, I wish someone
would enlighten me [1]) that there's no way to tell it to pull down all
the files in a directory unless there's a page that links to all those
files... and the directory index doesn't count (even though it contains
links to all those files.) So, after a minute or two of fiddling with
it, I came up with a following solution:
</p>

<p>
<pre class='code'>
#!/bin/bash
# Created by Ben Okopnik on Fri Jan 29 14:41:57 EST 2010
 
[ -z "$1" ] &amp;&amp; { printf "Usage: ${0##*/} &lt;URL&gt; \n"; exit; }
 
# Safely create a temporary file
file=`tempfile`
# Extract all the links from the directory listing into a local text file
wget -q -O - "$1"|\
URL="${1%/}" perl -wlne'print "$ENV{URL}/$2" if /href=(["\047])([^\1]+)\1/' &gt; $file
# Retrieve the listed links
wget -i $file &amp;&amp; rm $file
</pre>

<p>
To summarize, I used 'wget' to grab the directory listing, parse it
to extract all the links, prefixing them with the site URL, and saved
the result into a local tempfile. Then, I used that tempfile as a source
for 'wget's '-i' option (read the links to be retrieved from a file.)
</p>

<p>
I've tested this script on about a dozen directories with a variety of
servers, and it seems to work fine.
</p>



<p>
[1] <strong>Please</strong> test your proposed solution, though. I'm rather cranky at
'wget' with regard to its documentation; perhaps it's just me, but I
often find that the options described in its manpage do something rather
different from what they promise to do. For me, 'wget' is a terrific
program, but the documentation has lost something in the translation
from the original Martian.
</p>

<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>

<hr />


<!-- Thread anchor: Two-cent Tip: How big is that directory? --><a name='two_cent_tip__how_big_is_that_directory'></a>
<h3>Two-cent Tip: How big is that directory?</h3>
<p>
<b><p>
Dr. Parthasarathy S [drpartha at gmail.com]

</p>
</b><br />
<b>Tue, 2 Feb 2010 09:57:02 +0530</b>
</p>

<p>
At times, you may need to know exactly how big is a certain directory
(say top directory) along with all its contents and subdirectories(and
their contents). You may need this if you are copying a large diectory
along with its contents and structure.  And you may like to know if what
you got after the copy, is what you sent. Or you may need this when
trying to copy stuff on to a device where the space is limited. So you
want to make sure that you can accomodate the material you are planning
to send.
</p>

<p>
Here is a cute little script. Calling sequence::  
<pre>
howmuch &lt;top directory name&gt;
</pre>

<p>
You get a summary, which gives the total size, the number of
subdirectories, and the number of files (counted from the top
directory). Good for book-keeping.
</p>

<p>
<pre class='code'>
###########start-howmuch-script
# Tells you how many files, subdirectories and content bytes in a
# directory
# Usage :: how much &lt;directory-path-and-name&gt;
 
# check if there is no command line argument
if [ $# -eq 0 ]
then
echo "You forgot the directory to be accounted for !"
echo "Usage :: howmuch &lt;directoryname with path&gt;"
exit
fi
 
echo "***start-howmuch***"
pwd &gt; ~/howmuch.rep
pwd
echo -n "Disk usage of directory ::" &gt; ~/howmuch.rep
echo $1 &gt;&gt; ~/howmuch.rep
echo -n "made on ::" &gt;&gt; ~/howmuch.rep
du -s $1 &gt; ~/howmuch1
tree $1 &gt; ~/howmuch2
date &gt;&gt; ~/howmuch.rep
tail ~/howmuch1 &gt;&gt; ~/howmuch.rep
tail --lines=1 ~/howmuch2 &gt;&gt; ~/howmuch.rep
cat ~/howmuch.rep
# cleanup
rm ~/howmuch1
rm ~/howmuch2
#Optional -- you can delete howmuch.rep if you want
#rm ~/howmuch.rep
 
echo "***end-howmuch***"
# <strong>  </strong>
 
 
########end-howmuch-script
</pre>
 
 
<pre>-- 
---------------------------------------------------------------------------------------------
Dr. S. Parthasarathy                    |   mailto:drpartha@gmail.com
Algologic Research &amp; Solutions    |
78 Sancharpuri Colony                 |
Bowenpally  P.O                          |   Phone: + 91 - 40 - 2775 1650
Secunderabad 500 011 - INDIA     |
WWW-URL: <a href='http://algolog.tripod.com/nupartha.htm'>http://algolog.tripod.com/nupartha.htm</a>
---------------------------------------------------------------------------------------------
</pre>

<p>

</p>

<p><b>[  <a name="mb-two_cent_tip__how_big_is_that_directory"></a> <a href="misc/lg/two_cent_tip__how_big_is_that_directory.html">Thread continues here (5 messages/9.85kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: Two-cent tip: GRUB and inode sizes --><a name='two_cent_tip__grub_and_inode_sizes'></a>
<h3>Two-cent tip: GRUB and inode sizes</h3>
<p>
<b><p>
Ren&eacute; Pfeiffer [lynx at luchs.at]

</p>
</b><br />
<b>Wed, 3 Feb 2010 01:07:03 +0100</b>
</p>

<p>
Hello!
</p>

<p>
I had a decent fight with a stubborn server today. It was a Fedora Core
6 system (let's not talk about how old it is) that was scheduled for a
change of disks. This is fairly straightforward - until you have to
write the boot block. Unfortunately I prepared the new disks before
copying the files. As soon as I wanted to install GRUB 0.97 it told me
that it could not read the stage1 file. The problem is that GRUB only
deals with 128-byte inodes. The prepared / partition has 256-byte
inodes. So make sure to use
</p>

<pre>
mkfs.ext3 -I 128 /dev/sda1
</pre>

<p>
when preparing disks intended to co-exist with GRUB. I know this is old
news, but I never encountered this problem before.
<a href='http://www.linuxplanet.com/linuxplanet/tutorials/6480/2/'>http://www.linuxplanet.com/linuxplanet/tutorials/6480/2/</a> has more hints
ready.
</p>

<p>
Best,
Ren&eacute;,
who is thinking about moving back to LILO.
</p>

<p>

</p>

<p><b>[  <a name="mb-two_cent_tip__grub_and_inode_sizes"></a> <a href="misc/lg/two_cent_tip__grub_and_inode_sizes.html">Thread continues here (3 messages/2.77kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: Two-cent Tip: backgrounding the last stopped job without knowing its job ID --><a name='two_cent_tip__backgrounding_the_last_stopped_job_without_knowing_its_job_id'></a>
<h3>Two-cent Tip: backgrounding the last stopped job without knowing its job ID</h3>
<p>
<b><p>
Mulyadi Santosa [mulyadi.santosa at gmail.com]

</p>
</b><br />
<b>Mon, 22 Feb 2010 16:14:09 +0700</b>
</p>

<p>
For most people, to send a job to background after stopping a task,
he/she will take a note the job ID and then invoke "bg" command
appropriately like below:
</p>

<p>
<pre class='code'>
$ (while (true); do yes  &gt; /dev/null ; done)
^Z
[2]+  Stopped                 ( while ( true ); do
    yes &gt; /dev/null;
done )
 
$ bg %2
[2]+ ( while ( true ); do
    yes &gt; /dev/null;
done ) &amp;
</pre>

<p>
Can we omit the job ID? Yes, we can. Simply replace the above "bg %2"
with "bg %%". It will refer to the last stopped job ID. This way,
command typing mistake could be avoided too.
</p>

<pre>-- 
regards,
 
Mulyadi Santosa
Freelance Linux trainer and consultant
 
blog: the-hydra.blogspot.com
training: mulyaditraining.blogspot.com
</pre>

<p>

</p>

<p><b>[  <a name="mb-two_cent_tip__backgrounding_the_last_stopped_job_without_knowing_its_job_id"></a> <a href="misc/lg/two_cent_tip__backgrounding_the_last_stopped_job_without_knowing_its_job_id.html">Thread continues here (4 messages/4.27kB)</a>  ]</b></p>
<hr />


<br clear="all" />

<script type='text/javascript'>
digg_url = 'http://linuxgazette.net/172/lg_tips.html';
digg_title = '2-Cent Tips';
digg_bodytext = '<p>In this month\'s Linux Gazette, \'2-Cent Tips\' covers the following topics:<br>Two-cent Tip: Retrieving directory contents<br>Two-cent Tip: How big is that directory?<br>Two-cent tip: GRUB and inode sizes<br>Two-cent Tip: backgrounding the last stopped job without knowing its job ID<br></p>';
digg_topic = 'linux_unix';
</script>
<script src="http://digg.com/tools/diggthis.js" type="text/javascript"></script>

</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:172/lg_tips.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Published in Issue 172 of Linux Gazette, March 2010
</p>

</div>
</div>


<div class="content lgcontent">

<a name="lg_bytes"></a>
<h1>News Bytes</h1>
<p id="by"><b>By <a href="../authors/dokopnik.html">Deividson Luiz Okopnik</a> and <a href="../authors/dyckoff.html">Howard Dyckoff</a></b></p>

</b>
</p>

<p>
<style type="text/css">
<!--
#news h2 { color: green; text-align: center; }
#news h3 { color: green; }
-->
</style>


<p>
<center>
<table cellpadding="7">
<tr>
<td>
<img src="../gx/bytes.gif" border="1" alt="News Bytes">
</td>
<td>
<h3>Contents:</h3>
<ul>
<li><a href="#general">News in General</a>
<li><a href="#Events">Conferences and Events</a>
<li><a href="#distro">Distro News</a>
<li><a href="#commercial">Software and Product News</a>
</ul>
</td>
</tr>
</table>
<strong>Selected and Edited by <a href="mailto:bytes@linuxgazette.net">Deividson Okopnik</a></strong>
</center>
</p>

<p style="font-style: italic"> Please submit your News Bytes items in
<strong>plain text</strong>; other formats may be rejected without reading.
[You have been warned!]  A one- or two-paragraph summary plus a URL has a
much higher chance of being published than an entire press release. Submit
items to <a
href="mailto:bytes@linuxgazette.net">bytes@linuxgazette.net</a>. Deividson can also be reached via <a href="http://www.twitter.com/deivid_okop/">twitter</a>.</p>

<hr>

<div id="news">

<p>
<a name="general"></a>
<h2>News in General</h2>
<h3><img alt="lightning bolt" src="../gx/bolt.gif">Red Hat Announces Fourth Annual Innovation Awards</h3>

<p>The fourth annual Red Hat Innovation awards will be presented at the
2010 Red Hat Summit and JBoss World, co-located in Boston, from June 22 to
25. The Innovation Awards recognize the creative use of Red Hat and JBoss
solutions by customers, partners and the community.</p>

<p>Categories for the 2010 Innovation Awards include:</p>

<ul>
<li> Optimized Solutions: Recognition of striking performance,
scalability and/or usability enhancements delivered with open source
solutions;<br /></li>
<li>Superior Alternatives: Recognition of the most successful
migration from proprietary solutions to open source alternatives;<br /></li>
<li>Extensive Ecosystem: Recognition of the use of Red Hat or JBoss'
expanding partner ecosystem to create innovative architectures based
on open source solutions;<br /></li>
<li>Carved out Costs: Recognition of customers who have leveraged
open source solutions to significantly cut costs and extract added
value from existing systems;<br /></li>
<li>Outstanding Open Source Architecture: Recognition of the use of
Red Hat, JBoss and partner offerings to create innovative
architectures based on open source solutions.</li>
</ul>

<p>Submissions for the Innovations Awards will be accepted until April
15, 2010. Four categories will each recognize two winning submissions,
one from Red Hat and one from JBoss, and the Outstanding Open Source
Architecture category will recognize one winner who is deploying both
Red Hat and JBoss solutions. From these category winners, one Red Hat
Innovator of the Year and one JBoss Innovator of the Year will be
selected by the community through online voting, and will be announced
at an awards ceremony at the Red Hat Summit and JBoss World. To view
last year's winners, visit:
<a href="http://press.redhat.com/2009/09/10/red-hat-and-jboss-innovators-of-the-year-2009/">http://press.redhat.com/2009/09/10/red-hat-and-jboss-innovators-of-the-year-2009/</a>.</p>

<p>For more information, or to submit a nomination for the 2010
Innovation Awards, visit: www.redhat.com/innovationawards. For
more information on the 2010 Red Hat Summit and JBoss World, visit:
<a href="http://www.redhat.com/promo/summit/2010/">http://www.redhat.com/promo/summit/2010/</a>.</p>




<h3><img alt="lightning bolt" src="../gx/bolt.gif">Novell and LPI Partner on Linux Training and Certification</h3>

<p>Novell and The Linux Professional Institute (LPI) have announced an
international partnership to standardize their entry-level Linux
certification programs on LPIC-1. Under this program, Linux
professionals who have earned their LPIC-1 status will also satisfy 
the requirements for the Novell Certified Linux Administrator (CLA)
certification. In addition, Novell Training Services has formally 
agreed to include required LPIC-1 learning objectives in its CLA course
training material.</p>

<p>Adoption of Linux, including SUSE Linux Enterprise from Novell, is
accelerating as the industry pursues cost saving solutions that deliver
maxiumum reliability and manageability. A 2009 global survey of IT
executives revealed that 40 percent of survey participants plan to deploy
additional workloads on Linux over the next 12-24 months and 49 percent
indicated Linux will be their primary server platform within five
years.</p>

<p>"This partnership represents the strong support the LPI certification
program has within the wider IT and Linux community. This historical
support has included contributions from vendors such as Novell and has
assisted LPI to become the most widely recognized and accepted Linux
certification," said Jim Lacey, president and CEO of LPI. "We look forward
to working with Novell to promote the further development of the Linux
workforce of the future. In particular, by aligning its training and exam
preparation curriculum to support LPIC-1 objectives, Novell has recognized
the industry's need for a vendor-neutral certification program that
prepares IT professionals to work with any Linux distribution in an
enterprise environment."</p>

<p>Under the terms of the agreement, all qualified LPIC-1 holders - except
those in Japan - will have the opportunity to apply for Novell CLA
certification without additional exams or fees. Novell Training Services
will include LPIC-1 objectives into its Linux Administrator curriculum and
programs which include self study, on demand, and partner-led classroom
training.</p>

<p>More information about acquiring dual certification status can be 
found at Novell here: <a href="http://www.novell.com/training/certinfo/cla/">http://www.novell.com/training/certinfo/cla/</a> and at 
the Linux Professional Institute here: <a href="http://www.lpi.org/cla/">http://www.lpi.org/cla/</a>.</p>

<hr>

<a name="links"></a>
<h2>Conferences and Events</h2>
<p>
<dl> <dt> <strong>ESDC 2010 - Enterprise Software Development Conference</strong> <dd>
March 1 - 3, San Mateo Marriott, San Mateo, Ca<br />
<a href="http://www.go-esdc.com/">http://www.go-esdc.com/</a>.</dl>

<dl> <dt> <strong>RSA 2010 Security Conference</strong> <dd>
March 1 - 5, San Francisco, CA<br />
<a href="http://www.rsaconference.com/events/">http://www.rsaconference.com/events</a>.</dl>

<dl> <dt> <strong>Oracle + Sun Customer Events</strong> <dd>
March 2010,  US &amp; International Cities:<br />
March  2, New York<br />
March  3, Toronto<br />
March 11, London<br />
March 16, Hong Kong<br />
March 18, Moscow<br />
<a href="http://www.oracle.com/events/welcomesun/index.html">http://www.oracle.com/events/welcomesun/index.html</a></dl>

<dl> <dt> <strong>Enterprise Data World Conference</strong> <dd>
March 14 - 18, Hilton Hotel, San Francisco, CA<br />
<a href="http://edw2010.wilshireconferences.com/index.cfm">http://edw2010.wilshireconferences.com/index.cfm</a>.</dl>

<dl> <dt> <strong>Cloud Connect 2010</strong> <dd>
March 15 - 18, Santa Clara, CA<br />
<a href="http://www.cloudconnectevent.com/">http://www.cloudconnectevent.com/</a>.</dl>

<dl> <dt> <strong>Open Source Business Conference (OSBC)</strong> <dd>
March 17 - 18, Palace Hotel, San Francisco, CA<br />
<a href="https://www.eiseverywhere.com/ehome/index.php?eventid=7578">https://www.eiseverywhere.com/ehome/index.php?eventid=7578</a>.</dl>

<dl> <dt> <strong>EclipseCon 2010</strong> <dd>
March 22 - 25, Santa Clara, CA<br />
<a href="http://www.eclipsecon.org/2010/">http://www.eclipsecon.org/2010/</a>.</dl>

<dl> <dt> <strong>User2User - Mentor Graphics User Group</strong> <dd>
April 6, Marriott Hotel, Santa Clara, CA<br />
<a href="http://user2user.mentor.com/">http://user2user.mentor.com/</a>.</dl>

<dl> <dt> <strong>Texas Linux Fest</strong> <dd>
April 10, Austin, TX<br />
<a href="http://www.texaslinuxfest.org/">http://www.texaslinuxfest.org/</a>.</dl>

<dl> <dt> <strong>MySQL Conference &amp; Expo 2010</strong> <dd>
April 12 - 15, Convention Center, Santa Clara, CA<br />
<a href="http://en.oreilly.com/mysql2010/">http://en.oreilly.com/mysql2010/</a>.</dl>

<dl> <dt> <strong>Black Hat Europe 2010</strong> <dd>
April 12 - 15, Hotel Rey Juan Carlos, Barcelona, Spain<br />
<a href="http://www.blackhat.com/html/bh-eu-10/bh-eu-10-home.html">http://www.blackhat.com/html/bh-eu-10/bh-eu-10-home.html</a>.</dl>

<dl> <dt> <strong>4th Annual Linux Foundation Collaboration Summit</strong> <dd>
Co-located with the Linux Forum Embedded Linux Conference<br />
April 14 - 16, Hotel Kabuki, San Francisco, CA <br />
By invitation only.</dl>

<dl> <dt> <strong>eComm - The Emerging Communications Conference</strong> <dd>
April 19 - 21, Airport Marriott, San Francisco, CA<br />
<a href="http://america.ecomm.ec/2010/">http://america.ecomm.ec/2010/</a>.</dl>

<dl> <dt> <strong>STAREAST 2010 - Software Testing Analysis &amp; Review</strong> <dd>
April 25 - 30, Orlando, FL<br />
<a href="http://www.sqe.com/STAREAST/">http://www.sqe.com/STAREAST/</a>.</dl>

<dl> <dt> <strong>Usenix LEET '10, IPTPS '10, NSDI '10</strong> <dd>
April 27, 28 - 30, San Jose, CA<br />
<a href="http://usenix.com/events/">http://usenix.com/events/</a>.</dl>

<dl> <dt> <strong>Citrix Synergy-SF</strong> <dd>
May 12 - 14, San Francisco, CA<br />
<a href="http://www.citrix.com/lang/English/events.asp">http://www.citrix.com/lang/English/events.asp</a>.</dl>

<dl> <dt> <strong>Black Hat Abu Dhabi 2010</strong> <dd>
May 30 - June 2, Abu Dhabi, United Arab Emirates<br />
<a href="http://www.blackhat.com/html/bh-ad-10/bh-ad-10-home.html">http://www.blackhat.com/html/bh-ad-10/bh-ad-10-home.html</a>.</dl>

<dl> <dt> <strong>Uber Conf 2010</strong> <dd>
June 14 - 17, Denver, CO<br />
<a href="http://uberconf.com/conference/denver/2010/06/home/">http://uberconf.com/conference/denver/2010/06/home/</a>.</dl>

<dl> <dt> <strong>Semantic Technology Conference</strong> <dd>
June 21 - 25, Hilton Union Square, San Francisco, CA<br />
<a href="http://www.semantic-conference.com/">http://www.semantic-conference.com/</a>.</dl>

<dl> <dt> <strong>O'Reilly Velocity Conference</strong> <dd>
June 22 - 24, Santa Clara, CA<br />
<a href="http://en.oreilly.com/velocity/">http://en.oreilly.com/velocity/</a>.</dl>

</p>

<hr>

<a name="distro"></a>
<h2>Distro News</h2>
<h3><img alt="lightning bolt" src="../gx/bolt.gif">RHEL 5.5 Beta Available</h3>

<p>Red Hat has announced the beta release of RHEL 5.5
(kernel-2.6.18-186.el5) for the Red Hat Enterprise Linux 5 family of
products including RHEL 5 Server, Desktop, and Advanced Platform for x86,
AMD64/Intel64, Itanium Processor Family, and Power Systems
micro-processors.</p>

<p>The supplied beta packages and CD and DVD images are intended for
testing purposes only.   Benchmark and performance results cannot be
published based on this beta release without explicit approval from
Red Hat.</p>

<p>The beta of RHEL 5.5, called Anaconda, has an "upgrade" option for an
upgrade from Red Hat Enterprise Linux 4.8 or 5.4 to the Red Hat Enterprise 
Linux 5.5 beta.  However Red Hat cautions that there is no guarantee the
upgrade will preserve  all of a system's settings, services, and
custom configurations.  For this reason, Red Hat recommends a fresh
install rather than an upgrade.  Upgrading from beta release to the GA
product is not supported.</p>

<p>A public general discussion mailing list for beta testers is at:
<a href="https://www.redhat.com/mailman/listinfo/rhelv5-beta-list/">https://www.redhat.com/mailman/listinfo/rhelv5-beta-list/</a>
The beta testing period is scheduled to continue through March 16, 
2010.</p>

<h3><img alt="lightning bolt" src="../gx/bolt.gif">SimplyMEPIS 8.5 Beta5</h3>

<p>MEPIS has announced SimplyMEPIS 8.4.97, the fifth beta of the forthcoming 
MEPIS 8.5, now available from MEPIS and public mirrors. The ISO
files for 32 and 64 bit processors are SimplyMEPIS-CD_8.4.97-b5_32.iso 
and SimplyMEPIS-CD_8.4.97-b5_64.iso respectively. Deltas, requested by the 
MEPIS community, are also available.</p>

<p>According to project lead Warren Woodford,  "We are coming down the home
stretch on the development of SimplyMEPIS version 8.5. There is some work
to be done on MEPIS components, but the major packages are locked in to
what may be their final versions. We've been asked to make more package
updates, but we can't go much further without switching to a Debian Squeeze
base. Currently, we have kernel 2.6.32.8, KDE 4.3.4, gtk 2.18.3-1,
OpenOffice 3.1.1-12, Firefox 3.5.6-2, K3b 1.70.0-b1, Kdenlive 0.7.6,
Synaptic 0.63.1, Gdebi 0.5.9, and bind9 9.6.1-P3."</p>

<p>Progress on SimplyMEPIS development can be followed at
<a href="http://twitter.com/mepisguy/">http://twitter.com/mepisguy/</a> or at <a href="http://www.mepis.org/">http://www.mepis.org/</a>.</p>

<p>ISO images of MEPIS community releases are published to the 'released'
subdirectory at the MEPIS Subscriber's Site and at MEPIS public 
mirrors.</p>

<h3><img alt="lightning bolt" src="../gx/bolt.gif">Tiny Core Releases V2.8</h3>

<p>Tiny Core Linux is a very small (10 MB) minimal Linux GUI Desktop. It
is based on Linux 2.6 kernel, Busybox, Tiny X, and Fltk. The core runs
entirely in ram and boots very quickly. Also offered is Micro Core a 6
MB image that is the console based engine of Tiny Core. CLI versions
of Tiny Core's program allows the same functionality of Tiny Core's
extensions only starting with a console based system.</p>

<p>It is not a complete desktop nor is all hardware completely supported.
It represents only the core needed to boot into a very minimal X
desktop typically with wired Internet access.</p>

<p>Extra applications must be downloaded from online repositories or
compiled using tools provided.</p>

<p>The theme for this newest release is to have a single directory for
extensions and dependencies. This greatly improves systems resources
by having a single copy of dependencies, also greatly improves
flexibility in "moving" applications present upon boot, dependency
auditing, and both batch and selective updating.</p>

Tiny Core V2.8 is now posted at
<a href="http://distro.ibiblio.org/pub/linux/distributions/tinycorelinux/2.x/release/">http://distro.ibiblio.org/pub/linux/distributions/tinycorelinux/2.x/release/</a>.</p>

<h3><img alt="lightning bolt" src="../gx/bolt.gif">FreeBSD 7.3 Expected March 2010</h3>

<p>The FreeBSD 7.3 Beta-1 for legacy systems was released on January
30.  The production release is expected in early March.</p>
<a name="commercial"></a>
<hr>
<h2>Software and Product News</h2>

<h3><img alt="lightning bolt" src="../gx/bolt.gif">Ksplice "Abolishes the Reboot"</h3>

<p>Ksplice Inc. announced its Uptrack service patching service in
February, eliminating the need to restart Linux servers when
installing crucial updates and security patches.</p>

<p>Based on technology from the Massachusetts Institute of Technology,
Ksplice Uptrack is a subscription service that allows IT
administrators to keep Linux servers up-to-date without the disruption
and downtime of rebooting.</p>

<p>In 2009, major Linux vendors asked customers to install a kernel
update roughly once each month.  Before Uptrack, system administrators
had to schedule downtime in advance to bring Linux servers up-to-date,
because updating the kernel previously required rebooting the
computer. By allowing IT administrators to install kernel updates
without downtime, Uptrack dramatically reduces the cost of system
administration.</p>

<p>Ksplice Uptrack is now available for users of six leading versions of
Linux: Red Hat Enterprise Linux, Ubuntu, Debian GNU/Linux, CentOS,
Parallels Virtuozzo Containers, and OpenVZ. The subscription fee
starts at $3.95 per month per system, after a 30-day free trial. The
rate drops to $2.95 after 20 servers are covered.  A free version is
also available for Ubuntu.</p>

<p>Visit the Ksplice website here: <a
href="http://www.ksplice.com/">http://www.ksplice.com/</a>.
</p>

<h3><img alt="lightning bolt" src="../gx/bolt.gif">MariaDB Augments MySQL 5.1</h3>

<p>MariaDB 5.1.42, a new branch of the MySQL database - which includes
all major open source storage engines, myriad bug fixes, and many
community patches - has been released.</p>

<p>MariaDB is kept up to date with the latest MySQL release from the same
branch. MariaDB 5.1.42 is based on MySQL 5.1.42 and XtraDB 1.0.6-9.
In most respects MariaDB will work exactly as MySQL: all commands,
interfaces, libraries and APIs that exist in MySQL also exist in
MariaDB.</p>

<p>MariaDB contains extra fixes for Solaris and Windows, as well as
improvements in the test suite.</p>

<p>For information on installing MariaDB 5.1.42 on new servers or
upgrading to MariaDB 5.1.42 from previous releases, please check out
the installation guide at
<a href="http://askmonty.org/wiki/index.php/Manual:Installation/">http://askmonty.org/wiki/index.php/Manual:Installation/</a>.</p>

<p>MariaDB is available in source and binary form for a variety of
platforms and is available from the download pages:
<a href="http://askmonty.org/wiki/index.php/MariaDB:Download:MariaDB_5.1.42/">http://askmonty.org/wiki/index.php/MariaDB:Download:MariaDB_5.1.42/</a>.</p>

<h3><img alt="lightning bolt" src="../gx/bolt.gif">Moonlight 3.0 Preview Now Available</h3>

<p>The Moonlight project has released the second preview of Moonlight 3.0
in time for viewing the Winter Olympics.  The final release will
contain support for some features in Silverlight 4.</p>

<p>Moonlight is an open-source implementation of Microsoft's Silverlight
platform for creating and viewing multiple media types.</p>

<p>This release updates the Silverlight 3.0 support in Moonlight 2.0,
mostly on the infrastructure level necessary to support the rest of
the features.  This release includes:</p>

<ul>
<li> MP4 demuxer support. The demuxer is in place but there are no
codecs for it yet;<br /></li>
<li> Initial work on UI Virtualization;<br /></li>
<li> Platform Abstraction Layer: the Moonlight core is now separated
from the windowing system engine. This should make it possible for
developers to port Moonlight to other windowing/graphics systems that
are not X11/Gtk+ centric;<br /></li>
<li> New 3.0 Binding/BindingExpression support.</li>
</ul>

<p>Download Moonlight here: <a
href="http://www.go-mono.com/moonlight/">http://www.go-mono.com/moonlight/</a>.
</p>

<h3><img alt="lightning bolt" src="../gx/bolt.gif">OpenOffice.org 3.2 Faster, More Secure</h3>

<p>At the start of its tenth anniversary year, the OpenOffice.org 
Community
announced OpenOffice.org 3.2. The latest version starts faster,
offers new functions and better compatibility with other office
software.  The newest version also fixes bugs and potential security
vulnerabilities.</p>

<p>In just over a year from its launch, OpenOffice.org 3 had recorded 
over one hundred million downloads from its central download site alone.</p>

<p>OpenOffice.org 3.2 new features include:</p>

<ul>
<li> faster start up times;<br /></li>
<li> improved compatibility with ODF and proprietary file formats;<br /></li>
<li> improvements to all components, particularly the Calc spreadsheet;<br /></li>
<li> usability makeover and new chart types for the Chart module.</li>
</ul>

<p>A full guide to new features is available at
<a href="http://www.openoffice.org/dev_docs/features/3.2/">http://www.openoffice.org/dev_docs/features/3.2/</a>.</p>

<p>All users are encouraged to upgrade to the new release, because of the
potential security vulnerabilities addressed in 3.2. Any people still
using OpenOffice.org 2 should note that this version was declared 'end
of life' in December 2009, with no further security patches or 
bugfixes
being issued by the Community. For details, see
<a href="http://development.openoffice.org/releases/eol.html">http://development.openoffice.org/releases/eol.html</a>.</p>

<p>The security bulletin covering fixes for the potential vulnerabilities is
here: <a
href="http://www.openoffice.org/security/bulletin.html">http://www.openoffice.org/security/bulletin.html</a>.</p>

<p>Download OpenOffice.org 3.2 here: <a href="http://download.openoffice.org/">http://download.openoffice.org</a>.</p>

<h3><img alt="lightning bolt" src="../gx/bolt.gif">New Release of Oracle Enterprise Pack for Eclipse 11g</h3>
	
<p>In February, Oracle released the Oracle Enterprise Pack for Eclipse
11g, a component of Oracle's Fusion Middleware.</p>

<p>The Enterprise Pack for Eclipse is a free set of certified plug-ins
that enable developers to build Java EE and Web Services applications
for the Oracle Fusion Middleware platform where Eclipse is the
preferred Integrated Development Environment (IDE).  This release
provides an extension to Eclipse with Oracle WebLogic Server features,
WYSIWYG Web page editing, SCA support, JAX-WS Web Service validation,
an integrated tag and data palette, and smart editors.</p>

<p>Also new with this release is Oracle's AppXRay feature, a design time
dependency analysis and visualization tool that makes it easy for Java
developers to work in a team setting and reduce runtime debugging.
The new features in Oracle Enterprise Pack for Eclipse 11g allow
WebLogic developers to increase code quality by catching errors at
design time.</p>

<p>FastSwap support enables WebLogic developers to use FastSwap in
combination with AppXRay to allow changes to Java classes without
requiring re-deployment, also reducing the amount of time spent
deploying/re-deploying an application.</p>

<h3><img alt="lightning bolt" src="../gx/bolt.gif">GWOS Monitor Enterprise Release 6.1 Adds Ubuntu Support</h3>

<p>GWOS Monitor Enterprise Release 6.1 has added support for Ubuntu
servers.  It also includes many improvements and strengthens the GWOS
platform for monitoring heterogeneous, complex IT environments. This
new release, part of the ongoing evolution of the GWOS solution stack,
is available to Enterprise customers with a current subscription
agreement.  GroundWorks is also available in a community edition.</p>

<p>Highlights in this new release include:</p>

<ul>
<li> Ubuntu Server 8.04 LTS and Ubuntu Server 9.10 are now supported 
platforms;<br /></li>
<li> Tighter integration between the event view and state-oriented
views, reducing the time to diagnose a problem anywhere in the
monitored environment;<br /></li>
<li> Connect to affected systems directly from the status viewer with
two clicks. This feature, subject to administrator control, enables
faster remediation of detected problems by operations staff. Supports
console (SSH), web-based (HTTP/HTTPS) and graphical (VNC, RDP)
administration;<br /></li>
<li> Viewing all historical and performance information over
arbitrary time periods;<br /></li>
<li> Direct web links to specific service groups, services, host
groups and hosts within the portal application. Back-links to the
monitoring system are particularly useful when included in
notification messages and ticketing systems.<br /></li>
<li> Updated components including Nagios Core project 3.2 and the 
JBoss Portal;<br /></li>
<li> Performance improvements for large systems configuration.</li>
</ul>

<p>GroundWork 6.1, Enterprise Edition with Ubuntu support, is only $49
for the first 100 devices monitored. Download it here:
<a href="http://www.groundworkopensource.com/newExchange/flex-quickstart/">http://www.groundworkopensource.com/newExchange/flex-quickstart/</a>.</p>

<p>Also in January, GroundWork released Brooklyn, their iPhone app for
GroundWork Monitor and Nagios. The iPhone app costs $8.99 at the Apple
App Store.</p>
</div>

<hr>


<br clear="all" />

<script type='text/javascript'>
digg_url = 'http://linuxgazette.net/172/lg_bytes.html';
digg_title = 'News Bytes';
digg_bodytext = '<p>In this month\'s Linux Gazette, \'News Bytes\' covers the following topics:<br>Contents:<br>Red Hat Announces Fourth Annual Innovation Awards<br>Novell and LPI Partner on Linux Training and Certification<br>RHEL 5.5 Beta Available<br>SimplyMEPIS 8.5 Beta5<br>Tiny Core Releases V2.8<br>FreeBSD 7.3 Expected March 2010<br>Ksplice "Abolishes the Reboot"<br>MariaDB Augments MySQL 5.1<br>Moonlight 3.0 Preview Now Available<br>OpenOffice.org 3.2 Faster, More Secure<br>New Release of Oracle Enterprise Pack for Eclipse 11g<br>GWOS Monitor Enterprise Release 6.1 Adds Ubuntu Support<br></p>';
digg_topic = 'linux_unix';
</script>
<script src="http://digg.com/tools/diggthis.js" type="text/javascript"></script>

</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:172/lg_bytes.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<hr>
<p>
<img align="left" alt="[BIO]" src="../gx/authors/dokopnik.jpg" class="bio">
</p>

<em>
<p>
Deividson was born in Uni&atilde;o da Vit&oacute;ria, PR, Brazil, on
 14/04/1984. He became interested in computing when he was still a kid,
 and started to code when he was 12 years old. He is a graduate in
 Information Systems and is finishing his specialization in Networks and
 Web Development. He codes in several languages, including C/C++/C#, PHP,
 Visual Basic, Object Pascal and others.
</p>

<p>
Deividson works in Porto Uni&atilde;o's Town Hall as a Computer
 Technician, and specializes in Web and Desktop system development, and
 Database/Network Maintenance.
</p>



</em>

<br clear="all">


	<!-- *** BEGIN bio *** -->
<hr>
<p>

<img align="left" alt="Bio picture" src="../gx/authors/dyckoff.jpg" class="bio">

<em>
<p>
Howard Dyckoff is a long term IT professional with primary experience at
Fortune 100 and 200 firms. Before his IT career, he worked for Aviation
Week and Space Technology magazine and before that used to edit SkyCom, a
newsletter for astronomers and rocketeers. He hails from the Republic of
Brooklyn [and Polytechnic Institute] and now, after several trips to
Himalayan mountain tops, resides in the SF Bay Area with a large book
collection and several pet rocks.
</p>

<p>
Howard maintains the <a
href="http://technology-events.blogspot.com">Technology-Events</a> blog at
blogspot.com from which he contributes the Events listing for Linux
Gazette. Visit the blog to preview some of the next month's NewsBytes
Events.
</p>

</em>
<br clear="all">
<!-- *** END bio *** -->

<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2010, <a href="../authors/dokopnik.html">Deividson Luiz Okopnik</a> and <a href="../authors/dyckoff.html">Howard Dyckoff</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 172 of Linux Gazette, March 2010
</p>

</div>
</div>


<div class="content lgcontent">

<a name="brownss"></a>
<h1>Recovering FAT Directory Stubs with SleuthKit</h1>
<p id="by"><b>By <a href="../authors/brownss.html">Silas Brown</a></b></p>

</b>
</p>

<p>
<p>
When I accidentally dropped an old Windows Mobile PocketPC onto the floor
at the exact moment it was writing to a memory card, the memory card's
master FAT was corrupted and several directories disappeared from the root
directory.  Since it had not been backed up for some time, I connected the
memory card to a Linux system for investigation. (At this point it is
important not to actually mount the card. If you have an automounter, turn
it off before connecting. You have to access it as a device, for example
<tt>/dev/sdb1</tt>. To see which device it is, you can do <tt>ls
/dev/sd*</tt> both before and after connecting it and see what appears. The
following tools read from the device directly, or from an image of it
copied using the <tt>dd</tt> command.)
</p>

<p>
<tt>fsck.vfat -r</tt> offered to return over 1 GB of data to the free
space.  This meant there was over 1 GB of data that was neither indexed in
the FAT nor listed as free space, i.e. that was the lost directories.  It
was important not to let <tt>fsck.vfat</tt> mark it as free yet, as this
would impair data recovery.  <tt>fsck.vfat</tt> can also search this data
for contiguous blocks and reclaim them as files, but that would not have
been ideal either because the directory structure and all the filenames
would have been lost, leaving thousands of meaninglessly-named files to be
sorted out by hand (some of which may end up being split into more than one
file).
</p>

<p>
The directory structure was in fact still there; only the entries for the
top-level directories had been lost. The listings of their subdirectories
(including all filenames etc) were still present somewhere on the disk, but
the root directory was no longer saying where to find them. A few Google
searches seemed to suggest that the orphaned directory listings are
commonly known as "directory stubs" and I needed a program that could
search the disk for lost directory stubs and restore them. Unfortunately
such programs were not very common. The only one I found was a commercial
one called <A HREF="http://www.cnwrecovery.com/">CnW Recovery</A>, but that
requires administrator access to a Windows PC (which I do not have).
</p>

<h2>SleuthKit to the rescue</h2>

<p>
A useful utility is SleuthKit, available as a package on most distributions
(<tt>apt-get install sleuthkit</tt>) or from <A
HREF="http://www.sleuthkit.org/">sleuthkit.org</A>.  SleuthKit consists of
several commands, the most useful of which are <tt>fls</tt> and
<tt>icat</tt>. The <tt>fls</tt> command takes an image file (or device) and
an inode number, and attempts to display the directory listing that is
stored at that inode number (if there is one). This directory listing will
show files and other directories, and the inode numbers where they are
stored. The <tt>icat</tt> command also takes an image file and an inode
number, and prints out the contents of the file stored at that inode number
if there is one. Hence, if you know the inode number of the root directory,
you can chase through a normal filesystem with <tt>fls</tt> commands
(following the inode numbers of subdirectories etc) and use <tt>icat</tt>
to access the files within them.  <tt>fls</tt> also lists deleted entries
(marked with a <tt>*</tt>) as long as those entries are still present in
the FAT. (Incidentally, these tools also work on several other filesystems
besides FAT, and they make them all look the same.)
</p>

<p>
The range of valid inode numbers can be obtained using SleuthKit's
<tt>fsstat</tt> command. This tells you the root inode number (for example
2) and the maximum possible inode number (for example 60000000).
<tt>fsstat</tt> will also give quite a lot of other information, so you may
want to pipe its output through a pager such as <tt>more</tt> or
<tt>less</tt> (i.e. type <tt>fsstat|more</tt> or <tt>fsstat|less</tt>) in
order to catch the inode range near the beginning of the output.
</p>

<h2>Scanning for directory stubs</h2>

<p>
Because the root FAT had been corrupted, using <tt>fls</tt> on it did not
reveal the inode locations of the lost directories. Therefore it was
necessary to scan through all possible inode numbers in order to find them.
This is a lot of work to do manually, so I wrote a Python script to call
the necessary <tt>fls</tt> commands automatically. First it checks the root
directory and all subdirectories for the locations of "known" files and
directories, and then it scans all the other inodes to see if any of them
contain directory listings that are not already known about. If it finds a
lost directory listing, it will try to recover all the files and
subdirectories in it with their correct names, although it cannot recover
the name of the top-level directory it finds.
</p>

<p>
Sometimes it finds data that just happens to pass the validity check for a
directory listing, but isn't. This results in it creating a "recovered"
directory full of junk. But often it correctly recovers a lost directory.
</p>

<pre class="code">
image = "/dev/sdb1"
recover_to = "/tmp/recovered"

import os, commands, sys

def is_valid_directory(inode):
    exitstatus,outtext = commands.getstatusoutput("fls -f fat "+image+" "+str(inode)+" 2&gt;/dev/null")
    return (not exitstatus) and outtext
def get_ls(inode): return commands.getoutput("fls -f fat "+image+" "+str(inode))

def scanFilesystem(inode, inode2path_dic, pathSoFar=""):
    if pathSoFar: print "   Scanning",pathSoFar
    for line in get_ls(inode).split('\n'):
        if not line: continue
        try: theType,theInode,theName = line.split(None,2)
        except: continue # perhaps no name (false positive inode?) - skip
        if theInode=='*': continue # deleted entry (theName starts with inode) - ignore
        assert theInode.endswith(":") ; theInode = theInode[:-1]
        if theType=="d/d": # a directory
            if theInode in inode2path_dic: continue # duplicate ??
            inode2path_dic[theInode] = pathSoFar+theName+"/"
            scanFilesystem(theInode, inode2path_dic, pathSoFar+theName+"/")
        elif theType=="r/r": inode2path_dic[theInode] = pathSoFar+theName

print "Finding the root directory"
i=0
while not is_valid_directory(i): i += 1

print "Scanning the root directory"
root_inode2path = {}
scanFilesystem(i,root_inode2path)

print "Looking for lost directory stubs"

recovered_directories = {}
while i &lt; 60000000:
    i += 1
    if i in root_inode2path or i in recovered_directories: continue # already known
    sys.stdout.write("Checking "+str(i)+"   \r") ; sys.stdout.flush()
    if not is_valid_directory(i): continue
    inode2path = root_inode2path.copy()
    scanFilesystem(i,inode2path)
    for n in inode2path.keys():
        if n in root_inode2path: continue # already known
        p = recover_to+"/lostdir-"+str(i)+"/"+inode2path[n]
        if p[-1]=="/": # a directory
            recovered_directories[n] = True
            continue 
        print "Recovering",p
        os.system('mkdir -p "'+p[:p.rindex("/")]+'"')
        os.system("icat -f fat "+image+" "+str(n)+' &gt; "'+p+'"')
</pre>

<p>
Note that the script might find a subdirectory <em>before</em> it finds its
parent directory. For example if you have a lost directory <tt>A</tt> which
has a subdirectory <tt>B</tt>, it is possible that the script will find
<tt>B</tt> first and recover it, then later when it finds <tt>A</tt> it
will recover <tt>A</tt>, re-recover <tt>B</tt>, and place the new copy of
<tt>B</tt> inside the recovered <tt>A</tt>, so you will end up with both
<tt>A</tt>, <tt>B</tt> and <tt>A/B</tt>.  You have to manually decide which
of the recovered directories are actually the top-level ones. The script
does not bother to check for <tt>..</tt> entries pointing to a directory's
parent, because these were not accurate on the FAT storage card I had (they
may be more useful on other filesystems). If you want you can modify the
script to first run the inode scan <em>to completion</em> without
recovering anything, then analyze them, discarding any top-level ones that
are also contained within others. However, running the scan to completion
is likely to take far longer than looking at the directories by hand.
</p>

<p>
As it is, you can interrupt the script once it has recovered what you want.
If Control-C does not work, use Control-Z to suspend it and do <tt>kill
%1</tt> or whatever number <tt>bash</tt> gave you when you suspended the
script.
</p>

<p>
This script can take several days to run through a large storage card. You
can speed it up by using <tt>dd</tt> to take an image of the card to the
hard disk (which likely has faster access times than a card reader); you
can also narrow the range of inodes that are scanned if you have some idea
of the approximate inode numbers of the lost directories (you can get such
an idea by using <tt>fls</tt> to check on directories that are still there
and that were created in about the same period of time as the lost ones).
</p>

<p>
After all the directories have been recovered, you can run <tt>fsck.vfat
-r</tt> and let it return the orphaned space back to the free space, and
then mount the card and copy the recovered directory structures back onto
it.
</p>

<p class="editorial">
Some GNU/Linux live CDs have a forensic mode that doesn't touch local
storage media. For example if you boot the <a href="http://grml.org/">GRML</a>
live CD you can select "forensic mode" and can safely inspect attached harddisks
or other media. AFAIK Knoppix has a similar option. -- Ren&eacute;
</p>

<br clear="all" />

<script type='text/javascript'>
digg_url = 'http://linuxgazette.net/172/brownss.html';
digg_title = 'Recovering FAT Directory Stubs with SleuthKit';
digg_bodytext = '<p> When I accidentally dropped an old Windows Mobile PocketPC onto the floor at the exact moment it was writing to a memory card, the memory card\'s master FAT was corrupted and several directories disappeared from the root directory.  Since it had not been backed up for some time, I connected the memory card to a Linux system for investigation.</p> ';
digg_topic = 'linux_unix';
</script>
<script src="http://digg.com/tools/diggthis.js" type="text/javascript"></script>

</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:172/brownss.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<hr>
<p>
<img align="left" alt="[BIO]" src="../gx/authors/brownss.jpg" class="bio">
</p>

<em>
<p>
Silas Brown is a legally blind computer scientist based in Cambridge UK.
 He has been using heavily-customised versions of Debian Linux since
 1999.
</p>



</em>

<br clear="all">


<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2010, <a href="../authors/brownss.html">Silas Brown</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 172 of Linux Gazette, March 2010
</p>

</div>
</div>


<div class="content lgcontent">

<a name="crosby"></a>
<h1>The Next Generation of Linux Games - GLtron and Armagetron Advanced</h1>
<p id="by"><b>By <a href="../authors/crosby.html">Dafydd Crosby</a></b></p>

</b>
</p>

<p>
<p>This month in NGLG, I'm taking a look at a couple of 3D games that don't
require too much horsepower, but are still fun diversions.</p>

<p>Lightcycle games should seem familiar to anyone who has played the
'snake' type games that are common on cell phones. The goal is to box in
your opponents with your trail, all the while making sure you don't slam
into <i>their</i> trails (as well as your own!). Often, the games mimic the
look and feel from the 1982 film <i>Tron</i>, and require quick reflexes
and a mind for strategy.</p>

<img src="misc/crosby/gltron-1.png" alt="GLtron screen shot" width="500" height="389"/>

<p>First up is GLtron, which has a good original soundtrack, highly
detailed lightcycles, and very simple controls. The programmers have done a
great job in making sure the game can be expanded with different artpacks,
and the game is quite responsive and fluid. The head programmer, Andreas
Umbach, gave an update on the game's site a few months ago about his plans
to picking the game back up, and it sounds like network play might be
coming out shortly. That's a good thing, because without network play the
game doesn't get quite the replay value that it should. There's a
split-screen multiplayer mode, but those with small screens will have a
hard time with it.</p>

<p>Looking for an online multiplayer version of Tron-clones, I came across
Armagetron Advanced. This was originally a fork of Armagetron, as the lead,
Manual Moos, had stopped updating the original. After the extension by
several open source contributors, Manual returned, and the fork officially
obsoleted the original Armagetron. If you've only played the original, you
might want to give the Advanced version a try.</p>

<img src="misc/crosby/armegetron-1.png" alt="Armgetron Advanced screen shot"
width="500" height="386"/>

<p>Armegetron Advanced not only has network multiplayer capability, but it
also has other gameplay elements that add to the fun of the game, such as
'death zones' and lines that diminish over time. Getting on a server was a
snap, and the lag was minimal (a necessity for this type of game). The
folks behind the game have done a fantastic job putting up documentation on
their site: not just on how to tweak the game for that extra little bit of
speed, but also the various tactics and strategies you can use to master
the game.</p>

<p>GLtron's <a href="http://www.gltron.org/">website</a> has the source and
other helpful information about the game, and it's likely that your
distribution already has this packaged. The Armagetron Advanced <a
href="http://armagetronad.net/">website</a> has server lists, a
well-maintained wiki, and packages for the major distros. </p>

<p>There are Mac OS X and Windows versions of these games, so you can play
along with your friends who might not use Linux.</p>

<p>If there's a game you would like to see reviewed, or have an update on
one already covered, just <a href="mailto:dtcrsby@gmail.com">drop me a
line</a>.</p>

<br clear="all" />

<script type='text/javascript'>
digg_url = 'http://linuxgazette.net/172/crosby.html';
digg_title = 'The Next Generation of Linux Games - GLtron and Armagetron Advanced';
digg_bodytext = '<p>This month in NGLG, I\'m taking a look at a couple of 3D games that don\'t require too much horsepower, but are still fun diversions.</p> ';
digg_topic = 'linux_unix';
</script>
<script src="http://digg.com/tools/diggthis.js" type="text/javascript"></script>

</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:172/crosby.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<hr>
<p>
<img align="left" alt="[BIO]" src="../gx/authors/crosby.jpg" class="bio">
</p>

<em>
<p>
Dafydd Crosby has been hooked on computers since the first moment his
 dad let him touch the TI-99/4A. He's contributed to various different
 open-source projects, and is currently working out of Canada.
</p>



</em>

<br clear="all">


<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2010, <a href="../authors/crosby.html">Dafydd Crosby</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 172 of Linux Gazette, March 2010
</p>

</div>
</div>


<div class="content lgcontent">

<a name="dyckoff"></a>
<h1>Away Mission - Recommended for March</h1>
<p id="by"><b>By <a href="../authors/dyckoff.html">Howard Dyckoff</a></b></p>

</b>
</p>

<p>
<p>
The Great Recession's impact continues to grow and there are fewer IT and
Open Source conferences on the horizon. However, three upcoming events are
worth mentioning - the annual RSA security confab at the beginning of
March, the Open Source Business Conference (OSBC), and EclipseCon.  All
three are in the SF Bay Area and you could spend a great month dropping in on
all three venues.  Of course, you will have to risk getting swabbed down by
TSA agents to get there. Security first.
</p>

<p>
This year, much of the discussion at RSA will focus on the Advanced
Pervasive Threat, or APT.  These are the increasingly sophisticated,
multi-pronged attacks and compromises from criminal syndicates and,
reportedly, from foreign state cyberwarfare agencies.  Perhaps WW-IV has
already started - but you'll have to come to this year's RSA to find out.
</p>

<p>
As always, or at least in recent years, a prominent security Fed will be
highlighted in a keynote.  This year, the high-profile industry experts
include:
</p>

<ul>
    <li> Robert Mueller, Director of the FBI
    <li> Richard Clarke, former U.S. Cyber Security Czar
    <li> Michael Chertoff, former U.S. Secretary of Homeland Security
    <li> P.W. Singer, Director of the Brookings Institution's 21st Century Defense Initiative 
</ul>

<p>
If you would like to attend, visit this link: <a
href="http://www.rsaconference.com/2010/usa/last-chance/prospects.htm">http://www.rsaconference.com/2010/usa/last-chance/prospects.htm</a>
</p>

<p>
On the other hand, some vendors like Tipping Point and Qualsys are offering
free expo passes that that will allow you to attend the Monday sessions
organized by the Trusted Computing Group (TCG). I definitely recommend
going to the Monday TCG sessions. It may be a little late for this deal,
but try code SC10TPP.
</p>

<p>
OSBC is in its 7th year and is the leading business-oriented venue for
discussing open source companies and business models.  However, some of the
sessions do seem to be updates from the previous years.
</p>

<p>
It will be held again at the attractive and compact Palace Hotel in
downtown San Francisco which is a bit of a tourist magnet by itself.  The
West exit of the Montgomery St. BART station brings you up right by the
entrance.
</p>

<p>
 For this year the keynote addresses look at the developing open source
market:
</p>

<ul>
    <li> Jim Whitehurst, CEO of Red Hat, will speak on evolving open source market. 
    <li>  Facebook insider David Recordon discusses how the popular social media site has scaled up using open source as a major part of its infrastructure.
    <li>  Tim O'Reilly will discuss the varied opportunities that open source presents. 
    <li> and Bob Sutor, IBM VP for Open Source and Linux, asks the hard questions about open source software and the communities that produce it.
</ul>
 
<p>
Here's the agenda for this year's Open Source Business Conference: <a
href="http://www.osbc.com/ehome/index.php?eventid=7578&amp;tabid=3659">http://www.osbc.com/ehome/index.php?eventid=7578&amp;tabid=3659</a>
</p>

<p>
And this link shows the sessions held in 2009: <a
href="http://www.infoworld.com/event/osbc/09/osbc_sessions.html">http://www.infoworld.com/event/osbc/09/osbc_sessions.html</a>
</p>

<p>
The EclipseCon schedule gets morphed for 2010.  It will offer EclipseCon
2010 talks on dozens of topics, going from 12-minute lightning talks,
which introduce new topics, to three-hour tutorials. For four days, there will be
morning hands-on tutorials, then technical sessions in the afternoon,
followed later by panel discussions.  This format might led to less
conference burnout.
</p>

<p>
EclipseCon this year will offer a new Unconference in addition to its usual
evening BOFs. The first three nights (Monday-Wednesday) will offer three
rooms with AV and a chance to present any Eclipse-relevant topic on a
first-come, first-served basis.  These presentations must be short (a
maximum of 25 minutes) and focused.
</p>

<p>
If you are quick, you can save a bit on the EclipseCon price - the advance
registration price is $1795 until March 19, 2010.  Here's the link to
register: <a href="http://www.eclipsecon.org/2010/registration/">http://www.eclipsecon.org/2010/registration/</a>
</p>

<p>
Last year's EclipseCon was enjoyable but compressed and filled with session
conflicts.  To view many, many recorded videos from last year's eclipse,
just go to <a
href="http://www.eclipsecon.org/2009">http://www.eclipsecon.org/2009</a>
and select a session to play.  I would recommend the sessions on Android
and the Eclipse Ecosystem, e4 - flexible resources for next generation
applications and tools, and Sleeping Around: Writing tools that work in
Eclipse, Visual Studio, Ruby, and the Web.
</p>


</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:172/dyckoff.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<!-- *** BEGIN bio *** -->
<hr>
<p>

<img align="left" alt="Bio picture" src="../gx/authors/dyckoff.jpg" class="bio">

<em>
<p>
Howard Dyckoff is a long term IT professional with primary experience at
Fortune 100 and 200 firms. Before his IT career, he worked for Aviation
Week and Space Technology magazine and before that used to edit SkyCom, a
newsletter for astronomers and rocketeers. He hails from the Republic of
Brooklyn [and Polytechnic Institute] and now, after several trips to
Himalayan mountain tops, resides in the SF Bay Area with a large book
collection and several pet rocks.
</p>

<p>
Howard maintains the <a
href="http://technology-events.blogspot.com">Technology-Events</a> blog at
blogspot.com from which he contributes the Events listing for Linux
Gazette. Visit the blog to preview some of the next month's NewsBytes
Events.
</p>

</em>
<br clear="all">
<!-- *** END bio *** -->

<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2010, <a href="../authors/dyckoff.html">Howard Dyckoff</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 172 of Linux Gazette, March 2010
</p>

</div>
</div>


<div class="content lgcontent">

<a name="grebler"></a>
<h1>Tunnel Tales 1</h1>
<p id="by"><b>By <a href="../authors/grebler.html">Henry Grebler</a></b></p>

</b>
</p>

<p>
<h3>Justin</h3>

<p> 
I met Justin when I was contracting to one of the world's biggest
computer companies, OOTWBCC, building Solaris servers for one of
Australia's biggest companies (OOABC). Justin is in EBR (Enterprise
Backup and Recovery). (OOTWBCC is almost certainly the world's most
prolific acronym generator (TWMPAG).) I was writing scripts to
automate much of the install of EBR.
</p>

<p>
To do a good job of developing automation scripts, one needs a test
environment. Well, to do a good job of developing just about anything,
one needs a test environment. In our case, there was always an
imperative to rush the machines we were building out the door and
into production (pronounced BAU (Business As Usual) at TWMPAG).
Testing on BAU machines was forbidden (fair enough).
</p>

<p>
Although OOTWBCC is a huge multinational, it seems to be reluctant to
invest in hardware for infrastructure. Our test environment consisted
of a couple of the client's machines. They were "network orphans",
with limited connectivity to other machines.
</p>

<p>
Ideally, one also wants a separate development environment, especially
a repository for source code. Clearly this was asking too much, so
Justin and I shrugged and agreed to use one of the test servers as a
CVS repository.
</p>

<p>
The other test machine was constantly being trashed and rebuilt from
scratch as part of the test process. Justin started to get justifiably
nervous. One day he came to me and said that we needed to back up the
CVS repository. "And while we're at it, we should also back up a few
other directories."
</p>

<p> 
Had this been one of the typical build servers, it would have had
direct access to all of the network, but, as I said before, this one
was a network orphan. <A HREF="#Diag1">Diagram 1</A> indicates the
relevant connectivity.
</p>

<a name="Diag1"></a>
<img src="misc/grebler/diag1.jpeg" alt="Diagram 1" width="582" height="347"/>

<pre>

test	the test machine and home of the CVS repository
laptop	my laptop
jump	an intermediate machine
backup	the machine which partakes in regular tape backup
</pre>

<p>
If we could get the backup data to the right directory on
<strong>backup</strong>, the corporate EBR system would do the rest.
</p>

<p>
The typical response I got to my enquiries was, "Just copy the stuff to
your laptop, and then to the next machine, and so on." If this were a
one-off, I <i>might</i> do that. But what's the point of a single
backup? If this activity is not performed at least daily, it will soon
be useless. Sure, I <i>could</i> do it manually. Would you?

<h3>Step by Step</h3>

<p>
I'm going to present the solution step by step. Many of you will find
some of this just motherhoods<a id="grebler.html_1_back"></a><a href="#grebler.html_1">[1]</a>. Skip ahead.
</p>

<p>
My laptop ran Fedora 10 in a VirtualBox under MS Windows XP. All my
useful work was done in the Fedora environment.
</p>


<h3>Two machines</h3>

<p>
If I want to copy a single file from the test machine to my laptop,
then, on the laptop, I would use something like:
</p>

<pre class="code">
	scp -p test:/tmp/single.file /path/to/backup_dir
</pre>

This would create the file <tt>/path/to/backup_dir/single.file</tt> on
my laptop.

<p>
To copy a whole directory tree <i>once</i>, I would use:

<pre class="code">
	scp -pr test:/tmp/top_dir /path/to/backup_dir
</pre>

This would populate the directory <tt>/path/to/backup_dir/top_dir</tt>.

</p>

<h3>Issues</h3>

<ul>
<li>
<p>
Why did I say "once"? <tt>scp</tt> is fine if you want to copy
a directory tree once. And it's fine if the directory tree is not
large. And it's fine if the directory tree is extremely volatile (ie
frequently changes completely (or pretty much)).
</p>

<p>
But what we have here is a directory tree which simply accumulates
incremental changes. I guess over 80% of the tree will be the same
from one day to the next. Admittedly, the tree is not large, and the
network is pretty quick, but even so, it's nice to do it the right way
- if possible.
</p>
</li>

<li>
<p>
There is another problem, potentially a much bigger problem.
The choice of <tt>scp</tt> or some other program is about
efficiency and elegance. This problem can be a potential roadblock:
permissions.
</p>

<p>
The way <tt>scp</tt> works, I have to log in to <strong>test</strong>.
But I can only directly log in as myself (my user id on
<strong>test</strong>). If I want root privileges I have to use
<tt>su</tt> or <tt>sudo</tt>. In either case, I'd have to supply
another password. I could do it that way, but it requires even
stronger magic than I'm using so far (and I think it could be a bit
less secure than the solution I plan to present).
</p>
</li>

<li>
<p>
Have another look at <A HREF="#Diag1">Diagram 1</A>. Notice the
arrows? Yes, Virginia, they really are one-way arrows. (The link
between <strong>jump</strong> and <strong>backup</strong> is probably
two-way in real life, but the exercise is more powerful if it's
one-way, so let's go with the diagram as it is.)
</p>

<p>
To get from my laptop to the test machine, I go via an SSH proxy,
which I haven't drawn because it would complicate the diagram
unnecessarily. A firewall might be set up the same way. In either
case, I can establish an SSH session from my laptop to the other
machine; but I can't do the reverse. It's like a diode.
</p>

<p>
I'm going to show you how an SSH tunnel allows access in the other
direction. Not only that, but it will make <strong>jump</strong>
directly accessible from <strong>test</strong> as well!
</p>
</li>

<li>
<p>
One final point about <tt>ssh/scp</tt>. If I do nothing special, when
I run those <tt>scp</tt> commands above, I'll get a prompt
like:

<pre>
	henry@test's password:
</pre>

and I will have to enter my password before the copy will take place.
That's not very helpful for an automatic process.
</p>

<p>

</p>
</li>
</ul>

<h3>Look, ma! No hands!</h3>

<p>
Whenever I expect to go to a machine more than once or twice, I take
the time to set up <tt>$HOME/.ssh/authorized_keys</tt> on the
destination machine. See <tt>ssh(1)</tt>. Instead of using passwords,
the SSH client on my laptop

<pre>
	proves that it has access to the private key and the server
	checks that the corresponding public key is authorized to
	accept the account.

				- ssh(1)
</pre>

It all happens "under the covers". I invoke <tt>scp</tt>, and the
files get transferred. That's convenient for me, absolutely essential
for a cron job.
</p>

<h3>Permissions</h3>

<p>
There's more than one way to skin this cat. I decided to use a cron
job on <strong>test</strong> to copy the required backup data to an
intermediate repository. I don't simply copy the directories, I
package them with <tt>tar</tt>, and compress the tarfile with
<tt>bzip2</tt>. I then make myself the owner of the result. (I could
have used <tt>zip</tt>.)
</p>

<p>
The point of the <tt>tar</tt> is to preserve the permissions of all
the files and directories being backed up. The point of the
<tt>bzip2</tt> is to make the data to be transferred across the
network, and later copied to tape, as small as possible.
(Theoretically, some of these commendable goals may be defeated to
varying degrees by "smart" technology. For instance,
<tt>rsync</tt> has the ability to compress; and most modern
backup hardware performs compression in the tape drive.) The point of
the <tt>chown</tt> is to make the package accessible to a cron job on
my laptop running as me (an unprivileged user on
<strong>test</strong>).
</p>

<p>
Here's the root crontab entry:

<pre>
0 12 * * * /usr/local/sbin/packitup.sh &gt;/dev/null 2&gt;&amp;1  # Backup
</pre>

At noon each day, as the root user, run a script called
<strong>packitup.sh</strong>:

<pre>
#! /bin/sh
#       packitup.sh - part of the backup system

# This script collates the data on test in preparation for getting it
# backed up off-machine.

# Run on:       test
# from:         cron or another script.

BACKUP_PATHS='
/var/cvs_repository
/etc
'
REPO=/var/BACKUPS
USER=henry

        date_stamp=`date '+%Y%m%d'`

        for dir in $BACKUP_PATHS
        do
                echo Processing $dir
                pdir=`dirname $dir`
                tgt=`basename $dir`
                cd $pdir
                outfile=$REPO/$tgt.$date_stamp.tar.bz2
                tar -cf - $tgt | bzip2 &gt; $outfile
                chown $USER $outfile
        done
        exit
</pre>

If you are struggling with any of what I've written so far, this
article may not be for you. I've really only included much of it for
completeness. Now it starts to get interesting.
</p>

<h3>rsync</h3>

<p>
Instead of <tt>scp</tt>, I'm going to use <tt>rsync</tt> which invokes
<tt>ssh</tt> to access remote machines. Both <tt>scp</tt> and
<tt>rsync</tt> rely on SSH technology; this will become relevant when
we get to the tunnels.
</p>

<p>
Basically, <tt>rsync(1)</tt> is like <tt>scp</tt> on steroids. If I
have a 100MB of data to copy and 90% is the same as before,
<tt>rsync</tt> will copy a wee bit more than 10MB, whereas
<tt>scp</tt> will copy all 100MB. Every time.
</p>

<a name="tunnelsf"></a>
<h3>Tunnels, finally!</h3>

<p> 
Don't forget, I've already set up certificates on all the remote
machines.
</p>

<p>
To set up a tunnel so that <strong>test</strong> can access
<strong>jump</strong> directly, I simply need:

<pre class="code">
	ssh -R 9122:jump:22 test
</pre>
</p>

<p>
Let's examine this carefully because it is the essence of this
article. The command says to establish an SSH connection to
<strong>test</strong>. "While you're at it, I want you to listen on a
port numbered <strong>9122</strong> on <strong>test</strong>. If
someone makes a connection to port <strong>9122</strong> on
<strong>test</strong>, connect the call through to port
<strong>22</strong> on <strong>jump</strong>." The result looks like
this:
</p>

<img src="misc/grebler/diag2.jpeg" alt="Diagram 2" width="582" height="347"/>

<p>
</p>

<p>
So, immediately after the command in the last box, I'm actually logged
in on <strong>test</strong>. If I now issue the command

<pre class="code">
	henry@test:~$ ssh -p 9122 localhost
</pre>

I'll be logged in on <strong>jump</strong>. Here's what it all looks
like (omitting a lot of uninteresting lines):

<pre class="code">
	henry@laptop:~$ ssh -R 9122:jump:22 test
	henry@test:~$ ssh -p 9122 localhost
Last login: Wed Feb  3 12:41:18 2010 from localhost.localdomain
	henry@jump:~$ 
</pre>
</p>

<p>
It's worth noting that you don't "own" the tunnel; anyone can use it.
And several sessions can use it concurrently. But it only exists while
your first <tt>ssh</tt> command runs. When you exit from
<strong>test</strong>, your tunnel disappears (and all sessions using
the tunnel are broken).
</p>

<p>
Importantly, by default, "the listening socket on the server will be
bound to the loopback interface only" - <tt>ssh(1)</tt>. So, by
default, a command like the following won't work:

<pre class="code">
	ssh -p 9122 test		# usually won't work
	ssh: connect to address 192.168.0.1 port 9122: Connection refused
</pre>
</p>


<p>
Further, look carefully at how I've drawn the tunnel. It's like that
for a reason. Although, <i>logically</i> the tunnel seems to be a
direct connection between the 2 end machines, <strong>test</strong>
and <strong>jump</strong>, the physical data path is via
<strong>laptop</strong>. You haven't managed to skip a machine; you've
only managed to avoid a <i>manual</i> step. There may be performance
implications. 
</p>

<h3>Sometimes I Cheat</h3>

<p>
The very astute amongst my readers will have noticed that this hasn't
solved the original problem. I've only tunneled to
<strong>jump</strong>; the problem was to get the data to
<strong>backup</strong>. I could do it using SSH tunnels, but until
next time, you'll have to take my word for it. Or work it out for
yourself; it should not be too difficult.
</p>

<p>
But, as these things sometimes go, in this case, I had a much simpler
solution:

<pre class="code">
	henry@laptop:~$ ssh jump
	henry@jump:~$ sudo bash
Password:
	root@jump:~# mount backup:/backups /backups
	root@jump:~# exit
	henry@jump:~$ exit
	henry@laptop:~$ 
</pre>

I've NFS-mounted the remote directory <strong>/backups</strong> on its
local namesake. I only need to do this once (unless someone reboots
<strong>jump</strong>). Now, an attempt to write to the directory
<strong>/backups</strong> on <strong>jump</strong> results in the data
being written into the directory <strong>/backups</strong> on
<strong>backup</strong>.
</p>

<h3>The Final Pieces</h3>

<p>
Ok, in your mind, log out of all the remote machines mentioned in 
<A HREF="#tunnelsf">Tunnels, finally!</A>. In real life, this is going
to run as a cron job.
</p>

<p>
Here's my (ie user <strong>henry</strong>'s) crontab entry on
<strong>laptop</strong>:

<pre class="code">
	30 12 * * * /usr/local/sbin/invoke_backup_on_test.sh
</pre>

At 12:30 pm each day, as user <strong>henry</strong>, run a script
called <strong>invoke_backup_on_test.sh</strong>:

<pre class="code">
#! /bin/sh
#       invoke_backup_on_test.sh - invoke the backup

# This script should be run from cron on laptop.

# Since test cannot access the backup network, it cannot get to the
# real "backup" directly. An ssh session from "laptop" to "test"
# provides port forwarding to allow ssl to access the jump machine I
# have nfs-mounted /backups from "backup" onto the jump machine.

# It's messy and complicated, but it works.

        ssh -R 9122:jump:22 test /usr/local/sbin/copy2backup.sh
</pre>
</p>

<p>
Of course I had previously placed <strong>copy2backup.sh</strong> on
<strong>test</strong>.

<pre class="code">
#! /bin/sh
#       copy2backup.sh - copy files to be backed up

# This script uses rsync to copy files from /var/BACKUPS to
# /backups on "backup".

# 18 Sep 2009 Henry Grebler    Perpetuate (not quite the right word) pruning.
# 21 Aug 2009 Henry Grebler    Avoid key problems.
#  6 Aug 2009 Henry Grebler    First cut.
#=============================================================================#

# Note. Another script, prune_backups.sh, discards old backup data
# from the repo. Use rsync's delete option to also discard old backup
# data from "backup".

        PATH=$PATH:/usr/local/bin

# Danger lowbrow: Do not put tabs in the next line.

        RSYNC_RSH="ssh -o 'NoHostAuthenticationForLocalhost yes' \
                -o 'UserKnownHostsFile /dev/null' -p 9122"
        RSYNC_RSH="`echo $RSYNC_RSH`"
        export RSYNC_RSH

        rsync -urlptog --delete --rsync-path bin/rsync /var/BACKUPS/ \
                localhost:/backups
</pre>
</p>


<h3>Really important stuff</h3>

<p>
Notes on <strong>copy2backup.sh</strong>.
</p>

<ul>
<li>
<pre class="code">
	PATH=$PATH:/usr/local/bin
</pre>
The way that <strong>copy2backup.sh</strong> is invoked (on
<strong>test</strong>) from cron (on <strong>laptop</strong>) via
<strong>invoke_backup_on_test.sh</strong> means that you should not
count on any but the most basic of items in PATH. Even safer, would be
to define even things like <tt>/bin</tt>.

</li>

<li>
<pre class="code">
	RSYNC_RSH=...
	...
	export RSYNC_RSH
</pre>

These lines provide rsync with with details of the <tt>rsh</tt>
command (in this case, <tt>ssh</tt>) to run. Depending on which
version of <tt>ssh</tt> your machine has, and the options set in the
various SSH config files, your <tt>ssh</tt> may try to keep track of
the certificates of the SSH daemons on the remote machines. Using
<strong>localhost</strong> the way that we do here, the actual machine
at the end of the tunnel (and therefore its fingerprint or
certificate) may change from one run to the next. <tt>ssh</tt> will
try to protect you from the possibility of certain known forms of
attack. These incantations try to get <tt>ssh</tt> to keep out of the
way. It's safe enough on an internal private network; more risky if
you are venturing into the badlands of the Internet.

</li>
<li>
<tt>rsync</tt> is a pretty powerful program. Its options and
arguments can be complicated. I do not propose to cover chapter and
verse here. Check the man page, <tt>rsync(1)</tt>. I will just say that the
trailing slash in the "from" argument (<strong>/var/BACKUPS/</strong>)
<i>is</i> significant. It says to copy the <i>contents</i> of the
specified directory. Omitting the trailing slash would mean to copy
the directory. Recursion is specified in an earlier option (-r).
</li>

<li>
<pre class="code">
	--rsync-path bin/rsync
</pre>

When <tt>rsync</tt> runs on the local machine (in this case,
<strong>test</strong>), it makes an SSH connection to the remote
machine ("<strong>localhost</strong>" = <strong>jump</strong>) and
tries to run an <tt>rsync</tt> on the remote machine. This argument
indicates where to find the remote <tt>rsync</tt>. In this case, it
will be in the <strong>bin</strong> subdirectory of my (user
<strong>henry</strong>'s) HOME directory on <strong>jump</strong>. In
other words, I'm running a private copy of <tt>rsync</tt>.

<li>
<strong>prune_backups.sh</strong> and <strong>--delete</strong> -
these two components go together. They can be dangerous. I'll explain later.
</li>
</ul>

<h3>Recap</h3>

<ul>
<li>
Everyday at noon <strong>packitup.sh</strong> on
<strong>test</strong> gathers the data to be backed up into a local
holding repository.
</li>

<li>
Everyday, half an hour later, if my laptop is turned on, a local
script, <strong>invoke_backup_on_test.sh</strong> is invoked. It
simply connects to <strong>test</strong>, establishing an SSH tunnel
as it does, and invokes the script which performs the backup,
<strong>copy2backup.sh</strong>.
</li>

<li>
<strong>copy2backup.sh</strong> does the actual copy over the SSH
tunnel using <tt>rsync</tt> to transport the data.
</li>

<li>
When <strong>copy2backup.sh</strong> completes, it exits, causing the
<tt>ssh</tt> command to exit and the SSH tunnel to be torn down.
</li>

<li>
Next day, it all starts over again.
</li>
</ul>

<h3>Wrinkles</h3>

<p>
It's great when you finally get something like this to work. All the
pieces fall into place - it's very satisfying.
</p>

<p>
Of course, you monitor things carefully for the first few days. Then
you check less frequently. You start to gloat.
</p>

<p>
... until a few weeks elapse and you gradually develop a gnawing
concern. The data is incrementally increasing in size as more days
elapse. At first, that's a good thing. One backup good, two backups
better, ... Where does it end? Well, at the moment, it doesn't. Where
should it end? Good question. But, congratulations on realising that
it probably should end.
</p>

<p>
When I did, I wrote <strong>prune_backups.sh</strong>. You can see
when this happened by examining the history entries in
<strong>copy2backup.sh</strong>: about 6 weeks after I wrote the first
cut. Here it is:

<pre class="code">
#! /bin/sh
#	prune_backups.sh - discard old backups

# 18 Sep 2009 Henry Grebler    First cut.
#=============================================================================#

# Motivation

# packitup.sh collates the data on test in preparation for getting
# it backed up off-machine. However, the directory just grows and
# grows. This script discards old backup sets.

#----------------------------------------------------------------------#

REPO=/var/BACKUPS
KEEP_DAYS=28		# Number of days to keep

	cd $REPO
	find $REPO -type f -mtime +$KEEP_DAYS -exec rm {} \;
</pre>

Simple, really. Just delete anything that is more than 28 days old. NB
<strong>more than</strong> rather than <strong>equal to</strong>. If for
some reason the cron job doesn't run for a day or several, when next
it runs it will catch up. This is called <strong>self-correcting</strong>.
</p>

<p>
Here's the crontab entry:
<pre class="code">
	0 10 * * * /usr/local/sbin/prune_backups.sh &gt;/dev/null 2&gt;&amp;1
</pre>

At 10 am each day, as the root user, run a script called
<strong>prune_backups.sh</strong>. 
</p>


<p>
But, wait. That only deletes old files in the repository on
<strong>test</strong>. What about the copy of this data on
<strong>jump</strong>?!
</p>

<p>
Remember the <strong>--delete</strong> above? It's an rsync option; a
very dangerous one. That's not to say that you shouldn't use it; just
use it with extra care.
</p>

<p>
It tells <tt>rsync</tt> that if it discovers a file on the destination
machine that is not on the source machine, then it should delete the
file on the destination machine. This ensures that the local and
remote repositories stay truly in sync.
</p>

<p>
However, if you screw it up by, for instance, telling <tt>rsync</tt>
to copy an empty directory to a remote machine's populated directory,
and you specify the <strong>--delete</strong> option, you'll delete
all the remote files and directories. You have been warned: use it
with extra care.
</p>

<h3>Risks and Analysis</h3>

<p>
There is a risk that port 9122 on <strong>test</strong> may be in use
by another process. That happened to me a few times. Each time, it
turned out that I was the culprit! I solved that by being more
disciplined (using another port number for interactive work).
</p>

<p>
You could try to code around the problem, but it's not easy. 

<pre class="code">
	ssh -R 9122:localhost:22 fw
	Warning: remote port forwarding failed for listen port 9122
</pre>

Even though it could not create the tunnel (aka port forwarding), ssh
has established the connection. How do you know if port forwarding
failed? 
</p>

<p>
More recent versions of <tt>ssh</tt> have an option which caters for
this: <tt>ExitOnForwardFailure</tt>, see <tt>ssh_config(5)</tt>.
</pre>


<p>
If someone else has created a tunnel to the right machine, it doesn't
matter. The script will simply use the tunnel unaware that it is
actually someone else's tunnel.
</p>

<p>
But if the tunnel connects to the wrong machine?
</p>

<p>
Hey, I don't provide <i>all</i> the answers; I simply mention the risks,
maybe make some suggestions. In my case, it was never a serious
problem. Occasionally missing a backup is not a disaster. The scripts
are all written to be tolerant to the possibility that they may not
run every day. When they run, they catch up.
</p>

<p>
A bigger risk is the dependence on my laptop. I tried to do something
about that but without long-term success. I'm no longer there; the
laptop I was using will have been recycled.
</p>

<p>
I try to do the best job possible. I can't always control my environment.
</p>

<h3>Debugging</h3>

<p>
Because this setup involves cron jobs invoking scripts which in turn
invoke other scripts, this can be a nightmare to get right. (Once it's
working, it's not too bad.) 
</p>

<p>
My recommendation: run the pieces by hand.
</p>

<p>
So start at a cron entry (which usually has output redirected to
<tt>/dev/null</tt>) and invoke it manually (as the relevant user)
<i>without</i> redirecting the output.
</p>

<p>
If necessary, repeat, following the chain of invoked scripts. In other
words, for each script, invoke each command manually. It's a bit
tiresome, but none of the scripts is very long. Apart from the comment
lines, they are all very dense. The best example of density is the
<tt>ssh</tt> command which establishes the tunnel.
</p>

<p>
Use your mouse to copy and paste for convenience and to avoid
introducing transcription errors.
</p>

<h3>Coming Up</h3>

<p>
That took much longer than I expected. I'll leave at least one other
example for another time.
</p>

<hr>

<p>
<a id="grebler.html_1"></a><a href="#grebler.html_1_back">[1]</a>
A UK/AU expression, approximately "boring stuff you've heard before".<br>
 -- Ben
</p>


<br clear="all" />

<script type='text/javascript'>
digg_url = 'http://linuxgazette.net/172/grebler.html';
digg_title = 'Tunnel Tales 1';
digg_bodytext = '<p>  I met Justin when I was contracting to one of the world\'s biggest computer companies, OOTWBCC, building Solaris servers for one of Australia\'s biggest companies (OOABC). Justin is in EBR (Enterprise Backup and Recovery). (OOTWBCC is almost certainly the world\'s most prolific acronym generator (TWMPAG).) I was writing scripts to automate much of the install of EBR. </p> ';
digg_topic = 'linux_unix';
</script>
<script src="http://digg.com/tools/diggthis.js" type="text/javascript"></script>

</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:172/grebler.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<hr>
<p>
<img align="left" alt="[BIO]" src="../gx/authors/grebler.jpg" class="bio">
</p>

<em>
<p>
Henry was born in Germany in 1946, migrating to Australia in 1950. In
 his childhood, he taught himself to take apart the family radio and put
 it back together again - with very few parts left over.
</p>

<p>
After ignominiously flunking out of Medicine (best result: a sup in
 Biochemistry - which he flunked), he switched to Computation, the name
 given to the nascent field which would become Computer Science. His
 early computer experience includes relics such as punch cards, paper
 tape and mag tape.
</p>

<p>
He has spent his days working with computers, mostly for computer
 manufacturers or software developers. It is his darkest secret that he
 has been paid to do the sorts of things he would have paid money to be
 allowed to do. Just don't tell any of his employers.
</p>

<p>
He has used Linux as his personal home desktop since the family got its
 first PC in 1996. Back then, when the family shared the one PC, it was a
 dual-boot Windows/Slackware setup. Now that each member has his/her own
 computer, Henry somehow survives in a purely Linux world.
</p>

<p>
He lives in a suburb of Melbourne, Australia.
</p>



</em>

<br clear="all">


<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2010, <a href="../authors/grebler.html">Henry Grebler</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 172 of Linux Gazette, March 2010
</p>

</div>
</div>


<div class="content lgcontent">

<a name="peterson"></a>
<h1>Logging to a Database with Rsyslog</h1>
<p id="by"><b>By <a href="../authors/peterson.html">Ron Peterson</a></b></p>

</b>
</p>

<p>
<h3 id="intro">Introduction</h3>

<p>
<a href="http://www.rsyslog.com/">RSyslog</a> extends and improves on
the venerable syslogd service.  It supports the standard configuration
syntax of its predecessor, but offers a number of more advanced
features.  For example, you can construct advanced filtering expressions
in addition to the simple and limiting facility.priority selectors.  In
addition to the usual log targets, you can also write to a number of
different databases.  In this article, I'm going to show you how to
combine these features to capture specific information to a database.
In addition, I'll show you how to use trigger functions to parse the log
messages into a more structured format.
</p>

<h3 id="prereq">Prerequisites</h3>

<p>
Obviously you'll need to have rsyslog installed.  My examples will be
constructed using the packaged version of rsyslog available in the
current Debian Stable (Lenny).  You'll also need the plugin module
for writing to PostgreSQL.
</p>

<pre class="code">
1122# dpkg -l "*rsyslog*" | grep ^i
ii  rsyslog        3.18.6-4  enhanced multi-threaded syslogd
ii  rsyslog-pgsql  3.18.6-4  PostgreSQL output plugin for rsyslog
</pre>

<p>
I'll be using PostgreSQL version 8.4.2, built with plperl support.  I'm
using plperl to write my trigger functions, to take advantage of Perl's
string handling.
</p>

<p>
I'm not going to go into any detail about how to get these tools
installed and running, as there are any number of good resources (see
below) already available to help with that.
</p>

<h3 id="rsyslog">RSyslog Configuration</h3>

<p>
My distribution puts the RSyslog configuration files in two places.
It all starts with /etc/rsyslog.conf.  Near the top of this file, there
is a line like this, which pulls additional config files out of the
rsyslog.d directory:
</p>

<pre class="code">
$IncludeConfig /etc/rsyslog.d/*.conf
</pre>

<p>
I'm going to put my custom RSyslog configuration in a file called
/etc/rsyslog.d/lg.conf.  We're going to use this file to do several
things:
</p>

<ul>
  <li>Load the database driver module</li>
  <li>Configure RSyslog to buffer database output</li>
  <li>Define a template for mapping syslog output to the database</li>
  <li>Define filter expressions to net just the log messages we care about</li>
</ul>

<p>
Let's start with something easy - loading the database module.
</p>

<pre class="code">
$ModLoad ompgsql.so
</pre>

<p>
That wasn't too bad now, was it?  :)
</p>

<p>
So let's move on.  The next thing we'll do is configure RSyslog to
buffer the output headed for the database.  There's
a <a href="http://www.rsyslog.com/doc-rsyslog_high_database_rate.html">good
section</a> about why and how to do this in the RSyslog documentation.
The spooling mechanism we're configuring here allows RSyslog to queue
database writes during peak activity.  I'm also including a commented
line that shows you something you should <i>not</i> do.
</p>

<pre class="code">
$WorkDirectory /var/tmp/rsyslog/work

# This would queue _ALL_ rsyslog messages, i.e. slow them down to rate of DB ingest.
# Don't do that...
# $MainMsgQueueFileName mainq  # set file name, also enables disk mode

# We only want to queue for database writes.
$ActionQueueType LinkedList # use asynchronous processing
$ActionQueueFileName dbq    # set file name, also enables disk mode
$ActionResumeRetryCount -1   # infinite retries on insert failure
</pre>

<p>
Now we're going to define an RSyslog template that we'll refer to in
just a minute when we write our filter expression.  This template
describes how we're going to stuff log data into a database table.  The
%percent% quoted strings are RSyslog 'properties'.  A list of the
properties you might use can be found in
the <a href="http://www.rsyslog.com/doc-property_replacer.html">fine
documentation</a>.  You'll see why I call this 'unparsed' in just a
moment.
</p>

<pre class="code">
$template mhcUnparsedSSH, \
          "INSERT INTO unparsed_ssh \
          ( timegenerated, \
            hostname, \
            tag, \
            message ) \
          VALUES \
          ( '%timegenerated:::date-rfc3339%', \
            '%hostname%', \
            '%syslogtag%', \
            '%msg%' );", \
          stdsql
</pre>

<p>
All that's left to do on the RSyslog side of things is to find some
interesting data to log.  I'm going to log SSH login sessions.
</p>

<pre class="code">
if $syslogtag contains 'sshd' and $hostname == 'ahost' then :ompgsql:localhost,rsyslog,rsyslog,topsecret;mhcUnparsedSSH
</pre>

<p>
This all needs to be on one line.  It probably helps at this juncture
to look at some actual log data as it might appear in a typical log file.
</p>

<pre class="code">
Feb  2 10:51:24 ahost imapd[16376]: Logout user=ausername host=someplace.my.domain [10.1.56.8]
Feb  2 10:51:24 ahost sshd[17093]: Failed password for busername from 10.1.34.3 port 43576 ssh2
Feb  2 10:51:27 ahost imapds[17213]: Login user=cusername host=anotherplace.outside.domain [256.23.1.34]
Feb  2 10:51:27 ahost imapds[17146]: Killed (lost mailbox lock) user=dusername host=another.outside.domain [256.234.34.1]
Feb  2 10:51:27 ahost sshd[17093]: Accepted password for busername from 10.1.34.3 port 43576 ssh2
</pre>

<p>
This is real log data, modified to protect the innocent, from
/var/log/auth.log on one of my servers.  In a standard syslog setup,
this data would be captured with a configuration entry for the 'auth'
facility.  As you can see, it contains authorization information for
both IMAP and SSH sessions.  For my current purposes, I only care about
SSH sessions.  In a standard syslog setup, teasing this information
apart can be a real pain, because you only have so many facility.
selectors to work with.  With RSyslog, you can
write <a href="http://www.rsyslog.com/doc-rsyslog_conf_filter.html">advanced
filtering expressions</a> to help you capture just what you want.
</p>

<p>
In my case, I want to grab all log entries where the syslog tag
contains 'sshd' which originate from host 'ahost'.  The 'then' portion
of my expression says what to do with the data, namely, to use the
ompgsql driver and the mhcUnparsedSSH template to stuff it into the
'rsyslog' database found at 'localhost' as user 'rsyslog', password
'topsecret'.
</p>

<p>
The interesting information about these connections exists in the
message section, i.e. - the part specified by %msg% in my template.
This corresponds to all of the text after the syslog tag's colon:
</p>

<pre class="code">
Failed password for busername from 10.1.34.3 port 43576 ssh2
Accepted password for busername from 10.1.34.3 port 43576 ssh2
</pre>

<p>
We have our data in a database at this point.  We could just stop
where we are.  I want to take this a little farther, though.  I want to
break the message text down into the parts and pieces I care about, and
put it into a more structured table.  So let's turn to the database side
of things to see what we can do.
</p>

<h3 id="postgres">All Hail PostgreSQL</h3>

<p>
I'm going to create a database with two tables.  The first table
corresponds to the table we're referring to with our RSyslog template.
The second table will be a little more structured.  We will then write a
trigger function for the first table.  When a new row is added to our
first table, this trigger function will parse the data, tease it apart,
and put the constituent pieces into our second table.  Our tables will
look like this:
</p>

<pre class="code">
CREATE TABLE unparsed_ssh (
  timegenerated
    TIMESTAMP WITH TIME ZONE,
  hostname
    VARCHAR(12),
  tag
    TEXT,
  message
    TEXT
);

CREATE TABLE authlog (
  timegenerated
    TIMESTAMP WITH TIME ZONE,
  application_host
    VARCHAR(12),
  tag
    VARCHAR(24),
  application
    VARCHAR(24),
  origin_ip
    INET,
  username
    VARCHAR(24),
  oper
    VARCHAR(6)
    CHECK( oper IN ('login', 'logout') ),
  success
    BOOL
);
</pre>

<p>
You might imagine other fields that would be interesting to have in
the authlog table, but that starts to get off point.
</p>

<p>
I'm going to lay the trigger function on you all at once.  I'm using
plperl, because as I previously mentioned, perl makes short work of
string manipulation.  There is plenty of information about plperl in
the <a href="http://www.postgresql.org/docs/8.4/static/plperl.html">PostgreSQL
docs</a>. A few words to help you get your bearings.  The $_TD-&gt;{new}
variables refer to the trigger data made available to the function when
it is called.  You can see that these variables refer to the columns of
the table that RSyslog is stuffing data into.  The rest of the function
simply pulls the message text apart, and then constructs an INSERT sql
statement which pushes our parsed data into our second table.
</p>

<pre class="code">
CREATE OR REPLACE FUNCTION
  parse_ssh_log()
RETURNS TRIGGER AS
$$
  my $timegenerated = $_TD-&gt;{new}{timegenerated};
  my $hostname = $_TD-&gt;{new}{hostname};
  my $tag = $_TD-&gt;{new}{tag};
  my $message = $_TD-&gt;{new}{message};

  my $query;
  my $rv;
  my $user;
  my $ip;
  my $method;

  if( $message =~ /.*?(Failed|Accepted).*?for (.*?) from (.*?) .*/ ) {
    $success = $1;
    $user = $2;
    $ip = $3;
    if( $success eq "Failed" ) {
      $success = "false";
    } else {
      $success = "true";
    }
    $query = &lt;&lt;EOSQL;
INSERT INTO authlog ( timegenerated,
                      application_host,
                      tag,
                      application,
                      origin_ip,
                      username,
                      oper,
                      success )
VALUES ( '$timegenerated',
         '$hostname',
         '$tag',
         'ssh',
         '$ip',
         '$user',
         'login',
         '$success' );
EOSQL
    $rv = spi_exec_query( $query );
    return "SKIP";
  }
  return;
$$ LANGUAGE 'plperl';

CREATE TRIGGER parse_ssh_trigger
BEFORE INSERT ON unparsed_ssh
FOR EACH ROW EXECUTE PROCEDURE parse_ssh_log();
</pre>

<p>
Note that I have two different types of 'return' statement.  The
'return "SKIP"' tells the trigger to throw away the original row.  In
other words, our ssh log entries never actually land in the first table
at all.  That table essentially only exists as a placeholder for our
trigger function.  The final return is only called if our regular
expression fails to match.  Since it does not SKIP the insertion, any
row our function doesn't match will end up in our first table.  This is
a good way to check that you are capturing what you think you are.
</p>

<hr><h3 id="conclusion">Conclusion</h3>

<p>
Why do this?  Here's a real world example.  In my work, it's good to
be alerted about things like multiple login failures.  If the same
username is failing to log in from multiple different IP addresses, for
example, that usually indicates someone is attempting to break into
their account.  If someone is successfully logging in from multiple
widely separated IP addresses, is often means they have already been
broken into.  Unfortunately, we have multiple authentication systems,
which makes it difficult to watch all of this activity.  Using RSyslog,
I'm pulling log data from multiple different systems - ssh, imap, ldap,
etc. - into a single structured database.  This enhances our forensic
capabilities.
</p>

<p>
Of course, it's easy enough to simply push a bunch of syslog data to a
central server to consolidate the information in a central location.
That's what I'm doing here also, but rather than simply writing the log
data to a file, I'm using a database.  Often, 'grep' and friends are all
you need.  But a database lets you easily do more sophisticated queries.
How would you grep a specific time interval, for example?  Here's an simple
example query, written as a shell script:
</p>

<pre>
#! /bin/bash
export PGPASSWORD='topsecret';

psql -U rsyslog -d rsyslog -h localhost &lt;&lt;EOF
SELECT timegenerated, username, origin_ip, application_host,
application
FROM authlog
WHERE username = '$1' AND
      timegenerated &lt;= CURRENT_TIMESTAMP - '24 hours'::INTERVAL
ORDER BY timegenerated DESC;
EOF
</pre>

<p>
This is really the whole point of this exercise: being able to use
simple SQL statements to make it easier to do more advanced reporting.
</p>

<p>
I hope you found this article helpful.  Happy hacking.
</p>

<hr><h3 id="resources">Resources</h3>

<ul>
<li><a href="http://www.rsyslog.com/">RSyslog</a></li>
<li><a href="http://www.postgresql.org/docs/">Official PostgreSQL Documentation</a>
<li>Bruce Momjian's excellent <a href="http://www.postgresql.org/docs/books/awbook.html">PostgreSQL Book</a>
<li><a href="http://wiki.postgresql.org/wiki/Main_Page">PostgreSQL Community Wiki</a></li>
<li><a href="http://www.emacswiki.org/cgi-bin/wiki?SqlMode">SQL Mode for Emacs</a></li>
</ul>

<br clear="all" />

<script type='text/javascript'>
digg_url = 'http://linuxgazette.net/172/peterson.html';
digg_title = 'Logging to a Database with Rsyslog';
digg_bodytext = '<p> <a href="http://www.rsyslog.com/">RSyslog</a> extends and improves on the venerable syslogd service.  It supports the standard configuration syntax of its predecessor, but offers a number of more advanced features.  For example, you can construct advanced filtering expressions in addition to the simple and limiting facility.priority selectors.  In addition to the usual log targets, you can also write to a number of different databases.  In this article, I\'m going to show you how to combine these features to capture specific information to a database. In addition, I\'ll show you how to use trigger functions to parse the log messages into a more structured format. </p> ';
digg_topic = 'linux_unix';
</script>
<script src="http://digg.com/tools/diggthis.js" type="text/javascript"></script>

</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:172/peterson.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<!-- *** BEGIN bio *** -->
<hr>
<p>
<img align="left" alt="Bio picture" src="../gx/authors/peterson.jpg" class="bio">
</p>
<p>
<em>

Ron Peterson is a Network & Systems Manager at Mount Holyoke College in
the happy hills of western Massachusetts. He enjoys lecturing his three
small children about the maleficent influence of proprietary media
codecs while they watch Homestar Runner cartoons together.

</em>
</p>
<br clear="all">
<!-- *** END bio *** -->

<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2010, <a href="../authors/peterson.html">Ron Peterson</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 172 of Linux Gazette, March 2010
</p>

</div>
</div>


<div class="content lgcontent">

<a name="silva"></a>
<h1>Running Quake 3 and 4 on Fedora 12 (64-bit)</h1>
<p id="by"><b>By <a href="../authors/silva.html">Anderson Silva</a></b></p>

</b>
</p>

<p>
<p>
I started writing this article one night in December when I was bored and
looking for a game to play on Linux. I am not much of a computer gamer, so
I am not really current with what is out there for games in the Open Source
world. That night I stumbled across <a href=http://openarena.ws/ id=oz-t
title=OpenArena>OpenArena</a>, which is "a network enabled multiplayer
first person shooter based on the ioquake3 fork of the id tech 3 engine."
[<a href=http://openarena.wikia.com/wiki/Main_Page id=gzg1 title=1>1</a>]
</p>

<p>
I installed OpenArena on my Fedora 12 x86_64 install (i.e. yum install
openarena), and played it for a few minutes. That's how long it took for me
to travel back in time and have the urge to play some good-old Quake 3
Arena. I purchased my very own copy of 'Quake 3 Arena for Linux' back in
December of 1999 when it was released. Do you remember <a
href=http://www.lokigames.com/ id=aw4g title="Loki Game">Loki Games</a>?
[<strong>Note</strong>: Can you believe that this game is now over 10 years old?!]
</p>

<p>
<div style=TEXT-ALIGN:center>
  <img src="misc/silva/dgqqst99_55hwsbzhhp_b.jpg" style=HEIGHT:280px;WIDTH:280px>
</div>
</p>

<p>
I decided then to dig up my old Quake 3 CD, and see if it would still run
on my 'shiny' Fedora 12 x86_64 install. At first, I got all sorts of
errors, I couldn't install it from the original CD, nor by downloading the
most recent binaries from ID Software, which believe it or not, is already
6 years old.
</p>

<p>
<div style=TEXT-ALIGN:center>
  <img src="misc/silva/EXTERN_0000.png" style="HEIGHT:250px;TEXT-ALIGN:center;WIDTH:400px">
</div>
</p>

<p>
Anyway, I was able to get Quake 3 running on my Lenovo Thinkpad T500 (Intel
Graphics Card) running Fedora 12 x86_64, and here's what I had to do:
</p>


<h3>Installation:</h3>
<p>
<strong>1.</strong> Grab the two latest updates from ID Software's FTP: <a
href=ftp://ftp.idsoftware.com/idstuff/quake3/linux/linuxq3apoint-1.32b-3.x86.run>Q3A
Point Release 1.32b</a> and <a
href=ftp://ftp.idsoftware.com/idstuff/quake3/quake3-1.32c.zip> Q3A Point
Release 1.32c</a>
</p>

<pre class="code">
wget ftp://ftp.idsoftware.com/idstuff/quake3/linux/linuxq3apoint-1.32b-3.x86.run
wget ftp://ftp.idsoftware.com/idstuff/quake3/quake3-1.32c.zip
</pre>

<p>
<strong>2.</strong> Then make linuxq3apoint-1.32b-3.x86.run executable:
</p>

<pre class="code">
chmod 755 linuxq3apoint-1.32b-3.x86.run
</pre>

<p>
<strong>3.</strong> Execute it:
</p>

<pre class="code">
linux32 ./linuxq3apoint-1.32b-3.x86.run
</pre>

<p>
[<strong>Note</strong>: <i>linux32</i> tells Linux to execute
linuxq3apoint-1.32b-3.x86.run with CPU architecture set to 32 bit]
</p>

<p>
<strong>4.</strong> Unzip 1.32c, and copy over the binaries under the
'linux' directory to /usr/local/games/quake3:
</p>

<pre class="code">
unzip quake3-1.32c.zip
cp Quake III Arena 1.32c/linux/* /usr/local/games/quake3/
</pre>

<p>
<strong>5.</strong> Grab your original Quake 3 Arena CD, and copy over the
base3q files to /usr/local/games/quake3/base3q/
</p>

<p>
[<strong>Note</strong>: This will work with an original Quake 3 Arena CD
for Linux or Windows]
</p>


<h3> Fixing Sound: </h3>
<p>
Before you even try to start up the game, I can tell you right now the
sound will be broken; here's how to fix it. Quake 3 Arena needs the
/dev/dsp device to be present on the file system:
</p>

<p>
<strong>1.</strong> As root: 
</p>

<pre class="code">
/sbin/modprobe snd_pcm_oss
</pre>

<p>
[<strong>Note</strong>: This is non-persistent, through this module /dev/dsp is created]
</p>

<p>
<strong>2.</strong> Set permissions:
</p>

<pre class="code">
chmod 777 /proc/asound/card0/pcm0p/oss
</pre>

<h3> Starting the game: </h3>
<p>
Now we are ready to give Quake 3 Arena a try, but before you can even
execute the binary, there are a couple of things you need to be aware of:
</p>

<p>
<strong>1.</strong> Need to pass some parameters to the kernel so sound will work.<br>
<strong>2.</strong> Need to execute the binary under the 32-bit architecture.
</p>

<pre class="code">
echo "quake3-smp.x86 0 0 direct" &gt; /proc/asound/card0/pcm0p/oss &amp;&amp; linux32 /usr/local/games/quake3/quake3-smp
</pre>

<p>
[<strong>Note</strong>: I am running the SMP binary, since my laptop has 2
cores. If you run on a single processor, you will need to run]
</p>

<pre class="code">
echo "quake3.x86 0 0 direct" &gt; /proc/asound/card0/pcm0p/oss &amp;&amp; linux32 /usr/local/games/quake3/quake3
</pre>

<p>
<div style=TEXT-ALIGN:center>
<img src="misc/silva/EXTERN_0001.png" style="HEIGHT:250px;MARGIN:0px auto 10px;TEXT-ALIGN:center;WIDTH:400px"><p>
</div>
  
<h3> Troubleshooting Tips: </h3>
<p>
<strong>1.</strong>If your game freezes seconds after starting a match, use
'+set s_musicvolume -1' when starting the game.<br>
<strong>2.</strong> If the sound still not working after setting the
parameter above: Open your 'Sound Preferences' under GNOME and under the
Hardware tab, change the profile from Analog to Digital... it did the trick
for me.<br>
<strong>3. </strong>If you are using nvidia cards:
</p>

<pre class="code">
yum install xorg-x11-drv-nvidia-libs-32bit # from rpmfusion.org
</pre>

<p>
<strong>4. </strong>Make sure that modules like 'glx' are being loaded in
the xorg.conf file.
</p>


<h2> Quake 4 </h2>
<p>
After a few hours of playing Quake 3 on my laptop, I decided to take one
step further and try to get Quake 4 working on Fedora 12. And to my
surprise it actually took me longer to figure out how to get Quake 4
working than it took for Quake 3.
</p>

<p>
<div style=TEXT-ALIGN:center>
  <img height=280 src="misc/silva/dgqqst99_56fd5j3cck_b.jpg" width=196>
</div>
</p>

<h3> Installation </h3>
<p>
<strong>1.</strong> Grab the latest Quake 4 binaries from ID Software. <a
href=ftp://ftp.idsoftware.com/idstuff/quake4/linux/quake4-linux-1.4.2.x86.run>quake4-linux-1.4.2.x86.run</a>.
</p>

<pre class="code">
wget ftp://ftp.idsoftware.com/idstuff/quake4/linux/quake4-linux-1.4.2.x86.run
</pre>

<p>
<strong>2. </strong> Then make <em>quake4-linux-1.4.2.x86.run</em> executable:
</p>

<pre class="code">
chmod 755 quake4-linux-1.4.2.x86.run
</pre>

<p>
<strong>3.</strong> Execute it:
<p>

<pre class="code">
linux32 ./quake4-linux-1.4.2.x86.run
</pre>

<p>
<strong>4.</strong> Copy the the baseq4 files from your original Windows
CDs (or DVD) to /usr/local/games/quake4/base4q<br>
<strong>5. </strong> When I tried to execute the binary with:
<p>

<pre class="code">
linux32 /usr/local/games/quake4/quake4-smp
</pre>

<p>
I would get an error like this:
</p>

<pre class="code">
  X..GL_EXT_texture_compression_s3tc not found
</pre>

<p>
After several hours of 'googling' and reading documentation, I was able to
to fix this problem by doing the following:
</p>

<pre class="code">
yum install driconf
</pre>

<p>
Then, as the user who will be running the game, run:<p>
</p>

<pre class="code">
driconf
</pre>

<p>
and under Image Quality, enable <i>S3TC textures</i>. That did it for me.
</p>

<div style=TEXT-ALIGN:center>
<img src="misc/silva/EXTERN_0002.png" style="HEIGHT:250px;MARGIN:0px auto 10px;TEXT-ALIGN:center;WIDTH:400px">
</div>

<h3> Fixing Sound: </h3>
<p>
<strong>1.</strong> As root:
</p>

<pre class="code">
/sbin/modprobe snd_pcm_oss
chmod 777 /proc/asound/card0/pcm0p/oss
</pre>

<p>
<strong>2. </strong> My game still didn't have sound, so I had to edit
<i>~/.quake4/q4base/Quake4Config.cfg</i> and modify the option from:
</p>

<pre class="code">seta s_driver "best"</pre>

<p>
to
<p>

<pre class="code">seta s_driver "oss"</pre>

<h3> Starting the game: </h3>
<p>
And finally, much like how we started Quake 3 Arena, we may now start up
Quake 4:
</p>

<pre class="code">
echo "quake4-smp.x86 0 0 direct" &gt; /proc/asound/card0/pcm0p/oss &amp;&amp; linux32 /usr/local/games/quake4/quake4-smp
</pre>

<p>
<div style=TEXT-ALIGN:center>
<img src="misc/silva/EXTERN_0003.png" style="HEIGHT:250px;MARGIN:0px auto 10px;TEXT-ALIGN:center;WIDTH:400px">
</div>
</p>

<h3> Conclusion: </h3>

<p>
If you have been around Linux for over ten years and used to play these
'classic' games back in the day, I hope you enjoy getting them re-installed
and running again on your systems as much as I have. If you have just
started using Linux within the past few years, and you don't have the
original media to install Quake 3 Arena or Quake 4, stick around with
OpenArena. It's a great alternative to the Quake saga, and it's much easier
to install.
</p>

<br clear="all" />

<script type='text/javascript'>
digg_url = 'http://linuxgazette.net/172/silva.html';
digg_title = 'Running Quake 3 and 4 on Fedora 12 (64-bit)';
digg_bodytext = '<p> I started writing this article one night in December when I was bored and looking for a game to play on Linux. I am not much of a computer gamer, so I am not really current with what is out there for games in the Open Source world. That night I stumbled across <a href=http://openarena.ws/ id=oz-t title=OpenArena>OpenArena</a>, which is "a network enabled multiplayer first person shooter based on the ioquake3 fork of the id tech 3 engine." [<a href=http://openarena.wikia.com/wiki/Main_Page id=gzg1 title=1>1</a>] </p> ';
digg_topic = 'linux_unix';
</script>
<script src="http://digg.com/tools/diggthis.js" type="text/javascript"></script>

</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:172/silva.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<hr>
<p>
<img align="left" alt="[BIO]" src="../gx/authors/silva.jpg" class="bio">
</p>

<em>
<p>
Anderson Silva works as an IT Release Engineer at Red Hat, Inc. He
holds a BS in Computer Science from Liberty University, a MS in
Information Systems from the University of Maine. He is a Red Hat
Certified Engineer working towards becoming a Red Hat Certified
Architect and has authored several Linux based articles for
publications like: Linux Gazette, Revista do Linux, and Red Hat
Magazine. Anderson has been married to his High School sweetheart,
Joanna (who helps him edit his articles before submission), for 11
years, and has 3 kids. When he is not working or writing, he enjoys
photography, spending time with his family,  road cycling, watching
Formula 1 and Indycar races, and taking his boys karting,
</p>

</em>

<br clear="all">


<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2010, <a href="../authors/silva.html">Anderson Silva</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 172 of Linux Gazette, March 2010
</p>

</div>
</div>


<div class="content lgcontent">

<a name="collinge"></a>
<h1>HelpDex</h1>
<p id="by"><b>By <a href="../authors/collinge.html">Shane Collinge</a></b></p>

</b>
</p>

<p>

<p>
<em>These images are scaled down to minimize horizontal scrolling.</em>
</p>

<a href="http://linuxgazette.net/124/misc/nottag/flash.html"><b>Flash problems?</b></a>
<br>

<div class="cartoon">

<object>
<embed src="misc/collinge/030linuxtoday.swf" bgcolor="#ffffff" width="600" />
</object>

<p>
<a href="misc/collinge/030linuxtoday.swf">Click here to see the full-sized image</a>
</p>

</div>
<div class="cartoon">

<object>
<embed src="misc/collinge/021pencil.swf" bgcolor="#ffffff" width="600" />
</object>

<p>
<a href="misc/collinge/021pencil.swf">Click here to see the full-sized image</a>
</p>

</div>

<p> All HelpDex cartoons are at Shane's web site,
<a href="http://www.shanecollinge.com/">www.shanecollinge.com</a>.

<script src="http://www.google-analytics.com/urchin.js"
type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-1204316-1";
urchinTracker();
</script>


</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:172/collinge.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<!-- *** BEGIN bio *** -->
<hr>
<P>
<img ALIGN="LEFT" ALT="Bio picture" SRC="../gx/2002/note.png" class="bio">
<em>
Part computer programmer, part cartoonist, part Mars Bar. At night, he runs
around in his brightly-coloured underwear fighting criminals. During the
day... well, he just runs around in his brightly-coloured underwear. He
eats when he's hungry and sleeps when he's sleepy.
</em>
<br CLEAR="all">
<!-- *** END bio *** -->

<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2010, <a href="../authors/collinge.html">Shane Collinge</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 172 of Linux Gazette, March 2010
</p>

</div>
</div>


<div class="content lgcontent">

<a name="doomed"></a>
<h1>Doomed to Obscurity</h1>
<p id="by"><b>By <a href="../authors/trbovich.html">Pete Trbovich</a></b></p>

</b>
</p>

<p>

<p>
<em>These images are scaled down to minimize horizontal scrolling.</em>
</p>

<div class="cartoon">

<p>
<a href="misc/doomed/0000214.jpg">
<img src="misc/doomed/0000214.jpg" bgcolor="#ffffff" width="600" border="none" />
<br /> Click here to see the full-sized image
</a>
</p>

</div>
<div class="cartoon">

<p>
<a href="misc/doomed/0000212.jpg">
<img src="misc/doomed/0000212.jpg" bgcolor="#ffffff" width="600" border="none" />
<br /> Click here to see the full-sized image
</a>
</p>

</div>

<p> All "Doomed to Obscurity" cartoons are at Pete Trbovich's site,
<a
href="http://penguinpetes.com/Doomed_to_Obscurity/">http://penguinpetes.com/Doomed_to_Obscurity/</a>.

<script src="http://www.google-analytics.com/urchin.js"
type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-1204316-1";
urchinTracker();
</script>


</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:172/doomed.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<hr>
<p>
<img align="left" alt="[BIO]" src="../gx/2002/note.png" class="bio">
</p>

<em>
<p>
Born September 22, 1969, in Gardena, California, "Penguin" Pete Trbovich
 today resides in Iowa with his wife and children. Having worked various
 jobs in engineering-related fields, he has since "retired" from
 corporate life to start his second career. Currently he works as a
 freelance writer, graphics artist, and coder over the Internet. He
 describes this work as, "I sit at home and type, and checks mysteriously
 arrive in the mail."
</p>

<p>
He discovered Linux in 1998 - his first distro was Red Hat 5.0 - and has
 had very little time for other operating systems since. Starting out
 with his freelance business, he toyed with other blogs and websites
 until finally getting his own domain penguinpetes.com started in March
 of 2006, with a blog whose first post stated his motto: "If it isn't fun
 for me to write, it won't be fun to read."
</p>

<p>
The webcomic <em>Doomed to Obscurity</em> was launched New Year's Day,
 2009, as a "New Year's surprise". He has since rigorously stuck to a
 posting schedule of "every odd-numbered calendar day", which allows him
 to keep a steady pace without tiring. The tagline for the webcomic
 states that it "gives the geek culture just what it deserves." But is it
 skewering everybody but the geek culture, or lampooning geek culture
 itself, or doing both by turns?
</p>



</em>

<br clear="all">


<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2010, <a href="../authors/trbovich.html">Pete Trbovich</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 172 of Linux Gazette, March 2010
</p>

</div>
</div>


<div class="content lgcontent">

<a name="lg_backpage"></a>
<h1>The Backpage</h1>
<p id="by"><b>By <a href="../authors/okopnik.html">Ben Okopnik</a></b></p>

</b>
</p>

<p>
<p>
During the past month, LG has gone through several changes. For one thing,
our former proofreading coordinator, Rick Moen, is no longer associated
with LG in any capacity as of the 4th of last month; for another, our lists
are now hosted at a competent, reliable, responsive webhost (on a Linux
box, of course!) This provides additional reliability in our
infrastructure: if something should happen to our list server, LG itself
will not be affected; if the LG server should get whacked by an asteroid
strike, we'll still have a communication channel where we can organize
asteroid removal efforts and silicon chip-gluing parties, or whatever else
is necessary.
</p>

<p>
(A request for anyone who was subscribed to 'lg-announce', or anyone who
would like to be notified when the new issues come out: the membership
roster for this list was the only thing that I didn't manage to save from
our previous hosting arrangement - sorry about that! If you'd like to be
pinged about new issues, please go to
<a href="http://lists.linuxgazette.net/listinfo.cgi/lg-announce-linuxgazette.net">http://lists.linuxgazette.net/listinfo.cgi/lg-announce-linuxgazette.net</a> and sign up. Thanks!)
</p>

<p>
The Linux Gazette itself, by the way, remains hosted at <a
href="http://fullhartsoftware.com">Fullhart Software</a>, through the kind
offices of Thomas Fullhart, who also provides us with his stellar system
administration services. <em>(Unprompted ad: Thomas is awesome. I've known him
for years now, and can personally tell you stories of generosity, grace
under pressure, and great responsiveness and flexibility in tough times. If
you need great hosting, talk to him.)</em>
</p>

<p>
In addition, we've also decided to commit more time and effort to a "pet
project" that's close to my own heart: improving the quality and quantity
of technical writing available to the Open Source community by finding
interested authors and supporting them with positive editorial feedback and
critique. During my tenure as Editor-in-Chief, I've received a lot of
pleasure from watching a number of our authors grow and mature as technical
writers, and seeing their talents recognized in the business world. I take
a lot of pride in believing that LG was at least partly instrumental in
their current success, and that their experience here will improve their
lives and careers from here forward. It's all part of "Making Linux Just a
Little More Fun!" - by improving everyone's experience with Open Source
projects.
</p>

<h3>Recent Events: A Look Inside LG</h3>

<p>
Right after publication of the last issue, LG went through one of those
shake-ups for which Open Source projects and communities are well-known.
Tempers flared, metaphorical furniture was stomped into kindling, high
horses were brought out of their stables and ridden hard... and the end
result was a change in methods and policy. In this case, a change designed
to better serve our readership and our community.
</p>

<p>
I want to emphasize this point: as LG's Editor-in-Chief, I do my best to
<em>avoid</em> setting policy. I believe that, when given a mission and
left to their own devices within wide bounds, people will create solutions
and answers that will range from more-or-less reasonable to breath-takingly
wonderful. Policy is the thing that sets those bounds - and so, in the
traditional spirit of the Open Source community, I try to keep them as
broad possible.
</p>

<pre class="code">
UNIX was never designed to keep people from doing stupid things, because
that policy would also keep them from doing clever things.
 -- Doug Gwyn
</pre>

<pre class="code">
The handles of a craftsman's tools bespeak an absolute simplicity, the
plainest forms affording the greatest range of possibilities for the user's
hand. That which is overdesigned, too highly specific, anticipates outcome;
the anticipation of outcome guarantees, if not failure, the absence of
grace.
 -- William Gibson, "All Tomorrow's Parties"
</pre>

<p>
There is, however, a failure mode inherent in this method (as there is in
all methods). It is that of doing too little for too long - at which point,
flamewars erupt and furniture turns into splinters. In effect, the failure
is mine: as always, the captain of the ship is responsible for the welfare
of the ship and crew, and any failures are completely and utterly his
responsibility. For that, I apologize to anyone harmed by my previous
unwillingness to do the ugly but utterly necessary housecleaning - and
hereby, publicly, set the LG policy to handle those problems from here
forward.
</p>

<h3>In the "This shouldn't be news to anyone" category, we have...</h3>

<p>
<ol>
<li><strong>Be Polite. No Rudeness Allowed.</strong> This is the
all-encompassing policy statement that should have been at the top - Rule
#0 - of the LG list. Again, my fault - and my apologies to everyone - for
not realizing that it needed to be spelled out explicitly and that it
<em>was</em> this critical of an issue. This omission has now been
rectified, and will be enforced; by our list administrator in The Answer
Gang (my own posts explicitly <em>not</em> excluded; I confess to owning a
temper), and by me in our published material. 'Nough said.
</li>

<li><strong>LG is not anyone's personal rant forum.</strong> As the
Editor-in-Chief, I have entertained - and invited - viewpoints and styles
including some that <a href="..//117/raby.html">I disagree with</a>, some
that can be seen as <a href="../170/starks.html">inflammatory</a>, some
that are <a href="../109/park.html">completely opposite</a> of my own
thoughts on best computing practices, and so on. There's almost no
restriction on content other than LG's loose mandate of "making Linux just
a little more fun" - which, for the purposes of article acceptance, means
"related to Open Source computing, or at least humorous and relevant to
computing in some fashion."
</li>

<li><strong>There's no such thing as a stupid question.</strong>
<a href="http://lists.linuxgazette.net/mailman/admin/tag">The Answer Gang list</a>
exists, and has always existed, for the purpose of answering Linux
questions. As many such lists do, it has developed its own culture, one
that is amazing and powerful in its own right: it has produced some
<a href="../102/lg_backpage.html">hilariously funny exchanges</a>, a huge
number of <a href="../faq/kb.html">excellent answers and solutions</a>, and
- for those who participate - a terrific learning environment. It has also,
however, had instances of treating "clueless" questions - or, more to the
point, the people who ask them - less than respectfully. I want to make it
clear, here and now: there is NO such thing as a clueless question, this is
not alt.sysadmin.recovery, and there's no one here entitled to be a <a
href="http://www.iinet.net.au/~bofh/">Bastard Operator From Hell</a>.
<strong>Everyone</strong> is welcome to post and participate in TAG;
<strong>no one</strong> is allowed to be rude. The rule is, "if you can't
be pleasant, don't post."
</li>
</ol>
These rules also apply to the editing process all articles pass through.
The rules are not written in stone and shouldn't be too complicated.
If you have ideas for improvement or suggestions, we'd like to hear your
opinion.
</p>

<p>
(For those who, in times past, stopped by TAG and left due to a high
level of spam: that's gone as well, since we've changed our subscription
policy. <a
href="http://lists.linuxgazette.net/mailman/admin/tag">Come on in</a> - the
water is fine!)
</p>
</li>
</ol>

<h3>So, what's left in LG?</h3>
<p>
Everything except the unpleasantness and the rudeness. In fact, from a
certain perspective, you might say that there are now <em>more</em>
possibilities and open avenues within LG: all relevant questions are now
welcome in TAG, and everyone will be treated in a civil fashion. Let's
focus on the answers to your questions and on how to exchange knowledge.
</p>

<p>
None of the foregoing should be a surprise. Perhaps one way to put all of
this into context is to consider the Linux Gazette as a
Linux-representative "business": although none of us get paid in cash for
our work here, we get "paid" in community respect, in recognition, and in
reputation - things that are important to every other service-sector
business. All of the above, points of policy and all, is nothing more than
clarification of LG's stance toward you, its "customers":
</p>

<p>
<ul>
<li> We've swept out all the dark corners so that you won't get bitten by
any spiders. In addition, we've hung up some lights in those corners to give
you a beacon for navigation.
<li> Going forward, we'll make sure that all of our public spaces - and
that's all of LG - stay clean, bright, shiny, and pleasant.
<li> Pleasant, mannerly people are always welcome - as customers, staff,
friends, and neighbors. We'll even give you a 100% discount on all our
articles for as long as you participate (don't worry, we'll still make a
profit; we've got a great markup.)
</ul>
</p>

<p>
There will probably be some bobbles as things shake down; there inevitably
are. I don't expect instant perfection out of anyone (least of all myself.)
But I want everyone to know, and understand, that these are our goals; this
is what we're aiming for.
</p>

<p>
Welcome to your new, and hopefully ever-improving, LG.
</p>

<p class="sig"><img src="../gx/okopnik_sig.png" alt="B. Okopnik"></p>


<br clear="all" />

<script type='text/javascript'>
digg_url = 'http://linuxgazette.net/172/lg_backpage.html';
digg_title = 'The Backpage';
digg_bodytext = '<p> During the past month, LG has gone through several changes. For one thing, our former proofreading coordinator, Rick Moen, is no longer associated with LG in any capacity as of the 4th of last month; for another, our lists are now hosted at a competent, reliable, responsive webhost.</p> ';
digg_topic = 'linux_unix';
</script>
<script src="http://digg.com/tools/diggthis.js" type="text/javascript"></script>

</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:172/lg_backpage.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<!-- *** BEGIN bio *** -->
<hr>
<p>
<img alt="picture" src="../gx/authors/okopnik.jpg" align="left"  hspace="10" vspace="10" class="bio">
</p>

<p>
Ben is the Editor-in-Chief for Linux Gazette and a member of The Answer Gang.
</p>

<p>
<em>
Ben was born in Moscow, Russia in 1962. He became interested in electricity
at the tender age of six, promptly demonstrated it by sticking a fork into
a socket and starting a fire, and has been falling down technological
mineshafts ever since. He has been working with computers since the Elder
Days, when they had to be built by soldering parts onto printed circuit
boards and programs had to fit into 4k of memory (the recurring nightmares
have almost faded, actually.)
</p>

<p>
His subsequent experiences include creating software in more than two dozen
languages, network and database maintenance during the approach of a
hurricane, writing articles for publications ranging from sailing magazines
to technological journals, and teaching on a variety of topics ranging from
Soviet weaponry and IBM hardware repair to Solaris and Linux
administration, engineering, and programming. He also has the distinction
of setting up the first Linux-based public access network in St. Georges,
Bermuda as well as one of the first large-scale Linux-based mail servers in
St. Thomas, USVI.
</p>

<p>
After a seven-year Atlantic/Caribbean cruise under sail and passages up and
down the East coast of the US, he is currently anchored in northern
Florida. His consulting business presents him with a variety of challenges
such as teaching professional advancement courses for Sun Microsystems and
providing Open Source solutions for local companies.
</p>

<p>
His current set of hobbies includes flying, yoga, martial arts,
motorcycles, writing, Roman history, and <strike>mangling</strike> playing
with his Ubuntu-based home network, in which he is ably assisted by his <a
href="../authors/tanaka-okopnik.html">wife</a>, <a
href="../authors/okopnik1.html">son</a> and daughter; his Palm Pilot is
crammed full of alarms, many of which contain exclamation points.
</p>

<p>
He has been working with Linux since 1997, and credits it with his complete
loss of interest in waging nuclear warfare on parts of the Pacific Northwest.
</p>
</em>

<br CLEAR="all">
<!-- *** END bio *** -->

<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2010, <a href="../authors/okopnik.html">Ben Okopnik</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 172 of Linux Gazette, March 2010
</p>

</div>
</div>


<img src="../gx/tux_86x95_indexed.png" id="tux" alt="Tux"/>

<br />

<script src="http://www.google-analytics.com/urchin.js"
type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-1204316-1";
urchinTracker();
</script>

</body>
</html>

