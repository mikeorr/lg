<!DOCTYPE html
	PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
	 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
<title>TCP and Linux' Pluggable Congestion Control Algorithms LG #135</title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="stylesheet" href="../lg.css" type="text/css" media="screen, projection"  />
<link rel="alternate" type="application/rss+xml" title="LG RSS" href="lg.rss" />
<link rel="alternate" type="application/rdf+xml" title="LG RDF" href="lg.rdf" />
<link rel="alternate" type="application/atom+xml" title="LG Atom" href="lg.atom.xml" />
<link rel="shortcut icon" href="../favicon.ico" />

<style type="text/css" media="screen, projection">
<!--

-->
</style>

</head>
<body>

<a href="../">
<img src="../gx/2003/newlogo-blank-200-gold2.jpg" id="logo" alt="Linux Gazette"/>
</a>
<p id="fun">...making Linux just a little more fun!</p>

<div class="content articlecontent1">

<div id="previousnexttop">
<A HREF="okopnik.html" >&lt;-- prev</A> | <A HREF="root.html" >next --&gt;</A>
</div>



<h1>TCP and Linux' Pluggable Congestion Control Algorithms</h1>
<p id="by"><b>By <a href="../authors/pfeiffer.html">Ren&eacute; Pfeiffer</a></b></p>

<p>
Many months ago I had a few drinks with some fellow hackers. The discussion
touched on the inevitable
"my-Linux-is-better-than-your-*BSD-and-vice-versa" topic.  I tried to
argue in favour of Linux because it knows multiple TCP variants. They
corrected me and said that there is only one TCP and that *BSD speaks it
very well. Days after the side effects of the drinks were gone, I
remembered the discussion and felt a sense of <a
href="http://wordsmith.org/words/esprit_d_escalier.html"><em>l'esprit
d'escalier</em></a>. Of course, there is a clear definition what the famous
Tricky Communication Protocol (TCP) looks like, but there are many ways of
dealing with situations in Wide Area Networks (WANs) where congestion,
round-trip times, and packet loss play a major role. That's what I wanted
to say. Enter Linux's pluggable congestion control algorithms.  </p>

<h3>
Transmission Control Protocol (TCP)
</h3>

<p>
Every time you wish to transport data in a reliable way, you probably use TCP
(or your car, but let's stick to networks). 
TCP is able to transport large amounts of data in the correct order over
unreliable network links. The protocol keeps track of sent data, detects lost
packets, and retransmits them if necessary. This is done by acknowledging every packet to the
sender. Basically, this is how TCP works. My article would end here were it not
for some parameters of TCP connections that make things very interesting and
complicated. (To some, those attributes are the same.) The first
perturbation stems from the network itself, and materialises in the form
of three parameters that pertain to every TCP connection.

<ol>
<li> Bandwidth<br>
     The bandwidth indicates how many bits per time frame the link can
     transport. It is usually denoted by <em>Mbit/S</em> or <em>kbit/S</em> and limited
     by the hardware.
     Most people think that this indicates solely the performance of the network link.
     This is not quite true, as I will explain shortly.</li>
<li> Round-Trip Time (RTT)<br>
     Consider a network link between points A and B. The time a packet needs
     to travel from A to B and then back to A is called the Round-Trip Time. The
     RTT is highly variable, especially when a node on the way experiences
     congestion. Typically, you have an RTT from milliseconds to seconds (in the
     worst case).</li>
<li> Packet loss<br>
     Packets of a network transmission can get dropped. The ratio of lost
     packets to transported packets is called packet loss. There are many
     reasons for packet loss. A router might be under heavy load, a frame might
     get corrupted by interference (wireless networks like to drop packets this
     way), or an Ethernet switch may detect a wrong checksum.
     </li>
</ol>

So, you see that a 1000 Mbit/s link will get you nowhere if it has 25% packet loss
and an RTT of several seconds. The overall speed of your transmission depends on
all three parameters. Your local Ethernet is (with luck) fine, because it has
100 Mbit/s, 0% packet loss and RTTs below 1 millisecond. As soon as the RTT
rises, the management of the packets "in flight", i.e., the ones sent but not yet
acknowledged by the receiver, determine the real throughput of the data.
RTT and bandwidth are often combined and multiplied into the <em>bandwidth-delay product</em>.
</p>
<p>
How does TCP manage the parameter settings? Fortunately the protocol has some auto-tuning
properties. One important parameter is the <em>TCP window</em>. The window is
the amount of data the sender can send before receiving
an acknowledgement. This means that, as long as the window is "not full", the sender
can blindly keep sending packets. As soon as the window is filled, the sender stops
sending, and waits for acknowledgement packets. This is TCP's main mechanism of
flow control, that enables it to detect bottlenecks in the network during a
transmission. That's why it's also called the <em>congestion window</em>.
The default window size varies. Usually Linux starts at 32 kB.
<!-- I did not find a good source for the default window size. 32 kB seems to be common. -->
The window size is flexible, and can be changed during the transfer. This mechanism is
called <em>window scaling</em>. Starting with Linux kernel 2.6.17, the window can
be up to 4 MB. The window can grow only as long as there is no packet loss.
That's one of the reasons why data throughput is reduced on lossy network links.
</p>
<p>
TCP has some more interesting features, but I won't go into more details. We now know
enough, for the features of the Linux kernel I wish to describe.
</p>

<div class="pullquotes">
<a name="pullquote_135_pfeiffer_1"></a>
<table border="2">
<tr>
<td align="left" bgcolor="#B3C7BF">
<sup>Category: Protocols</sup>
<br />
<strong>
TCP is able to transport large amounts of data in the correct order over unreliable network links. The protocol keeps track of sent data, detects lost packets, and retransmits them if necessary.
</strong>
</td>
</tr>
</table>
</div>

<h3>
Always look on the WAN side of life
</h3>

<p>
The right size of the TCP window is critical to efficient transmission. The
tricky part is to find that right size. Either a too-small and a too-big window
will degrade throughput. A good guess is the use the bandwidth-delay product.
Furthermore, you can base estimates on periodically measured RTT or packet
loss, and make it dynamic. This is why several researchers have explored algorithms to help TCP
tune itself better, under certain circumstances. Beginning with 2.6.13, the Linux kernel 
supports plugins for the TCP stack, and enables
switching between algorithms depending on what the system is connected to. The
first strategy on the Internet was TCP Tahoe, proposed in 1988. A later variant was TCP Reno, now
widely implemented in network stacks, and its successor TCP NewReno.
Currently, the Linux kernel includes the following algorithms (as of kernel 2.6.19.1):
<ul>
<li> High Speed TCP<br>
     The algorithm is described in RFC 3649. The main use is for connections with
     large bandwidth and large RTT (such as Gbit/s and 100 ms RTT).</li>
<li> H-TCP<br>
     H-TCP was proposed by the Hamilton Institute for transmissions that recover more
     quickly after a congestion event. It is also designed for links with high bandwidth and RTT.</li>
<li> Scalable TCP<br>
     This is another algorithm for WAN links with high bandwidth and RTT. One of its design goals
     is a quick recovery of the window size after a congestion event. It achieves this goal by
     resetting the window to a higher value than standard TCP.</li> 
<li> TCP BIC<br>
     BIC is the abbreviation for Binary Increase Congestion control. BIC uses a unique window
     growth function. In case of packet loss, the window is reduced by a multiplicative factor.
     The window size just before and after the reduction is then used as parameters for a binary
     search for the new window size. BIC was used as standard algorithm in the Linux kernel.
     </li>
<li> TCP CUBIC<br>
     CUBIC is a less aggressive variant of BIC (meaning, it doesn't steal as much throughput from
     competing TCP flows as does BIC).
     </li>
<li> TCP Hybla<br>
     TCP Hybla was proposed in order to transmit data efficiently over satellite links
     and "defend" the transmission against TCP flows from other origins.
     </li>
<li> TCP Low Priority<br>
     This is an approach to develop an algorithm that uses excess bandwidth for TCP flows.
     It can be used for low priority data transfers without "disturbing" other TCP
     transmissions (which probably don't use TCP Low Priority).
     </li>
<li> TCP Tahoe/Reno<br>
     These are the classical models used for congestion control. They exhibit the typical slow start
     of transmissions. The throughput increases gradually until it stays stable. It is decreased as
     soon as the transfer encounters congestion, then the rate rises again slowly. The window is
     increased by adding fixed values. TCP Reno uses a multiplicative decrease algorithm for 
     the reduction of window size. TCP Reno is the most widely deployed algorithm.
     </li>
<li> TCP Vegas<br>
     TCP Vegas introduces the measurement of RTT for evaluating the link quality. It uses additive increases and
     additive decreases for the congestion window.
     </li>
<li> TCP Veno<br>
     This variant is optimised for wireless networks, since it was designed to handle random packet loss better. It tries
     to keep track of the transfer, and guesses if the quality decreases due to congestion or random packet errors.
<li> TCP Westwood+<br>
     Westwood+ addresses both large bandwidth/RTT values and random packet loss together with dynamically changing
     network loads. It analyses the state of the transfer by looking at the acknowledgement packets. Westwood+ is a
     modification of the TCP Reno algorithm.
     </li>
</ul>
This is only a rough outline of the modules. In case you want to understand what the algorithms do, you should
read the authors' publications. I have given links to most of them at the end of this article. While
investigating the algorithms and hunting for publications, I also came across
<a href="http://research.microsoft.com/research/pubs/view.aspx?type=Technical%20Report&id=940">CTCP</a>, created
by Microsoft Research Asia. I wonder if this will make its way into the Linux kernel, some day.
</p>
<p>
Switching between the different algorithms can be easily done, by writing text to a <tt>/proc/</tt> entry.
<pre>
nightfall:~# echo "westwood" &gt; /proc/sys/net/ipv4/tcp_congestion_control 
nightfall:~# cat /proc/sys/net/ipv4/tcp_congestion_control 
westwood
nightfall:~# 
</pre>
A list of available modules can be found here:
<pre>
nightfall:~# ls /lib/modules/`uname -r`/kernel/net/ipv4/
ip_gre.ko  netfilter   tcp_cubic.ko      tcp_htcp.ko   tcp_lp.ko        tcp_vegas.ko
ipip.ko    tcp_bic.ko  tcp_highspeed.ko  tcp_hybla.ko  tcp_scalable.ko  tcp_veno.ko
nightfall:~# 
</pre>
When writing to <tt>/proc/</tt>, you can skip the <tt>tcp_</tt> prefix. If you compile your own 
kernels, you will find the modules in the <em>Networking -&gt; Networking options -&gt; TCP:
advanced congestion control</em> section. Since some of the algorithms affect only the sender's side,
you may not notice a difference when enabling them. In order to see changed behaviour,
you have to create a controlled setup, and measure the parameters of TCP transmissions.
</p>

<h3>
Testing congestion situations
</h3>

<p>
Now that we know how to control the congestion algorithms, all we need is a bottleneck. As long
as your Linux box(es) are connected to the local Ethernet with 100 Mbit/s or more, all packets
will be sent immediately, and your local queues are always empty. The queues end up on your
modem or gateway, which may treat them differently 
from the way Linux does. Even if you use a Linux gateway, there won't be a queue there, because very often
this gateway connects via Ethernet to another device that handles the Internet connection. If you want
to know what Linux does with queues, you need to set up a controlled bottleneck between two or more
Linux boxes. Of course, you can try to saturate your LAN, but your core switch and your colleagues
might not be impressed.
</p>
<p>
<img src="misc/pfeiffer/bottleneck.png" alt="Artificial bottleneck between networked Linux systems." width="468" height="159">
</p>
<p>
You can use two separate routers and connect them with serial crossover cables. You can set up two
Linux boxes as routers, and create the bottleneck link with USB 1.1 crossover cabling, serial links,
Bluetooth devices, 802.11b/g WLAN equipment, parallel links, or slower Ethernet cards
(10 Mbit/s, for example). You could also do traffic shaping in-between,
and reduce the bandwidth or increase the latency. The Linux kernel has a nice tool for doing this.
It's called <em>netem</em>, which is short for Network Emulator. Recent distributions have it. In case
you compile your own kernels, you'll find it here:
<pre>
 Networking --&gt;
   Networking Options --&gt;
     QoS and/or fair queuing --&gt;
        Network emulator
</pre>
Additionally, you need to enable the <em>Advanced Router</em> config option. netem can be activated by the 
<tt>tc</tt> tool from the <em>iproute</em> package. It allows you to
<ul>
<li> emulate WAN links,</li>
<li> create packet loss,</li>
<li> create packet duplication,</li>
<li> reorder packets as they are routed,</li>
<li> and corrupt packets.</li>
</ul>
So, let's say you have a Linux router, and wish to simulate a WAN environment by increasing the RTT, adding
packet reordering and some corruption of bits. You do this by entering the following commands:
<pre>
tc qdisc add dev eth1 root netem corrupt 2.5% delay 50ms 10ms
tc qdisc add dev eth0 root netem delay 100ms reorder 25% 50%
</pre>
This means that network device <tt>eth1</tt> corrupts 2.5% of all packets passing through it. Additionally,
all packets get delayed by 50ms 10ms. Device <tt>eth1</tt> introduces a delay of at least 100 ms for reordered
packets. 25% of the packets will be sent immediately. The 50% indicates a correlation between random reordering
events. You can combine different <a href="http://linux-net.osdl.org/index.php/Netem">netem options</a>. Make sure
that you enter only one <tt>tc</tt> command for every network device with all options and parameters attached.
<tt>netem</tt> is a network queue, and can only be installed once for every network device. You can remove it
any time you want, though.
</p>

<h3>
Creating traffic and measuring TCP
</h3>

<p>
After your new and shiny bottleneck works, you can start moving data from one end to the other. Every
program that speaks TCP can be used for that. <tt>iperf</tt>, <tt>netpipe</tt>, any FTP client, <tt>netcat</tt>,
or <tt>wget</tt> are good tools. Recording the network flows with <tt>wireshark</tt> and to postprocess them
with <tt>tcptrace</tt> is also an option. <tt>wireshark</tt> has some features that allow you to analyse
a TCP connection such as creating a graph of RTT values, sequence numbers, and throughput -- though a real
evaluation for serious deployment must use a finer resolution and a better statistic than simple progress bars.
</p>
<p>
You can also do in-kernel measurement by using the module <em>tcpprobe</em>. It is available when the kernel
is compiled with kernel probes that provide hooks to kernel functions (called Kprobe support). <em>tcpprobe</em>
can be enabled by loading it with <em>modprobe</em> and giving a TCP port as module option. The documentation
of <em>tcpprobe</em> features a simple example:
<pre>
# modprobe tcpprobe port=5001
# cat /proc/net/tcpprobe &gt;/tmp/data.out &amp;
# pid=$!
# iperf -c otherhost
# kill $pid
</pre>
<em>tcpprobe</em> gets loaded and is bound to port 5001/TCP. Reading <tt>/proc/net/tcpprobe</tt> directly
gives access to the congestion window and other parameters of the transmission. It produces one line of 
date in text format
for every packet seen. Using port 0 instead of 5001 allows measuring
the window of all TCP connections to the machine. The Linux network people have more tips for
<a href="http://linux-net.osdl.org/index.php/TCP_testing">testing TCP</a> in their wiki. Also, be sure
to familiarise yourself with the methods and tools, in case you plan to do some serious testing.
</p>

<div class="pullquotes">
<a name="pullquote_135_pfeiffer_2"></a>
<table border="2">
<tr>
<td align="left" bgcolor="#B3C7BF">
<sup>Category: Protocols</sup>
<br />
<strong>
Keep in mind that TCP really is a can of worms that get very complex, and that the developers of all Free operating systems deserve a lot of recognition and thanks for dealing with these issues. 
</strong>
</td>
</tr>
</table>
</div>


<h3>
I'm on ADSL. Does it really matter?
</h3>

<p>
Most of the new TCP algorithms incorporated into the Linux kernel were designed for very specific 
purposes. The modules are no magic bullet. If you are connected to a 56k modem, then no congestion algorithm
in the Universe will give you more bandwidth than your modem can handle. However, if multiple TCP flows have to share 
the same link, then some algorithms give you more throughput than others. The best way to find out is to
create a test environment with defined conditions, and make comparisons on what you see. My intention was
to give you an overview of what's going on in the kernel, and how flexible Linux is when dealing with
variable network environments. Happy testing!
</p>

<h3>
Useful links
</h3>

<p>
No packets were harmed while preparing this article. You might wish to take a
look at the following tools and articles suitable to save your network link and
deepen the understanding of what the Linux kernel does with your packets.
<ul>
<li> <a href="http://www.cs.helsinki.fi/research/iwtcp/papers/linuxtcp.pdf">Congestion Control in Linux TCP (PDF)</a></li>
<li> <a href="http://www.hamilton.ie/net/draft-leith-tcp-htcp-00.txt">H-TCP</a></li>
<li> <a href="http://www.icir.org/floyd/hstcp.html">High Speed TCP</a></li>
<li> <a href="http://dast.nlanr.net/Projects/Iperf/">iperf</a></li>
<li> <a href="http://linux-net.osdl.org/index.php/TcpProbe">Linux TCP Probe module</a></li>
<li> <a href="http://www-didc.lbl.gov/TCP-tuning/linux.html">Linux TCP Tuning Guide</a></li>
<li> <a href="http://www.scl.ameslab.gov/netpipe/">Netpipe</a></li>
<li> <a href="http://linux-net.osdl.org/index.php/Netem">Network Emulation functionality of the Linux kernel (netem)</a><li>
<li> <a href="http://www.sigcomm.org/ccr/archive/1999/oct99/allman2.pdf">On the Effective Evaluation of TCP</a></li>
<li> <a href="http://www.deneholme.net/tom/scalable/">Scalable TCP</a></li>
<li> <a href="http://www.speedguide.net/bdp.php">SG Bandwidth*Delay Product Calculator</a></li>
<li> <a href="http://kb.pert.geant2.net/PERTKB/TransmissionControlProtocol">TCP at PERTKB</a></li>
<li> <a href="http://www.csc.ncsu.edu/faculty/rhee/export/bitcp/">TCP BIC</a></li>
<li> <a href="http://www.csc.ncsu.edu/faculty/rhee/export/bitcp/cubic-paper.pdf">TCP CUBIC (PDF)</a></li>
<li> <a href="http://www-ece.rice.edu/networks/TCP-LP/">TCP Low Priority</a></li>
<li> <a href="http://www.ntu.edu.sg/home5/ZHOU0022/papers/CPFu03a.pdf">TCP Veno</a></li>
<li> <a href="http://193.204.59.68/mascolo/tcp%20westwood/tcpwestwood.htm">TCP Westwood+</a></li>
<li> <a href="http://www.cs.ucla.edu/NRL/hpi/tcpw/">TCP Westwood</a></li>
<li> <a href="http://www.tcptrace.org/">tcptrace</a></li>
<li> <a href="http://www.wireshark.org/">Wireshark traffic analyser</a></li>
</ul>
You might even wish to study the kernel's source code. All things IPV4 can be found in
the <em>/lib/modules/`uname -r`/build/net/ipv4/</em> directory. The C files contain valuable comments
on what Linux does and how it handles certain situations and packets. You don't have to be
a programmer to understand it.
</p>

<h3>
Author's footnote
</h3>

<p>
The only reason that I mentioned the religious Linux/*BSD discussion is the fact of being
at a party and talking with friends. I don't wish to imply that one OS is better than the
other. You can solve and create all your problems with both systems. Keep in mind that TCP
really is a can of worms that get very complex, and that the developers of all Free operating
systems deserve a lot of recognition and thanks for dealing with these issues.
</p>



<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:135/pfeiffer.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
<!-- *** BEGIN bio *** -->
<hr>
<p>

<img align="left" alt="Bio picture" src="../gx/authors/pfeiffer.jpg" class="bio">

</p>
<em>

<p>
Ren&eacute; was born in the year of Atari's founding and the release of the game Pong.
Since his early youth he started taking things apart to see how they work. He
couldn't even pass construction sites without looking for electrical wires that
might seem interesting. The interest in computing began when his grandfather
bought him a 4-bit microcontroller with 256 byte RAM and a 4096 byte operating
system, forcing him to learn assembler before any other language.
</p>

<p>
After finishing school he went to university in order to study physics. He then
collected experiences with a C64, a C128, two Amigas, DEC's Ultrix, OpenVMS and
finally GNU/Linux on a PC in 1997. He is using Linux since this day and still
likes to take things apart und put them together again. Freedom of tinkering
brought him close to the Free Software movement, where he puts some effort into
the right to understand how things work. He is also involved with civil liberty
groups focusing on digital rights.
</p>

<p>
Since 1999 he is offering his skills as a freelancer. His main activities
include system/network administration, scripting and consulting. In 2001 he
started to give lectures on computer security at the Technikum Wien. Apart from
staring into computer monitors, inspecting hardware and talking to network
equipment he is fond of scuba diving, writing, or photographing with his digital
camera. He would like to have a go at storytelling and roleplaying again as soon
as he finds some more spare time on his backup devices.
</p>

</em>
</p>
<br clear="all">
<!-- *** END bio *** -->

<!-- *** END author bio *** -->

<div id="articlefooter">

<p>
Copyright &copy; 2007, Ren&eacute; Pfeiffer. Released under the <a
href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>

<p>
Published in Issue 135 of Linux Gazette, February 2007
</p>

</div>


<div id="previousnextbottom">
<A HREF="okopnik.html" >&lt;-- prev</A> | <A HREF="root.html" >next --&gt;</A>
</div>


</div>


<div id="navigation">
<table summary="masthead" width="100%">
<tr>
<td align="center" colspan="3" style="font-size: 10px; font-weight: bold">
<a href="../index.html">Home</a>
<a href="http://linuxgazette.net">Main Site</a>
<a href="../faq/index.html">FAQ</a>

<a href="../lg_index.html">Site Map</a>
<a href="../mirrors.html">Mirrors</a>
<a href="../mirrors.html">Translations</a>
<a href="../search.html">Search</a>
<a href="../archives.html">Archives</a>
<a href="../authors/index.html">Authors</a>
<a href="http://lists.linuxgazette.net/mailman/listinfo/">Mailing Lists</a>
<a href="../jobs.html">Join Us!</a>
<a href="../contact.html">Contact Us</a>

<hr width="99%" style="border: 1px inset #000033">
</td>
</tr>
<tr>
<td width="33%" align="left" style="font-size: 10px; font-weight: bold">
The Free Monthly Linux Publication
</td>
<td width="33%" align="center" style="font-size: 10px; font-weight: bold">
ISSN: 1934-371X
</td>
<td width="33%" align="right" style="font-size: 10px; font-weight: bold">
Main site: <a href="http://linuxgazette.net">http://linuxgazette.net</a> 
</td>
</table>
</div>




<div id="breadcrumbs1">

<a href="../index.html">Home</a> &gt; 
<a href="index.html">February 2007 (#135)</a> &gt; 
Article

</div>



<img src="../gx/tux_86x95_indexed.png" id="tux" alt="Tux"/>

</body>
</html>

