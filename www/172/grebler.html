<!DOCTYPE html
	PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
	 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="utf-8" xml:lang="utf-8">
<head>
<title>Tunnel Tales 1 LG #172</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" href="../lg.css" type="text/css" media="screen, projection"  />
<link rel="alternate" type="application/rss+xml" title="LG RSS" href="../lg.rss" />
<link rel="alternate" type="application/rdf+xml" title="LG RDF" href="../lg.rdf" />
<!-- link rel="alternate" type="application/atom+xml" title="LG Atom" href="lg.atom.xml" / -->
<link rel="shortcut icon" href="../favicon.ico" />

<style type="text/css" media="screen, projection">
<!--

-->
</style>

</head>
<body>

<a href="../">
<img src="../gx/2003/newlogo-blank-200-gold2.jpg" id="logo" alt="Linux Gazette"/>
</a>
<p id="fun">...making Linux just a little more fun!</p>

<div id="navigation">
<table summary="masthead" width="100%">
<tr>
<td align="center" colspan="3" style="font-size: 10px; font-weight: bold">
<a href="../index.html">Home</a>
<a href="http://linuxgazette.net">Main Site</a>
<a href="../faq/index.html">FAQ</a>

<a href="../lg_index.html">Site Map</a>
<a href="../mirrors.html">Mirrors</a>
<a href="../mirrors.html">Translations</a>
<a href="../search.html">Search</a>
<a href="../archives.html">Archives</a>
<a href="../authors/index.html">Authors</a>
<a href="http://lists.linuxgazette.net/listinfo.cgi">Mailing Lists</a>
<a href="../jobs.html">Join Us!</a>
<a href="../contact.html">Contact Us</a>

<hr width="99%" style="border: 1px inset #000033">
</td>
</tr>
<tr>
<td width="40%" align="left" style="font-size: 10px; font-weight: bold">
The Free International Online Linux Monthly
</td>
<td width="20%" align="center" style="font-size: 10px; font-weight: bold">
ISSN: 1934-371X
</td>
<td width="40%" align="right" style="font-size: 10px; font-weight: bold">
Main site: <a href="http://linuxgazette.net">http://linuxgazette.net</a> 
</td>
</table>
</div>


<div id="breadcrumbs1">

<a href="../index.html">Home</a> &gt; 
<a href="index.html">March 2010 (#172)</a> &gt; 
Article

</div>

<div class="articlecontent1">
<div class="content">

<div id="previousnexttop">
<A HREF="dyckoff.html" >&lt;-- prev</A> | <A HREF="peterson.html" >next --&gt;</A>
</div>

<h1>Tunnel Tales 1</h1>
<p id="by"><b>By <a href="../authors/grebler.html">Henry Grebler</a></b></p>

<h3>Justin</h3>

<p> 
I met Justin when I was contracting to one of the world's biggest
computer companies, OOTWBCC, building Solaris servers for one of
Australia's biggest companies (OOABC). Justin is in EBR (Enterprise
Backup and Recovery). (OOTWBCC is almost certainly the world's most
prolific acronym generator (TWMPAG).) I was writing scripts to
automate much of the install of EBR.
</p>

<p>
To do a good job of developing automation scripts, one needs a test
environment. Well, to do a good job of developing just about anything,
one needs a test environment. In our case, there was always an
imperative to rush the machines we were building out the door and
into production (pronounced BAU (Business As Usual) at TWMPAG).
Testing on BAU machines was forbidden (fair enough).
</p>

<p>
Although OOTWBCC is a huge multinational, it seems to be reluctant to
invest in hardware for infrastructure. Our test environment consisted
of a couple of the client's machines. They were "network orphans",
with limited connectivity to other machines.
</p>

<p>
Ideally, one also wants a separate development environment, especially
a repository for source code. Clearly this was asking too much, so
Justin and I shrugged and agreed to use one of the test servers as a
CVS repository.
</p>

<p>
The other test machine was constantly being trashed and rebuilt from
scratch as part of the test process. Justin started to get justifiably
nervous. One day he came to me and said that we needed to back up the
CVS repository. "And while we're at it, we should also back up a few
other directories."
</p>

<p> 
Had this been one of the typical build servers, it would have had
direct access to all of the network, but, as I said before, this one
was a network orphan. <A HREF="#Diag1">Diagram 1</A> indicates the
relevant connectivity.
</p>

<a name="Diag1"></a>
<img src="misc/grebler/diag1.jpeg" alt="Diagram 1" width="582" height="347"/>

<pre>

test	the test machine and home of the CVS repository
laptop	my laptop
jump	an intermediate machine
backup	the machine which partakes in regular tape backup
</pre>

<p>
If we could get the backup data to the right directory on
<strong>backup</strong>, the corporate EBR system would do the rest.
</p>

<p>
The typical response I got to my enquiries was, "Just copy the stuff to
your laptop, and then to the next machine, and so on." If this were a
one-off, I <i>might</i> do that. But what's the point of a single
backup? If this activity is not performed at least daily, it will soon
be useless. Sure, I <i>could</i> do it manually. Would you?

<h3>Step by Step</h3>

<p>
I'm going to present the solution step by step. Many of you will find
some of this just motherhoods<a id="grebler.html_1_back"></a><a href="#grebler.html_1">[1]</a>. Skip ahead.
</p>

<p>
My laptop ran Fedora 10 in a VirtualBox under MS Windows XP. All my
useful work was done in the Fedora environment.
</p>


<h3>Two machines</h3>

<p>
If I want to copy a single file from the test machine to my laptop,
then, on the laptop, I would use something like:
</p>

<pre class="code">
	scp -p test:/tmp/single.file /path/to/backup_dir
</pre>

This would create the file <tt>/path/to/backup_dir/single.file</tt> on
my laptop.

<p>
To copy a whole directory tree <i>once</i>, I would use:

<pre class="code">
	scp -pr test:/tmp/top_dir /path/to/backup_dir
</pre>

This would populate the directory <tt>/path/to/backup_dir/top_dir</tt>.

</p>

<h3>Issues</h3>

<ul>
<li>
<p>
Why did I say "once"? <tt>scp</tt> is fine if you want to copy
a directory tree once. And it's fine if the directory tree is not
large. And it's fine if the directory tree is extremely volatile (ie
frequently changes completely (or pretty much)).
</p>

<p>
But what we have here is a directory tree which simply accumulates
incremental changes. I guess over 80% of the tree will be the same
from one day to the next. Admittedly, the tree is not large, and the
network is pretty quick, but even so, it's nice to do it the right way
- if possible.
</p>
</li>

<li>
<p>
There is another problem, potentially a much bigger problem.
The choice of <tt>scp</tt> or some other program is about
efficiency and elegance. This problem can be a potential roadblock:
permissions.
</p>

<p>
The way <tt>scp</tt> works, I have to log in to <strong>test</strong>.
But I can only directly log in as myself (my user id on
<strong>test</strong>). If I want root privileges I have to use
<tt>su</tt> or <tt>sudo</tt>. In either case, I'd have to supply
another password. I could do it that way, but it requires even
stronger magic than I'm using so far (and I think it could be a bit
less secure than the solution I plan to present).
</p>
</li>

<li>
<p>
Have another look at <A HREF="#Diag1">Diagram 1</A>. Notice the
arrows? Yes, Virginia, they really are one-way arrows. (The link
between <strong>jump</strong> and <strong>backup</strong> is probably
two-way in real life, but the exercise is more powerful if it's
one-way, so let's go with the diagram as it is.)
</p>

<p>
To get from my laptop to the test machine, I go via an SSH proxy,
which I haven't drawn because it would complicate the diagram
unnecessarily. A firewall might be set up the same way. In either
case, I can establish an SSH session from my laptop to the other
machine; but I can't do the reverse. It's like a diode.
</p>

<p>
I'm going to show you how an SSH tunnel allows access in the other
direction. Not only that, but it will make <strong>jump</strong>
directly accessible from <strong>test</strong> as well!
</p>
</li>

<li>
<p>
One final point about <tt>ssh/scp</tt>. If I do nothing special, when
I run those <tt>scp</tt> commands above, I'll get a prompt
like:

<pre>
	henry@test's password:
</pre>

and I will have to enter my password before the copy will take place.
That's not very helpful for an automatic process.
</p>

<p>

</p>
</li>
</ul>

<h3>Look, ma! No hands!</h3>

<p>
Whenever I expect to go to a machine more than once or twice, I take
the time to set up <tt>$HOME/.ssh/authorized_keys</tt> on the
destination machine. See <tt>ssh(1)</tt>. Instead of using passwords,
the SSH client on my laptop

<pre>
	proves that it has access to the private key and the server
	checks that the corresponding public key is authorized to
	accept the account.

				- ssh(1)
</pre>

It all happens "under the covers". I invoke <tt>scp</tt>, and the
files get transferred. That's convenient for me, absolutely essential
for a cron job.
</p>

<h3>Permissions</h3>

<p>
There's more than one way to skin this cat. I decided to use a cron
job on <strong>test</strong> to copy the required backup data to an
intermediate repository. I don't simply copy the directories, I
package them with <tt>tar</tt>, and compress the tarfile with
<tt>bzip2</tt>. I then make myself the owner of the result. (I could
have used <tt>zip</tt>.)
</p>

<p>
The point of the <tt>tar</tt> is to preserve the permissions of all
the files and directories being backed up. The point of the
<tt>bzip2</tt> is to make the data to be transferred across the
network, and later copied to tape, as small as possible.
(Theoretically, some of these commendable goals may be defeated to
varying degrees by "smart" technology. For instance,
<tt>rsync</tt> has the ability to compress; and most modern
backup hardware performs compression in the tape drive.) The point of
the <tt>chown</tt> is to make the package accessible to a cron job on
my laptop running as me (an unprivileged user on
<strong>test</strong>).
</p>

<p>
Here's the root crontab entry:

<pre>
0 12 * * * /usr/local/sbin/packitup.sh &gt;/dev/null 2&gt;&amp;1  # Backup
</pre>

At noon each day, as the root user, run a script called
<strong>packitup.sh</strong>:

<pre>
#! /bin/sh
#       packitup.sh - part of the backup system

# This script collates the data on test in preparation for getting it
# backed up off-machine.

# Run on:       test
# from:         cron or another script.

BACKUP_PATHS='
/var/cvs_repository
/etc
'
REPO=/var/BACKUPS
USER=henry

        date_stamp=`date '+%Y%m%d'`

        for dir in $BACKUP_PATHS
        do
                echo Processing $dir
                pdir=`dirname $dir`
                tgt=`basename $dir`
                cd $pdir
                outfile=$REPO/$tgt.$date_stamp.tar.bz2
                tar -cf - $tgt | bzip2 &gt; $outfile
                chown $USER $outfile
        done
        exit
</pre>

If you are struggling with any of what I've written so far, this
article may not be for you. I've really only included much of it for
completeness. Now it starts to get interesting.
</p>

<h3>rsync</h3>

<p>
Instead of <tt>scp</tt>, I'm going to use <tt>rsync</tt> which invokes
<tt>ssh</tt> to access remote machines. Both <tt>scp</tt> and
<tt>rsync</tt> rely on SSH technology; this will become relevant when
we get to the tunnels.
</p>

<p>
Basically, <tt>rsync(1)</tt> is like <tt>scp</tt> on steroids. If I
have a 100MB of data to copy and 90% is the same as before,
<tt>rsync</tt> will copy a wee bit more than 10MB, whereas
<tt>scp</tt> will copy all 100MB. Every time.
</p>

<a name="tunnelsf"></a>
<h3>Tunnels, finally!</h3>

<p> 
Don't forget, I've already set up certificates on all the remote
machines.
</p>

<p>
To set up a tunnel so that <strong>test</strong> can access
<strong>jump</strong> directly, I simply need:

<pre class="code">
	ssh -R 9122:jump:22 test
</pre>
</p>

<p>
Let's examine this carefully because it is the essence of this
article. The command says to establish an SSH connection to
<strong>test</strong>. "While you're at it, I want you to listen on a
port numbered <strong>9122</strong> on <strong>test</strong>. If
someone makes a connection to port <strong>9122</strong> on
<strong>test</strong>, connect the call through to port
<strong>22</strong> on <strong>jump</strong>." The result looks like
this:
</p>

<img src="misc/grebler/diag2.jpeg" alt="Diagram 2" width="582" height="347"/>

<p>
</p>

<p>
So, immediately after the command in the last box, I'm actually logged
in on <strong>test</strong>. If I now issue the command

<pre class="code">
	henry@test:~$ ssh -p 9122 localhost
</pre>

I'll be logged in on <strong>jump</strong>. Here's what it all looks
like (omitting a lot of uninteresting lines):

<pre class="code">
	henry@laptop:~$ ssh -R 9122:jump:22 test
	henry@test:~$ ssh -p 9122 localhost
Last login: Wed Feb  3 12:41:18 2010 from localhost.localdomain
	henry@jump:~$ 
</pre>
</p>

<p>
It's worth noting that you don't "own" the tunnel; anyone can use it.
And several sessions can use it concurrently. But it only exists while
your first <tt>ssh</tt> command runs. When you exit from
<strong>test</strong>, your tunnel disappears (and all sessions using
the tunnel are broken).
</p>

<p>
Importantly, by default, "the listening socket on the server will be
bound to the loopback interface only" - <tt>ssh(1)</tt>. So, by
default, a command like the following won't work:

<pre class="code">
	ssh -p 9122 test		# usually won't work
	ssh: connect to address 192.168.0.1 port 9122: Connection refused
</pre>
</p>


<p>
Further, look carefully at how I've drawn the tunnel. It's like that
for a reason. Although, <i>logically</i> the tunnel seems to be a
direct connection between the 2 end machines, <strong>test</strong>
and <strong>jump</strong>, the physical data path is via
<strong>laptop</strong>. You haven't managed to skip a machine; you've
only managed to avoid a <i>manual</i> step. There may be performance
implications. 
</p>

<h3>Sometimes I Cheat</h3>

<p>
The very astute amongst my readers will have noticed that this hasn't
solved the original problem. I've only tunneled to
<strong>jump</strong>; the problem was to get the data to
<strong>backup</strong>. I could do it using SSH tunnels, but until
next time, you'll have to take my word for it. Or work it out for
yourself; it should not be too difficult.
</p>

<p>
But, as these things sometimes go, in this case, I had a much simpler
solution:

<pre class="code">
	henry@laptop:~$ ssh jump
	henry@jump:~$ sudo bash
Password:
	root@jump:~# mount backup:/backups /backups
	root@jump:~# exit
	henry@jump:~$ exit
	henry@laptop:~$ 
</pre>

I've NFS-mounted the remote directory <strong>/backups</strong> on its
local namesake. I only need to do this once (unless someone reboots
<strong>jump</strong>). Now, an attempt to write to the directory
<strong>/backups</strong> on <strong>jump</strong> results in the data
being written into the directory <strong>/backups</strong> on
<strong>backup</strong>.
</p>

<h3>The Final Pieces</h3>

<p>
Ok, in your mind, log out of all the remote machines mentioned in 
<A HREF="#tunnelsf">Tunnels, finally!</A>. In real life, this is going
to run as a cron job.
</p>

<p>
Here's my (ie user <strong>henry</strong>'s) crontab entry on
<strong>laptop</strong>:

<pre class="code">
	30 12 * * * /usr/local/sbin/invoke_backup_on_test.sh
</pre>

At 12:30 pm each day, as user <strong>henry</strong>, run a script
called <strong>invoke_backup_on_test.sh</strong>:

<pre class="code">
#! /bin/sh
#       invoke_backup_on_test.sh - invoke the backup

# This script should be run from cron on laptop.

# Since test cannot access the backup network, it cannot get to the
# real "backup" directly. An ssh session from "laptop" to "test"
# provides port forwarding to allow ssl to access the jump machine I
# have nfs-mounted /backups from "backup" onto the jump machine.

# It's messy and complicated, but it works.

        ssh -R 9122:jump:22 test /usr/local/sbin/copy2backup.sh
</pre>
</p>

<p>
Of course I had previously placed <strong>copy2backup.sh</strong> on
<strong>test</strong>.

<pre class="code">
#! /bin/sh
#       copy2backup.sh - copy files to be backed up

# This script uses rsync to copy files from /var/BACKUPS to
# /backups on "backup".

# 18 Sep 2009 Henry Grebler    Perpetuate (not quite the right word) pruning.
# 21 Aug 2009 Henry Grebler    Avoid key problems.
#  6 Aug 2009 Henry Grebler    First cut.
#=============================================================================#

# Note. Another script, prune_backups.sh, discards old backup data
# from the repo. Use rsync's delete option to also discard old backup
# data from "backup".

        PATH=$PATH:/usr/local/bin

# Danger lowbrow: Do not put tabs in the next line.

        RSYNC_RSH="ssh -o 'NoHostAuthenticationForLocalhost yes' \
                -o 'UserKnownHostsFile /dev/null' -p 9122"
        RSYNC_RSH="`echo $RSYNC_RSH`"
        export RSYNC_RSH

        rsync -urlptog --delete --rsync-path bin/rsync /var/BACKUPS/ \
                localhost:/backups
</pre>
</p>


<h3>Really important stuff</h3>

<p>
Notes on <strong>copy2backup.sh</strong>.
</p>

<ul>
<li>
<pre class="code">
	PATH=$PATH:/usr/local/bin
</pre>
The way that <strong>copy2backup.sh</strong> is invoked (on
<strong>test</strong>) from cron (on <strong>laptop</strong>) via
<strong>invoke_backup_on_test.sh</strong> means that you should not
count on any but the most basic of items in PATH. Even safer, would be
to define even things like <tt>/bin</tt>.

</li>

<li>
<pre class="code">
	RSYNC_RSH=...
	...
	export RSYNC_RSH
</pre>

These lines provide rsync with with details of the <tt>rsh</tt>
command (in this case, <tt>ssh</tt>) to run. Depending on which
version of <tt>ssh</tt> your machine has, and the options set in the
various SSH config files, your <tt>ssh</tt> may try to keep track of
the certificates of the SSH daemons on the remote machines. Using
<strong>localhost</strong> the way that we do here, the actual machine
at the end of the tunnel (and therefore its fingerprint or
certificate) may change from one run to the next. <tt>ssh</tt> will
try to protect you from the possibility of certain known forms of
attack. These incantations try to get <tt>ssh</tt> to keep out of the
way. It's safe enough on an internal private network; more risky if
you are venturing into the badlands of the Internet.

</li>
<li>
<tt>rsync</tt> is a pretty powerful program. Its options and
arguments can be complicated. I do not propose to cover chapter and
verse here. Check the man page, <tt>rsync(1)</tt>. I will just say that the
trailing slash in the "from" argument (<strong>/var/BACKUPS/</strong>)
<i>is</i> significant. It says to copy the <i>contents</i> of the
specified directory. Omitting the trailing slash would mean to copy
the directory. Recursion is specified in an earlier option (-r).
</li>

<li>
<pre class="code">
	--rsync-path bin/rsync
</pre>

When <tt>rsync</tt> runs on the local machine (in this case,
<strong>test</strong>), it makes an SSH connection to the remote
machine ("<strong>localhost</strong>" = <strong>jump</strong>) and
tries to run an <tt>rsync</tt> on the remote machine. This argument
indicates where to find the remote <tt>rsync</tt>. In this case, it
will be in the <strong>bin</strong> subdirectory of my (user
<strong>henry</strong>'s) HOME directory on <strong>jump</strong>. In
other words, I'm running a private copy of <tt>rsync</tt>.

<li>
<strong>prune_backups.sh</strong> and <strong>--delete</strong> -
these two components go together. They can be dangerous. I'll explain later.
</li>
</ul>

<h3>Recap</h3>

<ul>
<li>
Everyday at noon <strong>packitup.sh</strong> on
<strong>test</strong> gathers the data to be backed up into a local
holding repository.
</li>

<li>
Everyday, half an hour later, if my laptop is turned on, a local
script, <strong>invoke_backup_on_test.sh</strong> is invoked. It
simply connects to <strong>test</strong>, establishing an SSH tunnel
as it does, and invokes the script which performs the backup,
<strong>copy2backup.sh</strong>.
</li>

<li>
<strong>copy2backup.sh</strong> does the actual copy over the SSH
tunnel using <tt>rsync</tt> to transport the data.
</li>

<li>
When <strong>copy2backup.sh</strong> completes, it exits, causing the
<tt>ssh</tt> command to exit and the SSH tunnel to be torn down.
</li>

<li>
Next day, it all starts over again.
</li>
</ul>

<h3>Wrinkles</h3>

<p>
It's great when you finally get something like this to work. All the
pieces fall into place - it's very satisfying.
</p>

<p>
Of course, you monitor things carefully for the first few days. Then
you check less frequently. You start to gloat.
</p>

<p>
... until a few weeks elapse and you gradually develop a gnawing
concern. The data is incrementally increasing in size as more days
elapse. At first, that's a good thing. One backup good, two backups
better, ... Where does it end? Well, at the moment, it doesn't. Where
should it end? Good question. But, congratulations on realising that
it probably should end.
</p>

<p>
When I did, I wrote <strong>prune_backups.sh</strong>. You can see
when this happened by examining the history entries in
<strong>copy2backup.sh</strong>: about 6 weeks after I wrote the first
cut. Here it is:

<pre class="code">
#! /bin/sh
#	prune_backups.sh - discard old backups

# 18 Sep 2009 Henry Grebler    First cut.
#=============================================================================#

# Motivation

# packitup.sh collates the data on test in preparation for getting
# it backed up off-machine. However, the directory just grows and
# grows. This script discards old backup sets.

#----------------------------------------------------------------------#

REPO=/var/BACKUPS
KEEP_DAYS=28		# Number of days to keep

	cd $REPO
	find $REPO -type f -mtime +$KEEP_DAYS -exec rm {} \;
</pre>

Simple, really. Just delete anything that is more than 28 days old. NB
<strong>more than</strong> rather than <strong>equal to</strong>. If for
some reason the cron job doesn't run for a day or several, when next
it runs it will catch up. This is called <strong>self-correcting</strong>.
</p>

<p>
Here's the crontab entry:
<pre class="code">
	0 10 * * * /usr/local/sbin/prune_backups.sh &gt;/dev/null 2&gt;&amp;1
</pre>

At 10 am each day, as the root user, run a script called
<strong>prune_backups.sh</strong>. 
</p>


<p>
But, wait. That only deletes old files in the repository on
<strong>test</strong>. What about the copy of this data on
<strong>jump</strong>?!
</p>

<p>
Remember the <strong>--delete</strong> above? It's an rsync option; a
very dangerous one. That's not to say that you shouldn't use it; just
use it with extra care.
</p>

<p>
It tells <tt>rsync</tt> that if it discovers a file on the destination
machine that is not on the source machine, then it should delete the
file on the destination machine. This ensures that the local and
remote repositories stay truly in sync.
</p>

<p>
However, if you screw it up by, for instance, telling <tt>rsync</tt>
to copy an empty directory to a remote machine's populated directory,
and you specify the <strong>--delete</strong> option, you'll delete
all the remote files and directories. You have been warned: use it
with extra care.
</p>

<h3>Risks and Analysis</h3>

<p>
There is a risk that port 9122 on <strong>test</strong> may be in use
by another process. That happened to me a few times. Each time, it
turned out that I was the culprit! I solved that by being more
disciplined (using another port number for interactive work).
</p>

<p>
You could try to code around the problem, but it's not easy. 

<pre class="code">
	ssh -R 9122:localhost:22 fw
	Warning: remote port forwarding failed for listen port 9122
</pre>

Even though it could not create the tunnel (aka port forwarding), ssh
has established the connection. How do you know if port forwarding
failed? 
</p>

<p>
More recent versions of <tt>ssh</tt> have an option which caters for
this: <tt>ExitOnForwardFailure</tt>, see <tt>ssh_config(5)</tt>.
</pre>


<p>
If someone else has created a tunnel to the right machine, it doesn't
matter. The script will simply use the tunnel unaware that it is
actually someone else's tunnel.
</p>

<p>
But if the tunnel connects to the wrong machine?
</p>

<p>
Hey, I don't provide <i>all</i> the answers; I simply mention the risks,
maybe make some suggestions. In my case, it was never a serious
problem. Occasionally missing a backup is not a disaster. The scripts
are all written to be tolerant to the possibility that they may not
run every day. When they run, they catch up.
</p>

<p>
A bigger risk is the dependence on my laptop. I tried to do something
about that but without long-term success. I'm no longer there; the
laptop I was using will have been recycled.
</p>

<p>
I try to do the best job possible. I can't always control my environment.
</p>

<h3>Debugging</h3>

<p>
Because this setup involves cron jobs invoking scripts which in turn
invoke other scripts, this can be a nightmare to get right. (Once it's
working, it's not too bad.) 
</p>

<p>
My recommendation: run the pieces by hand.
</p>

<p>
So start at a cron entry (which usually has output redirected to
<tt>/dev/null</tt>) and invoke it manually (as the relevant user)
<i>without</i> redirecting the output.
</p>

<p>
If necessary, repeat, following the chain of invoked scripts. In other
words, for each script, invoke each command manually. It's a bit
tiresome, but none of the scripts is very long. Apart from the comment
lines, they are all very dense. The best example of density is the
<tt>ssh</tt> command which establishes the tunnel.
</p>

<p>
Use your mouse to copy and paste for convenience and to avoid
introducing transcription errors.
</p>

<h3>Coming Up</h3>

<p>
That took much longer than I expected. I'll leave at least one other
example for another time.
</p>

<hr>

<p>
<a id="grebler.html_1"></a><a href="#grebler.html_1_back">[1]</a>
A UK/AU expression, approximately "boring stuff you've heard before".<br>
 -- Ben
</p>


<br clear="all" />

<script type='text/javascript'>
digg_url = 'http://linuxgazette.net/172/grebler.html';
digg_title = 'Tunnel Tales 1';
digg_bodytext = '<p>  I met Justin when I was contracting to one of the world\'s biggest computer companies, OOTWBCC, building Solaris servers for one of Australia\'s biggest companies (OOABC). Justin is in EBR (Enterprise Backup and Recovery). (OOTWBCC is almost certainly the world\'s most prolific acronym generator (TWMPAG).) I was writing scripts to automate much of the install of EBR. </p> ';
digg_topic = 'linux_unix';
</script>
<script src="http://digg.com/tools/diggthis.js" type="text/javascript"></script>


<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:172/grebler.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
<hr>
<p>
<img align="left" alt="[BIO]" src="../gx/authors/grebler.jpg" class="bio">
</p>

<em>
<p>
Henry was born in Germany in 1946, migrating to Australia in 1950. In
 his childhood, he taught himself to take apart the family radio and put
 it back together again - with very few parts left over.
</p>

<p>
After ignominiously flunking out of Medicine (best result: a sup in
 Biochemistry - which he flunked), he switched to Computation, the name
 given to the nascent field which would become Computer Science. His
 early computer experience includes relics such as punch cards, paper
 tape and mag tape.
</p>

<p>
He has spent his days working with computers, mostly for computer
 manufacturers or software developers. It is his darkest secret that he
 has been paid to do the sorts of things he would have paid money to be
 allowed to do. Just don't tell any of his employers.
</p>

<p>
He has used Linux as his personal home desktop since the family got its
 first PC in 1996. Back then, when the family shared the one PC, it was a
 dual-boot Windows/Slackware setup. Now that each member has his/her own
 computer, Henry somehow survives in a purely Linux world.
</p>

<p>
He lives in a suburb of Melbourne, Australia.
</p>



</em>

<br clear="all">


<!-- *** END author bio *** -->

<div id="articlefooter">

<p>
Copyright &copy; 2010, Henry Grebler. Released under the <a
href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>

<p>
Published in Issue 172 of Linux Gazette, March 2010
</p>

</div>

<div id="previousnextbottom">
<A HREF="dyckoff.html" >&lt;-- prev</A> | <A HREF="peterson.html" >next --&gt;</A>
</div>

</div>
</div>

<script src="http://www.google-analytics.com/urchin.js"
type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-1204316-1";
urchinTracker();
</script>







<img src="../gx/tux_86x95_indexed.png" id="tux" alt="Tux"/>

</body>
</html>

