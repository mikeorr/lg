<!DOCTYPE html PUBLIC '-//W3C//DTD XHTML 1.0 Transitional//EN'
		 'http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd'>
<html xmlns='http://www.w3.org/1999/xhtml' lang='utf-8' xml:lang='utf-8'>
<head>
<title>Finding duplicate images, redux</title>
<meta http-equiv='Content-Type' content='text/html; charset=utf-8' />
<link rel='stylesheet' type='text/css' href='../../../lg.css' />
</head>
<body>
<a href="../../../"><img alt="Linux Gazette" src="../../../gx/2003/newlogo-blank-200-gold2.jpg" id="logo" /></a><img alt="Tux" src="../../../gx/tux_86x95_indexed.png" id="tux" /><p id="fun">...making Linux just a little more fun!</p><div class='content articlecontent'><a name="top"></a><h3>Finding duplicate images, redux</h3>
<p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>

<p>

</p>
</b><br />
<b>Tue, 28 Dec 2010 12:58:41 -0500</b>
</p>

<p>
Amusing example of serendipity: one of our readers just sent me an email
letting me know that a link to a SourceForge project in one of our
articles was outdated and needed to be pointed to the new, renamed
version of the project. I changed it after taking a moment to verify the
SF link - and noticed that some of the project functionality was
relevant to Neil Youngman's question of a couple of months ago.
</p>

<p>
Pulling down the (small) project tarball and reading the docs
supported that impression:
</p>

<pre>
  'repeats' searches for duplicate files using a multistage process. Ini-
  tially, all files in the specified directories (and all of their subdi-
  rectories) are listed as potential duplicates.  In the first stage, all
  files with a unique filesize are declared unique and are  removed  from
  the list.  In the second stage, any files which are actually a hardlink
  to another file are removed, since they don't actually take up any more
  disk space.  Next, all files for which the first 4096 bytes (adjustable
  with the -m option) have a unique filehash are declared unique and  are
  removed from the list.  Finally, all files which have a unique filehash
  (for the entire file) are declared unique  and  are  removed  from  the
  list.   Any remaining files are assumed to be duplicates and are listed
  on stdout.
</pre>

<p>
The project is called "littleutils", by Brian Lindholm. There's a number
of other handy little utilities in there, all worth exploring.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-finding_duplicate_images_redux">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Mulyadi Santosa [mulyadi.santosa at gmail.com]
</p>

<p>

</p>
</b><br />
<b>Wed, 29 Dec 2010 01:25:33 +0700</b>
</p>

<p>
On Wed, Dec 29, 2010 at 00:58, Ben Okopnik &lt;ben at linuxgazette.net&gt; wrote:
</p>

<pre>
&gt; The project is called "littleutils", by Brian Lindholm. There's a number
&gt; of other handy little utilities in there, all worth exploring.
</pre>

<p>
Reminds me to ssdeep as well (<a href='http://ssdeep.sourceforge.net/'>http://ssdeep.sourceforge.net/</a>). The
author call it sliding hash or something like that...
</p>

<p>
-- 
regards,
</p>

<p>
Mulyadi Santosa
Freelance Linux trainer and consultant
</p>

<p>
blog: the-hydra.blogspot.com
training: mulyaditraining.blogspot.com
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-finding_duplicate_images_redux">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>

<p>

</p>
</b><br />
<b>Tue, 28 Dec 2010 14:02:51 -0500</b>
</p>

<p>
On Wed, Dec 29, 2010 at 01:25:33AM +0700, Mulyadi Santosa wrote:
</p>

<pre>
&gt; On Wed, Dec 29, 2010 at 00:58, Ben Okopnik &lt;ben at linuxgazette.net&gt; wrote:
&gt; &gt; The project is called "littleutils", by Brian Lindholm. There's a number
&gt; &gt; of other handy little utilities in there, all worth exploring.
&gt; 
&gt; Reminds me to ssdeep as well (<a href='http://ssdeep.sourceforge.net/'>http://ssdeep.sourceforge.net/</a>). The
&gt; author call it sliding hash or something like that...
</pre>

<p>
I've just taken a look at "ssdeep" (it uses a "rolling hash" method to
compute "Context-Triggered Piecewise Hashes"); very interesting, but
wouldn't help Neil much, unfortunately.
</p>

<p>
The point of CTPHs is to allow you to identify small differences (and
identify the blocks where the difference occurs) in a large number of
files. That's useful because a system attacker may modify a program to
add a back door - and then obscure their tracks by randomly changing one
harmless bit in thousands of other files (the example given was changing
"This program cannot be run in DOS mode" to "This program cannot be run
on DOS mode"), which would create a huge list of MD5 mismatches, thus
hiding the hacked file.
</p>

<p>
"ssdeep" is a proof of concept for the CTPH technique. It's interesting
to note, though, that CTPHs are not guaranteed to be unique; in fact,
because they use a 6-bit hash, there's a 1 in 2^-6 probability of hash
collision. CTPH computation is also relatively slow - O(n log n) in the
worst case. However, when combined with a more traditional hash, they
can quickly sort out files with large modifications from ones with small
ones. I can see where, e.g., geneticists would find this useful.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-finding_duplicate_images_redux">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Mulyadi Santosa [mulyadi.santosa at gmail.com]
</p>

<p>

</p>
</b><br />
<b>Wed, 29 Dec 2010 02:12:53 +0700</b>
</p>

<p>
On Wed, Dec 29, 2010 at 02:02, Ben Okopnik &lt;ben at linuxgazette.net&gt; wrote:
</p>

<pre>
&gt; The point of CTPHs is to allow you to identify small differences (and
&gt; identify the blocks where the difference occurs) in a large number of
&gt; files. That's useful because a system attacker may modify a program to
&gt; add a back door - and then obscure their tracks by randomly changing one
&gt; harmless bit in thousands of other files (the example given was changing
&gt; "This program cannot be run in DOS mode" to "This program cannot be run
&gt; on DOS mode"), which would create a huge list of MD5 mismatches, thus
&gt; hiding the hacked file.
</pre>

<p>
Wow, Ben, I am amazed on how fast you deduced all these things right
after my reply few minutes ago :D
</p>

<p>
Looks like age doesn't bite you in :D
</p>

<p>
-- 
regards,
</p>

<p>
Mulyadi Santosa
Freelance Linux trainer and consultant
</p>

<p>
blog: the-hydra.blogspot.com
training: mulyaditraining.blogspot.com
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-finding_duplicate_images_redux">Back</a><hr width="50%" align="left" /><p><br /></p></div>
</body>
</html>