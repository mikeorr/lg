<!DOCTYPE html
	PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
	 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
<title>
Linux Gazette : August 2007 (#141) 
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link href="../lg.css" rel="stylesheet" type="text/css" media="screen, projection" />

<style type="text/css" media="screen, projection">
<!--

.twdtarticle {
	width: 84%;
}

.twdtarticle h1 {
	font-size:19px;
	text-align:center;
}

.lgcontent {
        width: 84%;
        margin-top: 30px;
}

-->
</style>

</head>

<body id="twdtbody">

<a href="../">
<img src="../gx/2003/newlogo-blank-200-gold2.jpg" alt="Linux Gazette" id="twdtlogo"/>
</a>
<p id="fun">...making Linux just a little more fun!</p>

<div id="navigation">

<a href="../index.html">Home</a>
<a href="../faq/index.html">FAQ</a>
<a href="../lg_index.html">Site Map</a>
<a href="../mirrors.html">Mirrors</a>
<a href="../mirrors.html">Translations</a>
<a href="../search.html">Search</a>
<a href="../archives.html">Archives</a>
<a href="../authors/index.html">Authors</a>
<a href="http://lists.linuxgazette.net/mailman/listinfo/">Mailing Lists</a>
<a href="../jobs.html">Join Us!</a>
<a href="../contact.html">Contact Us</a>
</div>

<div id="breadcrumbs1">

<a href="../index.html">Home</a> &gt;
<a href="index.html">August 2007 (#141)</a> &gt;
TWDT

</div>


<div class="content lgcontent">

<h2>August 2007 (#141):</h2>

<ul>

	<li><a href="#lg_mail">Mailbag</a>

	<li><a href="#lg_mail2">An Ongoing Discussion of Open Source Licensing Issues</a>

	<li><a href="#lg_talkback">Talkback</a>

	<li><a href="#lg_tips">2-Cent Tips</a>

	<li><a href="#lg_bytes">NewsBytes</a>, by <i>Howard Dyckoff and Samuel Kotel Bisbee-vonKaufmann</i></li>

	<li><a href="#anonymous">GRUB, PATA and SATA</a>, by <i>Anonymous</i></li>

	<li><a href="#brownss">An NSLU2 (Slug) Reminder Server</a>, by <i>Silas Brown</i></li>

	<li><a href="#kapil">Who is using your Network?</a>, by <i>Kapil Hari Paranjape</i></li>

	<li><a href="#lazar">Serving Your Home Network on a Silver Platter with Ubuntu</a>, by <i>Shane Lazar</i></li>

	<li><a href="#pfeiffer">One Volunteer Per Child - GNU/Linux and the Community</a>, by <i>Ren&eacute; Pfeiffer</i></li>

	<li><a href="#collinge">HelpDex</a>, by <i>Shane Collinge</i></li>

</ul>

</div>



<br />


<div class="content lgcontent">

<a name="lg_mail"></a>
<h1>Mailbag</h1>

</b>
</p>

<p>
<h3>This month's answers created by:</h3><strong>[  Amit Kumar Saha, Ben Okopnik, Kapil Hari Paranjape, Karl-Heinz Herrmann, Ren&eacute; Pfeiffer, Neil Youngman,  Raj Shekhar, Rick Moen, Samuel Kotel Bisbee-vonKaufmann, Thomas Adam  ]</strong>
<br />...and you, our readers!<br /><hr width="50%" align="center" size="3" /><h1>Still Searching</h1>
<hr />

<!-- Thread anchor: Sudden Failure on Centos --><a name='sudden_failure_on_centos'></a>
<h3>Sudden Failure on Centos</h3>
<p>
<b><p>
Smile Maker [britto_can at yahoo.com]
</p>
</b><br />
<b>Sat, 7 Jul 2007 02:35:58 -0700 (PDT)</b>
</p>

<p>
Folks ,
</p>

<p>
 I have got centos 5 on my box and i went through the default installation which creates LVM  and mounts in /.
</p>

<p>
When i just rebooted linux thrown error :
</p>

<pre>
"System lookup error: mount:undefined symbol:blkid_get_cache"
</pre>
Any advice.
</p>

<p>
Thanks &amp; regards,
Britto
</p>


<p>

</p>

<hr />


<!-- Thread anchor: XEN Installation Problems on Ubuntu 7.04 --><a name='xen_installation_problems_on_ubuntu_7_04'></a>
<h3>XEN Installation Problems on Ubuntu 7.04</h3>
<p>
<b><p>
Amit Kumar Saha [amitsaha.in at gmail.com]
</p>
</b><br />
<b>Mon, 2 Jul 2007 15:56:06 +0530</b>
</p>

<p>
Hi all
</p>

<p>
I finally installed Xen using Synaptic on a fresh installed version of
Ubuntu 7.04 (32-bit). For some reason the earlier efforts did not
succeed <img src="../gx/frown.png" alt=":("> However now I can boot to my custom Xen enabled 2.6.19
kernel.
</p>

<p>
Is there a problem with the amd-64 version of XEN? Can some one confirm this?
</p>

<p>
Cheers,
<pre>-- 
Amit Kumar Saha
[URL]:<a href="http://amitsaha.in.googlepages.com">http://amitsaha.in.googlepages.com</a>
</pre>

<hr />


<!-- Thread anchor: Clock problem --><a name='clock_problem'></a>
<h3>Clock problem</h3>
<p>
<b><p>
Jimmy Kaifiti [dgeemee03 at hotmail.com]
</p>
</b><br />
<b>Mon, 30 Jul 2007 12:27:48 +1200</b>
</p>

<p>
Hi , my name is Jimmy,can anyone help me fix the time on my PC.
I change the Battery so many time ,I mean the new CMOS Battery ,but my time is still not read correct
</p>

<p>

</p>

<hr />


<!-- Thread anchor: Keeping indices of filesystems to handle backup archives --><a name='keeping_indices_of_filesystems_to_handle_backup_archives'></a>
<h3>Keeping indices of filesystems to handle backup archives</h3>
<p>
<b><p>
Ren&eacute; Pfeiffer [lynx at luchs.at]
</p>
</b><br />
<b>Wed, 11 Jul 2007 18:06:59 +0200</b>
</p>

<p>
Hello!
</p>

<p>
So, here's my question about that index problem I mentioned in the
answer to Ben's question about backups.
</p>

<p>
Imagine you have two backups servers. Server A keeps a rather recent
copy of live servers. Server B tries to archive stuff from server A in
order to keep recent backups recent and to save space on server A. Of
course this means that server B keeps accumulating files and
directories. In order to avoid this one could think of a strategy of
deleting files according to a mathematical distribution. There's a tool
called fileprune which does just that
(<a href="http://www.spinellis.gr/sw/unix/fileprune/">http://www.spinellis.gr/sw/unix/fileprune/</a>). I found it by browsing
through an old issue of ;login:. fileprune deletes data by using a
Gaussian, exponential oder Fibonacci distribution. The problem is that
fileprune needs to read the metadata of the entire tree into memory
before it can decide which files to delete. Backup storage may have
millions of files and directories.
</p>

<p>
I'd like to ask the filesystem directly "which files have an access time
of older than X" and get an answer. In the database world you have
indices for that. (Most) filesystems don't have such things (at least
not exported to userspace), so you would have to maintain one for
yourself. This could be done by the Linux kernel's Inotify API which
tells you what changes were done in a specific filesystem tree. I tried
it, it works, but I have no idea if I catch every modification when
rsync or other tools come along (I am going to test this with higher load
as soon as my load is lower).
</p>

<p>
Another way is to see whether existing filesystems have similar
functionality. I believe Reiser4 went into this direction. Yet another
way is to parse the filesystem tree seperately in order to maintain a
metadata index.
</p>

<p>
Do you have some more ideas besides writing a new filesystem?
</p>

<p>
Just being curious,
Ren&eacute;.
</p>


<p>

</p>

<hr />


<!-- Thread anchor: Linksys router --><a name='linksys_router'></a>
<h3>Linksys router</h3>
<p>
<b><p>
Stelian Iancu [stelu at siancu.net]
</p>
</b><br />
<b>Tue, 3 Jul 2007 15:05:06 +0200</b>
</p>

<p>
Hello all,
</p>

<p>
I have recently received a Linksys WRT54GS wireless router from a
friend of mine who had it laying around and didn't need it anymore. He
told me he changed the firmware of the device several times, when he
originally bought it and now he doesn't remember which firmware is
installed.
</p>

<p>
So how can I figure out which one it is? I tried to connect with
Firefox to the specific IP administrative addresses of DD-WRT and
OpenWRT but there's no response.
</p>

<p>
Any help is more than appreciated!
</p>

<p>
With regards,
Stelian Iancu
</p>

<p>

</p>

<hr />

<h1>Our Mailbag</h1>
<hr />

<!-- Thread anchor: Things Debian Project Leaders Do --><a name='things_debian_project_leaders_do'></a>
<h3>Things Debian Project Leaders Do</h3>
<p>
<b><p>
Kapil Hari Paranjape [kapil at imsc.res.in]
</p>
</b><br />
<b>Fri, 27 Jul 2007 17:09:08 +0530</b>
</p>

<p>
Hello,
</p>

<p>
Well one of the things at least!
	<a href="http://sam.zoy.org/zzuf/">http://sam.zoy.org/zzuf/</a>
(Also available in Debian testing of course!)
Quite a neat toy!
</p>

<p>
Regards,
</p>

<p>
Kapil.
--
</p>


<p>

</p>

<p><b>[  <a name="mb-things_debian_project_leaders_do"></a> <a href="misc/lg/things_debian_project_leaders_do.html">Thread continues here (3 messages/3.00kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: Latex Editors --><a name='latex_editors'></a>
<h3>Latex Editors</h3>
<p>
<b><p>
Amit Kumar Saha [amitsaha.in at gmail.com]
</p>
</b><br />
<b>Thu, 12 Jul 2007 16:22:43 +0530</b>
</p>

<p>
Hey all,
</p>

<p>
Suggest me a good, complete LATEX system - editors, typesetting system
- preferably for Ubuntu (if thats important).
</p>

<p>
Cheers,
<pre>-- 
Amit Kumar Saha
[URL]:<a href="http://amitsaha.in.googlepages.com">http://amitsaha.in.googlepages.com</a>
</pre>

<p><b>[  <a name="mb-latex_editors"></a> <a href="misc/lg/latex_editors.html">Thread continues here (13 messages/12.67kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: perl: forking and children.... again --><a name='perl__forking_and_children_again'></a>
<h3>perl: forking and children.... again</h3>
<p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Wed, 4 Jul 2007 22:37:56 -0400</b>
</p>

<p>
As sometimes happens, shortly - or not so shortly - after we finish a
discussion in TAG, more info about the problem pops up out of nowhere,
usually as a result of a completely unrelated Google search. I
<strong>swear</strong>, if we were to have a discussion about Martian blood-drinking
weasels, next week I'd run into a "The Vampire Mustelids of Ares: A
Personal Interview" while searching for soap-scum removal info...
</p>

<p>
Anyway, Karl-Heinz: while I was fiddling about trying to come up with a
"max-spread" sorting algorithm [1], I ran across a reference to
Parallel::ForkManager on '<a href="http://perlmonks.org">http://perlmonks.org</a>'. Doing a quick lookup on
CPAN came back with this (snipped from the documentation):
</p>

<pre>
This module is intended for use in operations that can be done in
parallel where the number of processes to be forked off should be
limited.  Typical use is a downloader which will be retrieving
hundreds/thousands of files.
 
The code for a downloader would look something like this:
 
[...]
</pre>
This sounds like exactly the kind of thing you were describing. It
allows nicely fine-grained individual control of the child processes,
etc. - take a look!
</p>

<p>
<a href="http://search.cpan.org/author/DLUX/Parallel-ForkManager-0.7.5/ForkManager.pm">http://search.cpan.org/author/DLUX/Parallel-ForkManager-0.7.5/ForkManager.pm</a>
</p>

<p>
There's also Parallel::ForkControl -
</p>

<p>
<a href="http://search.cpan.org/author/BLHOTSKY/Parallel-ForkControl-0.04/lib/Parallel/ForkControl.pm">http://search.cpan.org/author/BLHOTSKY/Parallel-ForkControl-0.04/lib/Parallel/ForkControl.pm</a>
</p>




<p>
[1] For the mathematicians among us, you might find this to be fun. See
my next post.
</p>

<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>

<p><b>[  <a name="mb-perl__forking_and_children_again"></a> <a href="misc/lg/perl__forking_and_children_again.html">Thread continues here (2 messages/3.46kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: MI/X PPC Classic Macintosh X Window server (freeware) --><a name='mi_x_ppc_classic_macintosh_x_window_server_freeware'></a>
<h3>MI/X PPC Classic Macintosh X Window server (freeware)</h3>
<p>
<b><p>
axel.stevens [axel.stevens at webcreator.be]
</p>
</b><br />
<b>Mon, 02 Jul 2007 13:17:38 +0200</b>
</p>

<p>
Martin, Ben
</p>

<p>
There are still .sit.bin files to be found.
</p>

<p>
I located 2 files - mix68k.sit.bin and mixppc.sit.bin both 1,8 MB
</p>

<p>
best of luck
</p>

<p>
axel stevens
</p>

<p>
support engineer - Macintosh, Linux, Windows
</p>

<p>
Belgium
</p>

<p>

</p>

<p><b>[  <a name="mb-mi_x_ppc_classic_macintosh_x_window_server_freeware"></a> <a href="misc/lg/mi_x_ppc_classic_macintosh_x_window_server_freeware.html">Thread continues here (2 messages/0.99kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: Tracking load average issues --><a name='tracking_load_average_issues'></a>
<h3>Tracking load average issues</h3>
<p>
<b><p>
Neil Youngman [ny at youngman.org.uk]
</p>
</b><br />
<b>Wed, 18 Jul 2007 09:27:12 +0100</b>
</p>

<p>
I was asked to look at a system that had a consistent load average around 5.3 
to 5.5. Now I know very little about how to track down load average issues, 
so I haven't been able to find much. The CPU usage is about 90% idle, so it's 
not CPU bound.
</p>

<p>
I googled for "load average", "high load average" and "diagnose load average" 
and I found very little of use. the one thing I found was that if it's 
processes stuck waiting on I/O "ps ax" should show processes in state "D". 
There are none visible on this box.
</p>

<p>
Do the gang know of any good resources for diagnosing load average issues or 
have any useful tips?
</p>

<p>
Neil Youngman
</p>

<p>

</p>

<p><b>[  <a name="mb-tracking_load_average_issues"></a> <a href="misc/lg/tracking_load_average_issues.html">Thread continues here (12 messages/18.50kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: TUX Images --><a name='tux_images'></a>
<h3>TUX Images</h3>
<p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Fri, 6 Jul 2007 15:43:29 -0400</b>
</p>

<p>
----- Forwarded message from Scott Rainey &lt;scott.rainey@thelinuxfund.org&gt; -----
</p>

<pre>
Date: Wed, 27 Jun 2007 14:19:54 -0700
From: Scott Rainey &lt;scott.rainey@thelinuxfund.org&gt;
Reply-To: scott.rainey@thelinuxfund.org
To: editor@linuxgazette.net
Subject: TUX Images
</pre>
Hi,
</p>

<p>
I'm looking for a large format vector-based digital versions of Tux, both 
color and monochrome.  I'm even willing to pay for a really good one in 
monochrome suitable for sand-blasting on glass.
</p>

<p>
Do you know whom I should contact?
</p>

<p>
All the best,
</p>

<p>
Scott
</p>

<p>
----- End forwarded message -----
</p>

<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>

<p><b>[  <a name="mb-tux_images"></a> <a href="misc/lg/tux_images.html">Thread continues here (2 messages/2.44kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: need some help --><a name='need_some_help'></a>
<h3>need some help</h3>
<p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Wed, 11 Jul 2007 11:49:04 -0400</b>
</p>

<p>
On Tue, Jul 10, 2007 at 12:34:05PM +0530, Nayanam,Sarsij wrote:
</p>

<pre>
&gt;    I am writing a shell script to run on MC/SG cluster , and I am facing an
&gt;    issue as mentioned below:
&gt; 
&gt;    if we have a package with a dash in the name say sgpkg-cust :
&gt; 
&gt;    # PKGsgpkg-cust=hello
&gt;    sh: PKGsgpkg-cust=hello:  not found.
&gt;    # PKGsgpkg=hello
&gt;    # echo $PKGsgpkg-cust
&gt;    hello-cust
</pre>

<p>
This is not surprising; a dash is not a valid character in a variable
for Bourne-derived shells.
 
</p>

<pre>
&gt;    I have a function get_package_fqdn which starts as below:
&gt; 
&gt;        37  get_package_fqdn()
&gt;        38  {
&gt;        39      eval var=$`echo PKG$1`
&gt;        40      if [[ -z $var ]]; then
&gt;                [...]
&gt;        64      fi
&gt;        65  }
&gt; 
&gt;    we will notice that if $1=sgpkg-cust, var will be equal to "-cust" and the
&gt;    rest of the funcion "if[[ -z $var ]];" will not be used and nothing will
&gt;    be introduced in the PKG$packagename variable.
</pre>

<p>
This isn't shell-specific, but an excellent Perl programmer named
Mark-Jason Dominus has a writeup called "Why it's stupid to use a
variable as a variable name" (<a href="http://perl.plover.com/varvarname.html">http://perl.plover.com/varvarname.html</a>).
The above problem is explicitly cited. In short: since variable names
are restricted to a specific set of characters, and the set of
characters that could be contained in your '$1' is essentially
arbitrary, you're creating a problem when you do that. So don't do that.
</p>

<p>
In Perl, the answer is "use a hash". In Bash, well, you need to rethink
what it is that you're trying to do and use different functionality.  As
a general approach, you could try "flattening" that arbitrary character
set - be sure to do do in both populating <em>and</em> reading the strings:
</p>

<p>
<pre class="code">
package_name=$(echo -n "PKG$1"|tr -c 'a-zA-Z0-9_' '_')
</pre>
<pre class="code">
ben@Tyr:~$ var=$(echo -n "xyzabc@$%^&amp;*()_++sadbjkfjdf" | tr -c 'a-zA-Z0-9_' '_')
ben@Tyr:~$ echo $var
xyzabc____________sadbjkfjdf
</pre>
Do note that this will <em>still</em> break if a "special" Bash character
(e.g., '!') appears in the string. Overall, you just need to rethink
your approach to this problem. MJD is right: it's a bad idea to use a
variable as a variable name.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>

<hr />


<!-- Thread anchor: Virtual Desktops with individual folders --><a name='virtual_desktops_with_individual_folders'></a>
<h3>Virtual Desktops with individual folders</h3>
<p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Sat, 14 Jul 2007 01:01:36 -0400</b>
</p>

<p>
On Wed, Jun 20, 2007 at 01:20:31PM +0200, Peter Holm wrote:
</p>

<pre>
&gt; I have searched the net (google, newsgroups ...) to find an answer to
&gt; this question .- but without success.
&gt; 
&gt; In KDE (for example) you can get individual desktops backgrounds for
&gt; each virtual desktop.
&gt; Well - i am used to a utility for M$-Windows called Xdesk that also
&gt; can set the the desktops to have individual icons / folders.
&gt; 
&gt; I know that in the windows world they change a regkey that tells where
&gt; the desktop belongs for each switch so such a 'true virtual desktop'
&gt; 
&gt; I have also in M$-Windows created bathc-files to use with less
&gt; intelligence window-managers, theese batch-files separately update the
&gt; regkey to get my own way to create 'true virtual desktops'
&gt; 
&gt; Is there any program that i can get to have different desktops-folders
&gt; or is there any way  to trick either  kde / gnomw / idesk to have
&gt; different desktops?
</pre>

<p>
I have very little experience with it myself, but based on what I do
know, FVWM can probably accomodate you. You would, however, need to
learn to write config files for it. I have no doubt that it has some
kind of a "DetectDesktopSwitch" function, as well as either the
capability of hiding the icons or allowing you to script such a
function.
</p>

<p>
Here's an example of a very complicated (but still readable) FVWM config
file:
</p>

<pre>
<a href="http://www.cl.cam.ac.uk/~pz215/fvwm-scripts/config">http://www.cl.cam.ac.uk/~pz215/fvwm-scripts/config</a>
</pre>
<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>

<p><b>[  <a name="mb-virtual_desktops_with_individual_folders"></a> <a href="misc/lg/virtual_desktops_with_individual_folders.html">Thread continues here (5 messages/13.80kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: Help with Bash script --><a name='help_with_bash_script'></a>
<h3>Help with Bash script</h3>
<p>
<b><p>
Chiew May Cheong [chiew_may at hotmail.com]
</p>
</b><br />
<b>Tue, 26 Jun 2007 05:19:29 +0000</b>
</p>

<p>
I have got a bash script called del_level0.sh that runs every Friday
that looks like this:
</p>

<pre>
#!/bin/bash
cd /u1/database/prod/level0
rm *
</pre>
There's a cron entry for this script that runs every Friday:
</p>

<pre>
linux:/usr/local/bin # crontab -l
 
# DO NOT EDIT THIS FILE - edit the master and reinstall.
# (/tmp/crontab.XXXXvi5yHq installed on Tue Jun 26 14:13:04 2007)
# (Cron version V5.0 -- $Id: crontab.c,v 1.12 2004/01/23 18:56:42 vixie Exp $)
0 15 * * 5 /usr/local/bin/del_level0.sh &gt; /dev/null 2&gt;&amp;1
</pre>
Can you help me so that the script runs at 3pm every 2nd and 4th Friday of the month?
</p>

<p>
Thanks.
</p>

<p>
Regards,
</p>

<p>
Chiew May
</p>

<p>

</p>

<p><b>[  <a name="mb-help_with_bash_script"></a> <a href="misc/lg/help_with_bash_script.html">Thread continues here (6 messages/5.55kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: Good vacation mailers? --><a name='good_vacation_mailers'></a>
<h3>Good vacation mailers?</h3>
<p>
<b><p>
Ren&eacute; Pfeiffer [lynx at luchs.at]
</p>
</b><br />
<b>Thu, 19 Jul 2007 00:55:01 +0200</b>
</p>

<p>
Hello, TAG!
</p>

<p>
Every once in a while I am looking for a good vacation mailer that can
read emails as good as I can while I am as far away from my mailbox as
possible. I already tried
</p>

<pre>
 - vacation from Sendmail,
 - the Sieve vacation mailer that can be enabled in Cyrus and
 - a Perl script I wrote which is buried under the rubble of the company
   I worked for many years ago.
</pre>
What are your favourite vacation mailers that cause the least trouble
with auto-generated emails? Do you have any preferences or experiences?
</p>

<p>
Best wishes,
Ren&eacute;.
</p>

<p>

</p>

<p><b>[  <a name="mb-good_vacation_mailers"></a> <a href="misc/lg/good_vacation_mailers.html">Thread continues here (8 messages/10.67kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: A Router With Just One Ethernet Port... --><a name='a_router_with_just_one_ethernet_port'></a>
<h3>A Router With Just One Ethernet Port...</h3>
<p>
<b><p>
Jim Jackson [jj at franjam.org.uk]
</p>
</b><br />
<b>Mon, 9 Jul 2007 14:30:40 +0100 (BST)</b>
</p>

<p>
I was interested to see the discussion in this article on passive ethernet
"hubs" etc. Other maybe interested in this passive 3 port ethernet hub
design...
</p>

<p>
  <a href="http://www.zen22142.zen.co.uk/Circuits/Interface/pethhub.htm">http://www.zen22142.zen.co.uk/Circuits/Interface/pethhub.htm</a>
</p>

<p>
I've not actually built it yet, but plan to do so soon.
</p>

<p>
Jim
</p>



<p>

</p>

<p><b>[  <a name="mb-a_router_with_just_one_ethernet_port"></a> <a href="misc/lg/a_router_with_just_one_ethernet_port.html">Thread continues here (2 messages/2.56kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: Shuttle-SD39P2: Should I buy one? --><a name='shuttle_sd39p2__should_i_buy_one'></a>
<h3>Shuttle-SD39P2: Should I buy one?</h3>
<p>
<b><p>
Thomas Adam [thomas.adam22 at gmail.com]
</p>
</b><br />
<b>Sun, 22 Jul 2007 01:10:28 +0100</b>
</p>

<p>
Hey all,
</p>

<p>
As I am sure many of you know, I don't get on with hardware.  <img src="../gx/smile.png" alt=":)">
Thanks to Ben's suggestion I now have a USB Sun keyboard though, and
despite people's horrific claims, <strong>I</strong> at least like it.  So thanks,
Ben.  <img src="../gx/smile.png" alt=":)">
</p>

<p>
My next question concerns a possibe replacement for my workstation.
I've had my current PC for about three years now, kindly donated by a
friend of mine.  It's a nice system, but it needs replacing.  I've had
enough of the CPU being at 70C plus, despite cooling attempts.
</p>

<p>
So I was looking at buying a Shuttle PC.  Specifically the SD39P2
which would be a bare-bones system [1].  What I'm curious to know is
whether any of you have used one, and how they stack up against a
regular PC?  My reading suggests they can act as a pretty good desktop
replacements.  Whilst the model I'm looking at only has two PCI slots,
I only really need to add a wireless PCI card and an NVidia graphics
card, so that's perfect.
</p>

<p>
Does the model I'm listing [1] suggest any problems with running Linux
on it?  I can't see how it would at a cursive glance of what's
available.  My only real reservation is what would be the driving
force of me buying this model when I could go to Dell and spend an
equivalent amount of money and get a whole lot more.  <img src="../gx/smile.png" alt=":)">
</p>

<p>
Kindly,
</p>

<p>
Thomas Adam
</p>

<p>
[1]  <a href="http://www.trustedreviews.com/pcs/review/2007/05/30/Shuttle-SD39P2-Barebone/p1">http://www.trustedreviews.com/pcs/review/2007/05/30/Shuttle-SD39P2-Barebone/p1</a>
</p>

<p>

</p>

<p><b>[  <a name="mb-shuttle_sd39p2__should_i_buy_one"></a> <a href="misc/lg/shuttle_sd39p2__should_i_buy_one.html">Thread continues here (8 messages/9.57kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: Max-spread algorithm --><a name='max_spread_algorithm'></a>
<h3>Max-spread algorithm</h3>
<p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Wed, 4 Jul 2007 22:50:50 -0400</b>
</p>

<p>
[ If you're not a programmer or a mathematician, you might want to hit
the 'delete' key right about now. Either that, or risk being bored to
tears. Remember, I warned you! <img src="../gx/smile.png" alt=":)"> ]
</p>

<p>
As I've just mentioned in my previous post, I've been fiddling with a
"max-spread" algorithm - i.e., if I have two lists, and I want the items
in the first list to be spread as widely as possible (using the items in
the second list as the "padding"), how do I interpolate them?
</p>

<p>
This can also be stated as follows: given a barbecue, a bunch of pork
cubes, and a number of cherry tomatoes, how would you arrange the
skewers in such a way that a) there's a pork chunk at the beginning and
the end of every skewer, b) each skewer is arranged in as even a manner
as possible, and c) you use up all the pork and all the tomatoes?
</p>

<p>
I got most of the way to a solution - essentially reinventing the
Bresenham line algorithm [1] (and the wheel... and fire... sheesh. I'm a
<em>very</em> poor mathematician, and a worse statistician), but got scooped by
a fellow Perl monk from the Monastery (perlmonks.org) - really nice work
on his part. I rewrote his script to actually sort arrays rather than
strings and added some guard conditions. Sample output looks like this:
</p>

<p>
<pre class="code">
ben@Tyr:~/devel/0$ ./skewer 2 2
pork1|tmt1|tmt2|pork2
 
 ---#00#---
 
ben@Tyr:~/devel/0$ ./skewer 3 3
pork1|tmt1|tmt2|pork2|tmt3|pork3
 
 ---#00#0#---
 
ben@Tyr:~/devel/0$ ./skewer 4 4
pork1|tmt1|tmt2|pork2|tmt3|pork3|tmt4|pork4
 
 ---#00#0#0#---
 
ben@Tyr:~/devel/0$ ./skewer 5 4
pork1|tmt1|pork2|tmt2|pork3|tmt3|pork4|tmt4|pork5
 
 ---#0#0#0#0#---
 
ben@Tyr:~/devel/0$ ./skewer 7 4
pork1|tmt1|pork2|tmt2|pork3|tmt3|pork4|tmt4|pork5|pork6|pork7
 
 ---#0#0#0#0###---
</pre>
Can you reproduce this algorithm? <img src="../gx/smile.png" alt=":)"> I found it a very interesting
exercise, myself.
</p>



<p>
[1] <a href="http://en.wikipedia.org/wiki/Special:Search?search=Bresenham%20line%20algorithm">http://en.wikipedia.org/wiki/Special:Search?search=Bresenham%20line%20algorithm</a>
</p>

<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>

<p><b>[  <a name="mb-max_spread_algorithm"></a> <a href="misc/lg/max_spread_algorithm.html">Thread continues here (11 messages/20.47kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: Backup software/strategies --><a name='backup_software_strategies'></a>
<h3>Backup software/strategies</h3>
<p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Wed, 11 Jul 2007 09:24:40 -0400</b>
</p>

<p>
All of us know - at least I hope we do - that we should all be doing
regular backups; that's a given. Chances are, though, that we've all
skipped one or two (or more) on the schedule ("heck, nothin's happened
so far; it shouldn't make too much of a difference...") - that is, if
you even have a schedule, rather than relying on some vague sense of
"it's probably about time..." I have to admit that, as much as I advise
my customers to have a solid backup plan, I'm less than stellar about
having a polished, perfect plan myself.
</p>

<p>
In part, this is because backups are much easier with a desktop than a
laptop - or even with a desktop to which you synch the laptop once in a
while. Operating purely from a laptop, as I do, means that I don't have
an always-connected tape drive (or whatever) - and doing a backup is
always a hassle, involving digging out the external HD, hooking it up,
and synchronizing. Moreover, since I do a lot of travelling, setting an
alarm doesn't seem to be very useful; it usually goes off while I'm on
the road, at which point I can only glare at it in frustration.
</p>

<p>
As with so many things, what I <strong>really</strong> need is a copy of "at" installed
in my brain... but lacking that, well, I decided to dump the problem on
you folks. <img src="../gx/smile.png" alt=":)">
</p>

<p>
Can anyone here think of a sensible backup plan for the situation that
I've described - laptop, external backup, arbitrary schedule - and some
way to set up a schedule that work with that? Also, does anyone have a
favorite WRT backup software? I really miss the ability to do incremental
backups; that would be awf'lly nice (I don't mind carrying a few DVDs
with me, and using the external HD for a monthly full backup.)
</p>

<p>
Good ideas in this regard - whether they competely answer the question
or not - are highly welcome.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>

<p><b>[  <a name="mb-backup_software_strategies"></a> <a href="misc/lg/backup_software_strategies.html">Thread continues here (30 messages/58.31kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: Re-Update Your Online Banking ! --><a name='re_update_your_online_banking'></a>
<h3>Re-Update Your Online Banking !</h3>
<p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Fri, 6 Jul 2007 14:13:35 -0400</b>
</p>

<p>
<p class="editorial">
[[[  I viciously snipped out the entirety of the original message which
headed this thread, as it was horrendously replete with html garbage. -
Kat  ]]]
</p>

</p>

<p>
On Fri, Jul 06, 2007 at 11:33:15AM -0400, Bank Of America wrote:
</p>

<pre>
&gt;    Bank of America Higher Standards
&gt;                             Online Banking Alert
&gt;     Need additional up to    Re-Update Your Online Banking                   
&gt;     the minute account                                                       
&gt;     information? Sign  in    Because of unusual number of invalid login      
&gt;                              attempts on you account, we had to believe      
&gt;                              that, their might be some security problem on   
&gt;                              you account. So we have decided to put an extra 
&gt;                              verification process to ensure your identity    
&gt;                              and your account security. Please click on sign 
&gt;                              in to Online Banking to continue to the         
&gt;                              verification process and ensure your account    
&gt;                              security. It is all about your security. Thank  
&gt;                              you. and visit the customer service section.    
</pre>

<p>
[snip] Nice, even though completely ungrammatical. The 'sign in to
Online Banking' link points to
</p>

<pre>
<a href="http://www.tsv-betzigau.de/contenido/includes/hypper/repute/bankofamerica/online_bofa_banking/e-online-banking/index.htm">http://www.tsv-betzigau.de/contenido/includes/hypper/repute/bankofamerica/online_bofa_banking/e-online-banking/index.htm</a>
</pre>
Their images are coming from yet another domain:
</p>

<pre>
<a href="http://release35.par3.com/images/client/bankofamerica/em_logo.gif">http://release35.par3.com/images/client/bankofamerica/em_logo.gif</a>
</pre>
Yet another example of PHP fuckmuppetry. And this isn't going to stop,
since the creators of PHP <strong>refuse</strong> to fix the well-known vulnerabilities
in the language. [sigh]
</p>

<p>
If someone who speaks Deutsch wants to contact the owners of the first
domain and let them know, that would be a nice thing to do. I've just
contacted the webmaster of the second domain; hopefully, they'll take
that garbage offline and fix their leaks.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>

<p><b>[  <a name="mb-re_update_your_online_banking"></a> <a href="misc/lg/re_update_your_online_banking.html">Thread continues here (4 messages/6.33kB)</a>  ]</b></p>
<hr />


</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:141/lg_mail.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Published in Issue 141 of Linux Gazette, August 2007
</p>

</div>
</div>


<div class="content lgcontent">

<a name="lg_mail2"></a>
<h1>An Ongoing Discussion of Open Source Licensing Issues</h1>

</b>
</p>

<p>
<h2>Editor's Note</h2>Rick Moen has been actively keeping TAG updated in an ongoing discussion on
open source licensing issues. Starting this month, these messages will be
presented on their own page. 
<hr />

<!-- Thread anchor: Microsoft GPLv3 "statement" --><a name='microsoft_gplv3_statement'></a>
<h3>Microsoft GPLv3 "statement"</h3>
<p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Sat, 7 Jul 2007 17:55:35 -0700</b>
</p>

<p>
This is a small excerpt from the discussion thread at Linux Weekly New,
in response to LWN's news story about a "statement" posted at Microsoft
Corporation's Web site, claiming they are not, and never will be,
subject to the provisions of the GNU General Public License v. 3.
</p>

<p>
LWN is subscriber-supported, and well worth the minor expense.
</p>


<p>
<a href="http://lwn.net/Articles/240822/">http://lwn.net/Articles/240822/</a>
</p>



<p>
*They're involved in the distribution*
</p>

<p>
Posted Jul 6, 2007 14:31 UTC (Fri) by guest coriordan
</p>

<p>
Microsoft arranged for Novell to give GNU/Linux to anyone with an MS
voucher, and then proceeded to distribute those vouchers. Sounds like
distribution to me (with a middle man which isn't legally relevant).
</p>

<p>
According to the GPLv3 lawyers, they're "procuring the distribution of"
GPL'd software, and that requires permission from the copyright holder.
So Microsoft are either distributing under the permissions which the GPL
grants them, or they are violating copyright.
</p>

<p>
And, as I understand it, there's no time limit on those vouchers. Novell
might have to declare the deal non-applicable (and thus the "protection"
too) when they distribute GPLv3 software, or maybe Microsoft will have
to make that declaration.
</p>



<p>
*They're involved in the distribution*
</p>

<p>
Posted Jul 6, 2007 15:09 UTC (Fri) by guest moltonel 
</p>

<p>
In an eWeek article, they have a quote from "Bruce Lowry, a Novell
spokesperson" saying "Customers who have already received SUSE Linux
Enterprise certificates from Microsoft are not affected in any way by
this, since their certificates were fully delivered and redeemed prior
to the publication of the GPLv3".
</p>

<p>
So it sounds like Microsoft does not plan to be "distributing" GPL code
in this manner anymore, and that what has already been distributed is
protected the GPL's grandfather clause.
</p>

<p>
Well, that may not be a huge victory (Did anybody expect Microsoft to
suddenly give up on its patents or start GPL'ing its code because of
GPLv3 and the Novell deal ?), but it's something. It'll be interesting
to watch the GPLv3 / Novell deal interpretation match in the next few
weeks.
</p>



<p>
*They're involved in the distribution*
</p>

<p>
Posted Jul 8, 2007 0:47 UTC (Sun) by subscriber rickmoen 
</p>

<p>
Ciaran O'Riordan wrote:
</p>


<pre>
&gt; According to the GPLv3 lawyers, they're "procuring the distribution of"
&gt; GPL'd software, and that requires permission from the copyright holder.
&gt; So Microsoft are either distributing under the permissions which the GPL
&gt; grants them, or they are violating copyright.
</pre>

<p>
Quite. Moreover, this <em>does</em> affect preexisting software covered by the
Novell-Microsoft patent-shakedown agreement, too (not just future
releases under GPLv3), because a great deal of existing software in both
Novell SLES10/SLED10, per upstream licensors' terms, can be received by
users under GPLv2 or, at their option, _any later version_.
</p>

<p>
For that matter, Microsoft Services for Unix (nee Interix) is affected
in exactly the same fashion, because it, too, includes a great deal of
upstream, third-party code that users may accept under GPLv2 or any
later version.
<p>
[ ... ]
</p><p><b>[  <a name="mb-microsoft_gplv3_statement"></a> <a href="misc/lg/microsoft_gplv3_statement.html">Thread continues here (1 message/4.27kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: SugarCRM goes to GPLv3 --><a name='sugarcrm_goes_to_gplv3'></a>
<h3>SugarCRM goes to GPLv3</h3>
<p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Wed, 25 Jul 2007 21:22:04 -0700</b>
</p>

<p>
LWN has a new story (currently subscriber-only) about SugarCRM 
announcing that its upcoming 5.0 release of SugarCRM Community Edition
will be under GPLv3 (as opposed to the company's current badgeware
licence, this having been the firm that invented the concept).
</p>

<p>
Press release:  
<a href="http://www.prnewswire.com/cgi-bin/stories.pl?ACCT=104&amp;STORY=/www/story/07-25-2007/0004632607&amp;EDATE=LWN">http://www.prnewswire.com/cgi-bin/stories.pl?ACCT=104&amp;STORY=/www/story/07-25-2007/0004632607&amp;EDATE=LWN</a> item (brief mention only; no analysis):
<a href="http://lwn.net/Articles/242968/">http://lwn.net/Articles/242968/</a>
</p>

<p>
I've just posted this comment to LWN:
</p>



<p>
*Is this a blunder, or just too subtle for me?*
</p>

<p>
Posted Jul 26, 2007 4:18 UTC (Thu) by subscriber rickmoen
</p>

<p>
I may be missing something, here, so I'm phrasing this in the form of a
question or two, and it's not rhetorical: Didn't FSF bow to pressure
from sundry interest groups and <em>remove</em> the "ASP loophole" language[1]
that had been present in some GPLv3 drafts? Therefore, what in Sam Hill
is a Software as a Service (Saas) / ASP / Web 2.0 firm doing adopting a
copyleft language whose copyleft language gets finessed by hosted
deployment?
</p>

<p>
FYI, there are a number of genuinely open source licences, a couple of
them OSI certified, that do apply copyleft obligations to the ASP
industry. One of the best is Larry Rosen's OSL, and there is also
Apple's ASPL, both of those being OSI-certified. Non-certified options
include Affero GPL (newly reissued as a patch to GPLv3, by the way) and
Honest Public License.
</p>

<p>
On the basis of recent history, it's possible that SugarCRM not only
lacks any clever, non-obvious reason why it picked a non-ASP copyleft
licence for ASP code, but also doesn't really have any idea what it's
doing in this area, and picked GPLv3 just because it has had good press
(good press that it generally deserved, IMVAO). Remember, this is the
firm that created the first MPL-based ASP licence, and then acted
shocked and indignant when it belatedly discovered that its licence
permitted forking (when TigerCRM of Chennai forked the codebase), and
overreacted by writing what became the prototype MPL + Exhibit B
"badgeware" licence that impairs third-party usage through mandated logo
advertising without a trademark licence.
</p>

<p>
It'd be more reassuring if I thought this firm had a master plan, but I
now rather strongly suspect it's just a bunch of sales people in an
office in Cupertino, staggering from one inadvertant move to the next.
</p>

<p>
Rick Moen
rick@linuxmafia.com
</p>


<p>
[1] <a href="http://weblog.infoworld.com/openresource/archives/2007/03/gplv3_goes_weak.html">http://weblog.infoworld.com/openresource/archives/2007/03/gplv3_goes_weak.html</a>
</p>


<p>

</p>

<p><b>[  <a name="mb-sugarcrm_goes_to_gplv3"></a> <a href="misc/lg/sugarcrm_goes_to_gplv3.html">Thread continues here (2 messages/3.61kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: [lg-announce] Linux Gazette #140 is out! --><a name='lg_announce_linux_gazette_140_is_out'></a>
<h3>[lg-announce] Linux Gazette #140 is out!</h3>
<p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Mon, 2 Jul 2007 09:32:25 -0700</b>
</p>

<p>
<p class="editorial">
[[[  I have included a portion of this thread for general interest, but
the rest of the housekeeping has been elided. -- Kat  ]]]
</p>

</p>

<p>
Quoting Ben Okopnik (ben@linuxgazette.net):
</p>


<pre>
&gt; July 2007 (#140):
&gt; 
&gt;   * Mailbag
&gt;   * Talkback
&gt;   * NewsBytes, by Howard Dyckoff
</pre>

<p>
There was something I annotated at the time of my svn checkin of
lg_bytes -- but just realised I should have ALSO put into the STATUS
notes.  (I'll bet in retrospect that nobody pays attention to svn
checkin comments.)
</p>

<p>
Howard had:
<pre>
  Red Hat Adds Business Solutions to Open Source RHX
 
  RHX launch partners include Alfresco, CentricCRM, Compiere,
  EnterpriseDB, Groundwork, Jaspersoft, Jive, MySQL, Pentaho, Scalix,
  SugarCRM, Zenoss, Zimbra, and Zmanda.
</pre>
Problem:  A bunch of those are JUST NOT OPEN SOURCE.  Zimbra, SugarCRM,
Compiere, Groundwork, and Scalix are classic "badgeware", which is under
MPL-variant software with some restrictions -- while with CentricCRM,
there's not even any room for controversy, since <em>their</em> licence doesn't
even permit code redistribution.  Jive Software (which I'd not heard of,
before) turns out to be equally bad.
</p>

<p>
I have brought this matter, several times, to Red Hat's attention, and
the presence of actively misleading wording on the Red Hat Exchange
site, such as this at the top of
<a href="http://rhx.redhat.com/rhx/support/article/DOC-1285:">http://rhx.redhat.com/rhx/support/article/DOC-1285:</a>  
<pre>
  Red Hat Exchange helps you compare, buy, and manage open source
  business applications. All in one place and backed by the open source
  leader. We've collaborated with our open source software partners to
  validate that RHX applications run on Red Hat Enterprise Linux and are
  delivered through the Red Hat Network. At RHX, Red Hat provides
  customers with a single point of contact for support.
</pre>
There has been no response and no correction of this error, but the very
bottom of that page now has this "FAQ" item:
<pre>
  Are you only accepting open source ISVs into RHX?
 
  The initial set of participating ISVs all have an open source focus. We
  realize that there is debate about which companies are truly open
  source. To make it transparent to users, RHX includes information about
  each ISV's license approach. Longer term, we may introduce proprietary
  applications that are friendly with open source applications.
</pre>
That is, of course, anything but a straight answer.  First, it's
nonsense to speak of <em>companies</em> being open source or not -- and the
above paragraph in general ducks the question.  The issue is whether
software is.  Second, even if there were debate about the badgeware
offerings allegedly being open source, there could be absolutely none
about Jive Software's Clearspace or CentricCRM 4.1, which are
unambiguously proprietary.
</p>

<p>
I'm surprised that this Red Hat's deceptive characterisation got past
Howard without comment, given that the matter's been extensively covered
in recent _Linux Gazette_ issues.
<p>
[ ... ]
</p><p><b>[  <a name="mb-lg_announce_linux_gazette_140_is_out"></a> <a href="misc/lg/lg_announce_linux_gazette_140_is_out.html">Thread continues here (2 messages/5.40kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: when is an open source license open source? --><a name='when_is_an_open_source_license_open_source'></a>
<h3>when is an open source license open source?</h3>
<p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Wed, 27 Jun 2007 14:51:36 -0700</b>
</p>

<p>
----- Forwarded message from Rick Moen &lt;rick@linuxmafia.com&gt; -----
</p>

<pre>
Date: Wed, 27 Jun 2007 14:45:53 -0700
From: Rick Moen &lt;rick@linuxmafia.com&gt;
To: Ashlee Vance &lt;ashlee.vance@theregister.co.uk&gt;
Cc: Karsten Self &lt;karsten@linuxmafia.com&gt;
Subject: Re: (forw) Re: when is an open source license open source?
</pre>
Quoting Ashlee Vance (ashlee.vance@theregister.co.uk):
</p>


<pre>
&gt; Rick, your explanation helped a great. Who are the main culprits of 
&gt; consequence besides Sugar?
</pre>

<p>
SugarCRM started the trend, and the other dozen-odd firms (Socialtext,
Alfresco, Zimbra, Qlusters, Jitterbit, Scalix, MuleSource,
Dimdim, Agnitas AG, Openbravo, Emu Software, Terracotta, Cognizo
Technologies, ValueCard, KnowledgeTree, OpenCountry, 1BizCom, MedSphere,
vTiger) literally copied their so-called "MPL-style" licence, with minor
variations.  MuleSource was for a long time a vocal backer of SugarCRM's
position (but see below).
</p>

<p>
Alfresco <em>used</em> to be a major backer of that position, but then suddenly
decided to shift to GPLv2, which is what they use now.  (They are no
longer a badgeware firm.)   Company spokesman (and OSI Board member, and
attorney) Matt Asay says he tried all along to convince them to do that,
and I do believe him.
</p>

<p>
SocialText (and especially CEO Ross Mayfield) has taken a lead online
role in trying to resolve the impasse -- though I personally find most
of what he says to be almost purely rationalising, and for him to be
mostly unresponsive to (or evasive of) substantive criticism.
</p>

<p>
MuleSource and Medsphere have, within the last few months, improved their 
respective MPL + Exhibit B licences (<a href="http://www.mulesource.com/MSPL/">http://www.mulesource.com/MSPL/</a> 
<a href="http://medsphere.org/license/MSPL.html">http://medsphere.org/license/MSPL.html</a>) <em>very</em> dramatically, in direct
response to criticism on OSI's license-discuss mailing list.  This work
seems to be that of attorney Mitch Radcliffe, and encouraged by
MuleSource and Medsphere Board member Larry Augustin (formerly of VA
Linux Systems).  I have great respect for this work, though I am still
trying to properly assess and analyse it.
</p>

<p>
(Disclaimer:  Larry is a friend of mine, though I see him only rarely,
and I once was employed at one of his firms.)
</p>


<p>
You should be made aware of the role of vTiger (of Chennai) in all of
this:  In August 2004, it forked and since then has offered
independently, under its own name and brand, an early version of the
SugarCRM codebase, plus various changes of their own devising.  It was
in response to that forking event, to which SugarCRM CEO John Roberts
responded very angrily at the time, that SugarCRM adopted the "Exhibit
B" restrictive licensing addendum that then became the hallmark of
badgeware licences generally.  See second post on
<a href="http://www.vtiger.com/forums/viewtopic.php?p=22">http://www.vtiger.com/forums/viewtopic.php?p=22</a> , and the matching
apologia at <a href="http://www.vtiger.com/forums/viewtopic.php?p=22">http://www.vtiger.com/forums/viewtopic.php?p=22</a> by
Christiaan Erasmus of badgeware firm ValueCard (South Africa).
</p>


<p>
----- End forwarded message -----
</p>

<p>

</p>

<p><b>[  <a name="mb-when_is_an_open_source_license_open_source"></a> <a href="misc/lg/when_is_an_open_source_license_open_source.html">Thread continues here (5 messages/13.37kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: Red Hat flags OSI offenders on partner site - Register Article --><a name='red_hat_flags_osi_offenders_on_partner_site_register_article'></a>
<h3>Red Hat flags OSI offenders on partner site - Register Article</h3>
<p>
<b><p>
Martin J Hooper [martinjh at blueyonder.co.uk]
</p>
</b><br />
<b>Wed, 25 Jul 2007 07:40:32 +0100</b>
</p>

<p>
<a href="http://www.theregister.co.uk/2007/07/25/rhx_change_redhat/">http://www.theregister.co.uk/2007/07/25/rhx_change_redhat/</a>
</p>

<p>
Rick you might be interested in this article as you had been
commenting on the topic recently...
</p>


<p>

</p>

<p><b>[  <a name="mb-red_hat_flags_osi_offenders_on_partner_site_register_article"></a> <a href="misc/lg/red_hat_flags_osi_offenders_on_partner_site_register_article.html">Thread continues here (2 messages/1.60kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: [Tech briefing invite: 'What can be called open source?'] --><a name='tech_briefing_invite__what_can_be_called_open_source'></a>
<h3>[Tech briefing invite: 'What can be called open source?']</h3>
<p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Thu, 28 Jun 2007 08:59:08 -0700</b>
</p>

<p>
[Forwarding Ben's private mail, with commentary, at his invitation.]
</p>

<p>
As a reminder, Centric CRM, Inc. has recently been one of the most
problematic of the ASP/Web firms abusing the term "open source" for
their products, in part because their flagship product (Centric CRM)
has been notorious during most of this past year as the <em>most</em> clearly
and unambiguously proprietary software to be offered with the ongoing
public claim of being "open source".
</p>

<p>
I'd call this (below-cited) PR campaign blitz -- apparently, they're
intensively hitting reporters known to be following this matter --
really good news, though it has to be read attentively:
<pre class="code">
o  Former OSI General Counsel Larry Rosen's "OSL 3.0" licence is a
   really good, excellently designed, genuine copyleft licence that is
   especially well suited for ASP use, because it's one of the very 
   few that have a clause enforcing copyleft concepts within the 
   otherwise problematic ASP market.  (In ASP deployments, there is
   ordinarily no <em>distribution</em> of the code, so the copyleft provisions
   of most copyleft licences such as GPLv2 have no traction, and are
   toothless.)  Also, as Centric CRM, Inc. is keen to point out, OSL
   3.0 is an OSI-certified open source licence.
  
o  At the same time, the careful observer will note that this 
   announcement concerns the product "Centric Team Elements v. 0.9",
   which is <em>not</em> (yet?) the firm's flagship product.  That flagship
   product remains the entirely separate -- and very, very clearly
   proprietary, product "Centric CRM v. 4.1", which one wryly notices
   has been carefully omitted completely from this communique.
 
   Just in case there is any doubt about Centric CRM 4.1's proprietary
   status, here's one key quotation from the product brochure, about
   the applicable licence, "Centric Public Licence (CRM)":  "The major 
   restriction is that users may not redistribute the the Centric CRM 
   source code."
</pre>
Now, it may be that the Centric CRM product is on the way out, and that
Centric Team Elements (with genuine open source licence) will be taking
its place.  Or maybe not.  Either way:  
</p>

<p>
The bad news, but perhaps not too bad, is that Centric CRM, Inc. has
spent this past year to date falsely and misleadingly claiming that its
product line is open source -- and deflecting critics by claiming that
the term "open source" is (paraphrasing) subject to redefinition and
needn't be limited to what OSI (inventer of that term in the software
context, and standard body) defines it to be.  That misleading and
deceptive language is still very much a prominent part of the company's
pronouncements to this day, remains on the Web site, and doesn't seem to
be disappearing.
</p>

<p>
The good news is that the firm appears to be sensitive to the public
relations problem it created for itself, and <em>may</em> be taking steps to
fix it.
</p>

<p>
----- Forwarded message from Ben Okopnik &lt;ben@linuxgazette.net&gt; -----
<p>
[ ... ]
</p><p><b>[  <a name="mb-tech_briefing_invite__what_can_be_called_open_source"></a> <a href="misc/lg/tech_briefing_invite__what_can_be_called_open_source.html">Thread continues here (1 message/16.71kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: I kill me. I just do. --><a name='i_kill_me_i_just_do'></a>
<h3>I kill me. I just do.</h3>
<p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Thu, 26 Jul 2007 14:38:28 -0700</b>
</p>

<p>
My reply, just posted, is reproduced below the forwarded posting from "khim".
</p>

<p>
----- Forwarded message from LWN notifications &lt;lwn@lwn.net&gt; -----
</p>

<pre>
Date: 26 Jul 2007 16:01:24 -0000
To: rick@linuxmafia.com
From: LWN notifications &lt;lwn@lwn.net&gt;
Subject: LWN Comment response notification
</pre>


<p>
The following comment (<a href="http://lwn.net/Articles/243195/">http://lwn.net/Articles/243195/</a>) has
been posted in response to <a href="http://lwn.net/Articles/243075/">http://lwn.net/Articles/243075/</a>.
</p>

<p>
As you requested, the text of the response is being sent to you.
</p>

<pre>
*Is this a blunder, or just too subtle for me?*
 
[Announcements] Posted Jul 26, 2007 16:00 UTC (Thu) by khim
 
FYI, there are a number of genuinely open source licences, a couple of
them OSI certified, that do apply copyleft obligations to the ASP
industry.
 
Yup. And they are mostly unsuccessful ones. It's quite hard to
distinguish two cases:
<pre>
1) where your package is used for SaaS (like Google)
2) where your package is used for some private endeavour (like LWN)
</pre>
Licenses like AGPL/APSL punish equally - that's why I'll probably never
use AGPL/APSL-licensed software. And if I'll be forced to use such
software I'll do everything possible to not ever fix or change it. Even
badgeware is better from practical viewpoint. If you'll think about it
it's only logical.
 
Yes, usurpation of the code by SaaS vendors is a problem but AGPL is
worse medicine then disease itself...
 
 
</pre>
To stop receiving these notifications, please go to
<a href="http://lwn.net/MyAccount/rickmoen/">http://lwn.net/MyAccount/rickmoen/</a>.  Thank you for supporting LWN.net.
</p>

<p>
----- End forwarded message -----
</p>



<p>
*Is this a blunder, or just too subtle for me?*
</p>

<p>
Posted Jul 26, 2007 21:38 UTC (Thu) by subscriber rickmoen 
</p>

<p>
"khim" wrote:
</p>


<pre>
&gt; Yup. And they are mostly unsuccessful ones.
</pre>

<p>
Your "unsuccessful". My "underappreciated so far".
</p>


<pre>
&gt; It's quite hard to distinguish two cases:
</pre>

<p>
And, in my personal view, pointless. (My opinion, yours for a small fee
and agreement to post my logo on your forehead. And by the way, I also
deny the premise that LWN is a "private endeavour" in any sense
meaningful to this context. Of course, Jon and co. happen to use their
own code, IIRC.)
</p>

<p>
Rick Moen
rick@linuxmafia.com
</p>

<p>

</p>

<hr />


<!-- Thread anchor: For Approval: Open Source Hardware License --><a name='for_approval__open_source_hardware_license'></a>
<h3>For Approval: Open Source Hardware License</h3>
<p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Fri, 6 Jul 2007 12:16:43 -0700</b>
</p>

<p>
Ever have one of those days, where you fear that your sarcasm might
have become so caustic that it might need hazardous-substance labels?
</p>

<p>
----- Forwarded message from Simon Phipps &lt;Simon.Phipps@Sun.COM&gt; -----
</p>

<pre>
Date: Fri, 06 Jul 2007 18:21:01 +0100
From: Simon Phipps &lt;Simon.Phipps@Sun.COM&gt;
To: Rick Moen &lt;rick@linuxmafia.com&gt;
Cc: license-discuss@opensource.org
</pre>X-Mailer: Apple Mail (2.752.3)
<pre>
Subject: Re: For Approval: Open Source Hardware License
</pre>

<p>
On Jul 6, 2007, at 02:12, Rick Moen wrote:
</p>


<pre>
&gt;Quoting Jamey Hicks (jamey.hicks@nokia.com):
&gt;
&gt;&gt;There are no OSI-approved licenses for open source hardware, so I am
&gt;&gt;proposing this license.
&gt;
&gt;My understanding is that OSI's licence approval process
&gt;(<a href="http://www.opensource.org/docs/certification_mark.html">http://www.opensource.org/docs/certification_mark.html</a>) is specifically
&gt;for <em>software</em> licences.  That scope limitation came up previously when
&gt;people proposed documentation licences for certification; I suspect the
&gt;same logic applies here.
</pre>

<p>
I'm afraid the distinction between "software" and "hardware" is  
getting harder and harder to make. The Verilog that's used to make  
the UltraSPARC T1 is definitely software, and the GPL (or any other  
Free software license approved for open source community use by OSI)  
seems 100% applicable to me.
</p>

<p>
If we allow special "hardware" licenses because the copyrighted work  
is used for that purpose, we are on a slippery slope towards many  
other specialist (an in my view redundant) sub-categories.
</p>

<p>
S.
</p>



<p>
----- End forwarded message -----
</p>

<p>
----- Forwarded message from Rick Moen &lt;rick@linuxmafia.com&gt; -----
</p>

<pre>
Date: Fri, 6 Jul 2007 11:43:36 -0700
From: Rick Moen &lt;rick@linuxmafia.com&gt;
To: license-discuss@opensource.org
Subject: Re: For Approval: Open Source Hardware License
</pre>
Quoting Simon Phipps (Simon.Phipps@Sun.COM):
</p>


<pre>
&gt; I'm afraid the distinction between "software" and "hardware" is  
&gt; getting harder and harder to make.
</pre>

<p>
You know, there are circumstances in which I'd raise that point, too.  
However, I'd feel a bit silly raising it in circumstances where
something is described very unambiguously aas a licence specifically for
hardware _and used_ (or at least planned to be used) only for that
purpose, to a groups that certifies licences specifically for software.
</p>

<p>
(I do doubt that OSI would refuse to certify a licence actually <em>used</em>
for software, on no better grounds than it having the word "hardware" in
it.) 
</p>

<p>
Nonetheless, your ability to discern shades of grey is admirable. ;-&gt;
</p>

<pre>-- 
Cheers,                English is essentially a text parser's way of getting 
Rick Moen              faster processors built.
rick@linuxmafia.com    -- John M. Ford, <a href="http://ccil.org/~cowan/essential.html">http://ccil.org/~cowan/essential.html</a>
</pre>

<hr />


<!-- Thread anchor: RHX and License Clarity --><a name='rhx_and_license_clarity'></a>
<h3>RHX and License Clarity</h3>
<p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Thu, 5 Jul 2007 11:03:57 -0700</b>
</p>

<p>
Trying to figure out how little they can do?
</p>

<p>
----- Forwarded message from Matt Mattox &lt;mmattox@redhat.com&gt; -----
</p>

<pre>
Date: Thu, 05 Jul 2007 13:20:56 -0400
From: Matt Mattox &lt;mmattox@redhat.com&gt;
To: rick@linuxmafia.com
Subject: RHX and License Clarity
</pre>
Hi Rick,
</p>

<p>
Just a quick note responding to your comment in the "More About RHX" 
section of RHX. We're working on a solution that will make the license 
approach used by each RHX software vendor very clear to users, including 
whether or not they are OSI-approved. I'd love to get your feedback on 
our approach if you have the time and interest. Let me know....
</p>

<p>
Thanks,
Matt
Product Manager, RHX
</p>

<p>
----- End forwarded message -----
</p>

<p>
----- Forwarded message from Rick Moen &lt;rick@linuxmafia.com&gt; -----
</p>

<pre>
Date: Thu, 5 Jul 2007 11:02:52 -0700
From: Rick Moen &lt;rick@linuxmafia.com&gt;
To: Matt Mattox &lt;mmattox@redhat.com&gt;
Subject: Re: RHX and License Clarity
</pre>
Quoting Matt Mattox (mmattox@redhat.com):
</p>


<pre>
&gt; Just a quick note responding to your comment in the "More About RHX" 
&gt; section of RHX. We're working on a solution that will make the license 
&gt; approach used by each RHX software vendor very clear to users, including 
&gt; whether or not they are OSI-approved. I'd love to get your feedback on 
&gt; our approach if you have the time and interest. Let me know....
</pre>

<p>
Hi, Matt.  Thank you for your note.  
</p>

<p>
The main problem is actually the statements on the RHX main Web pages
that serve as entry points to RHX (and, in the recent past, by all RHX
press releases).  For example:
</p>

<p>
<a href="http://www.redhat.com/rhx/">http://www.redhat.com/rhx/</a>
<pre>
  Starts out with "Trusted open source software" -- without saying that 
  some offerings are open source and some proprietary -- and goes on for
  the entire page talking how RHX has helped you select open source 
  applications, etc.
</pre>
<a href="http://rhx.redhat.com/rhx/support/article/DOC-1285">http://rhx.redhat.com/rhx/support/article/DOC-1285</a> ("More about RHX" page)
<pre>
  Starts out with "Red Hat Exchange helps you compare, buy, and manage
  open source business applications. All in one place and backed by the
  open source leader. We've collaborated with our open source software
  partners to validate that RHX applications run on Red Hat Enterprise
  Linux and are delivered through the Red Hat Network."
</pre>
Letting people _dig down_ to licensing specifics would be nice but
wouldn't fix the problem of false and misleading general statements 
everyone encounters on the way in.  The latter should be replaced
without delay.
</p>

<p>
And future RHX press releases should mention that it includes both
proprietary and open source applications.
</p>

<p>
The longer the delay in fixing this problem, the more Red Hat's
reputation for integrity is suffering.  Please make no mistake:  Your
firm is being damaged by this.
</p>


<p>
----- End forwarded message -----
</p>

<p>

</p>

<p><b>[  <a name="mb-rhx_and_license_clarity"></a> <a href="misc/lg/rhx_and_license_clarity.html">Thread continues here (3 messages/5.47kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: Red Hat Exchange has a serious flaw --><a name='red_hat_exchange_has_a_serious_flaw'></a>
<h3>Red Hat Exchange has a serious flaw</h3>
<p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Wed, 27 Jun 2007 11:56:06 -0700</b>
</p>

<p>
----- Forwarded message from rick -----
</p>

<pre>
Date: Tue, 26 Jun 2007 21:21:42 -0700
To: ashlee.vance@theregister.co.uk
Cc: Karsten Self &lt;karsten&gt;
Subject: Red Hat Exchange has a serious flaw
</pre>
Dear Ashlee:
</p>

<p>
I note with interest your recent articles in ElReg: "Red Hat's Exchange
roars like a muted lamb" and "Red Hat RHXes out to open source
partners".  However, I'd like to point out one problem you haven't yet
covered: 
</p>

<p>
Many of Red Hat Exchange's offerings, although all are implied to be open
source, are in fact nothing of the kind.  For example, the offered
products from Zimbra, SugarCRM, Compiere, CentricCRM, and GroundWork are
very clearly under proprietary licences of various descriptions.
</p>

<p>
When I attended Red Hat's RHEL5 product launch in San Francisco on March
14, I heard RHX described for the first time, immediately noticed the
problem, and quietly called it to the attention of Red Hat CTO Brian
Stevens.  Stevens acknowledged the point, and said (loosely paraphrased)
that their Web pages should to be adjusted to make clear that not all
RTX offerings are open source -- which is indeed a sensible remedy, but
it hasn't yet happened.
</p>

<p>
I also attempted to call Red Hat's attention to the problem via the
designated RHX feedback forum, at
<a href="http://rhx.redhat.com/rhx/feedback/feedback.jspa">http://rhx.redhat.com/rhx/feedback/feedback.jspa</a> .  (You'll note my
comment near the bottom.)
</p>

<p>
I'm sure it's an honest slip-up, but, accidentally or not, Red Hat has
mislead its customers for the past several months on this matter, and is
continuing to do so.
</p>

<p>
Best Regards,
Rick Moen
rick@linuxmafia.com
</p>


<p>
----- End forwarded message -----
</p>

<p>
----- Forwarded message from Rick Moen &lt;rick@linuxmafia.com&gt; -----
</p>

<pre>
Date: Wed, 27 Jun 2007 11:54:04 -0700
From: Rick Moen &lt;rick@linuxmafia.com&gt;
To: Ashlee Vance &lt;ashlee.vance@theregister.co.uk&gt;
Cc: Karsten Self &lt;karsten&gt;
Subject: Re: Red Hat Exchange has a serious flaw
</pre>
Quoting Ashlee Vance (ashlee.vance@theregister.co.uk):
</p>


<pre>
&gt; Thanks so much for this, mate. Will investigate.
</pre>

<p>
Sure.  In case it will help:  
</p>

<p>
Most if not all of those codebases are ASP (Web app) code, which poses a
thorny problem:  Suppose you are, say, Google, and wish to behave
benignly towards open source with your Web apps.  You deploy a Web 2.0 
hosted application, and release its source code to the community under a
proper, forkable licence such as BSD / MIT X11 (simple permissive type)
or GPLv2 (copyleft), that fully satisfies the Open Source Definition.
For the sake of illustration, let's assume GPLv2.
</p>

<p>
Google's competitor EvilCo swoops by, grabs the source tarball, modifies
it extensively behind closed doors, and deploys it under a completely
different name as a hosted Web app (product/service) of its own, bearing
very little resemblance to Google's original application.  Let's say
that EvilCo nowhere mentions its borrowing from Google, and that EvilCo
doesn't provide anyone outside its employees access to the modified
source code.
<p>
[ ... ]
</p><p><b>[  <a name="mb-red_hat_exchange_has_a_serious_flaw"></a> <a href="misc/lg/red_hat_exchange_has_a_serious_flaw.html">Thread continues here (1 message/6.71kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: RHX: Red Hat, Inc. does the right thing --><a name='red_hat_inc_does_the_right_thing'></a>
<h3>RHX: Red Hat, Inc. does the right thing</h3>
<p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Fri, 13 Jul 2007 11:48:26 -0700</b>
</p>

<p>
Red Hat, Inc. seems to have taken quick and effective action to correct
some misleading statements about licensing that had previously been on 
the firm's Web pages for the "Red Hat Exchange" (RHX) partner-software
sales program:
</p>

<p>
<a href="http://www.redhat.com/rhx/">http://www.redhat.com/rhx/</a>
https://rhx.redhat.com (and sub-pages)
</p>

<p>
Their corrections have been quite thorough and accurate!  The firm
should be commended for this very responsive action.  E.g., the main
description pages for RHX say things like:
<pre>
  RHX helps you compare, buy, and manage business applications -- all
  available from the open source leader. All in one place.
 
  We've done the work for you. You'll find profiles, ratings, priceseven
  free trials -- for every application. Working in collaboration with our
  partners, applications are validated to run on Red Hat Enterprise Linux,
  delivered through Red Hat Network, and backed by Red Hat as the single
  point of contact for support.
</pre>
All of the former claims and implications of RHX's offerings being
uniformly open source have been corrected, top to bottom.  I've sent a
specific thanks to the manager in question.
</p>

<pre>-- 
Cheers,     "Learning Java has been a slow and tortuous process for me.  Every 
Rick Moen   few minutes, I start screaming 'No, you fools!' and have to go
rick@linuxmafia.com       read something from _Structure and Interpretation of
            Computer Programs_ to de-stress."   -- The Cube, www.forum3000.org
</pre>

<hr />


</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:141/lg_mail2.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Published in Issue 141 of Linux Gazette, August 2007
</p>

</div>
</div>


<div class="content lgcontent">

<a name="lg_talkback"></a>
<h1>Talkback</h1>

</b>
</p>

<p>

<!-- Thread anchor: Talkback:124/smith.html --><a name='talkback_124_smith_html'></a>
<h3>Talkback:124/smith.html</h3>
<p><b>[ In reference to "<a href='../124/smith.html'>Build a Six-headed, Six-user Linux System</a>" in LG#124 ]</b></p><p>
<b><p>
Dave Wiebe [dawiebe at gmail.com]
</p>
</b><br />
<b>Thu, 12 Jul 2007 18:10:07 -0700</b>
</p>

<p>
Hello,
</p>

<p>
In response to Bobs "Build a six headed, six user linux system"
</p>

<p>
I am curious to know if this would work on my laptop, using the laptop
display screen and a monitor attached through the monitor connection at the
back.  Would my standard laptop ATI video card be able to support that (as I
read somewhere that Dual Head video cards are preferred).
</p>

<p>
Thanks for your response.
</p>

<p>
--David Wiebe.
</p>

<p>
Laptop:  Dell Inspiron 1501
</p>


<p>

</p>

<p><b>[  <a name="mb-talkback_124_smith_html"></a> <a href="misc/lg/talkback_124_smith_html.html">Thread continues here (2 messages/2.20kB)</a>  ]</b></p>
<hr />


<!-- Thread anchor: Talkback:68/tag/9.html --><a name='talkback_68_tag_9_html'></a>
<h3>Talkback:68/tag/9.html</h3>
<p><b>[ In reference to "<a href='../68/tag/9.html'>/tag/9.html</a>" in LG#68 ]</b></p><p>
<b><p>
Mike Steele [mike_steele_2000 at yahoo.com]
</p>
</b><br />
<b>Thu, 12 Jul 2007 13:32:20 -0500</b>
</p>

<p>
In response to
</p>

<p>
Users are caught in the middle of a debate over whether reverse records 
should be used for identification. The pro argument is that it helps 
identify spammers and abusers. The con argument (which I believe) is 
that the purpose of domain names is convenience: so you don't have to 
remember a number, and so that a site can maintain a "permanent" 
identifier even if they move to another server or a different ISP. You 
shouldn't /have/ to have a domain name, much less have it set to any 
particular value. And to identify scRipT kIddyZ, just do a simple 
traceroute. The second-last hop is their ISP (or part of their own 
network), and ISPs always have their own domain name showing. And what 
if a computer has several domain names, each hosted at a different 
organization? There can be only one reverse record, so all the other 
names will be left out in the cold.
</p>

<p>
at <a href="../issue68/tag/9.html">http://linuxgazette.net/issue68/tag/9.html</a>
</p>

<p>
I always assumed the cost to be for the liability. Otherwise they would 
just automate it and have you do it online.  This seems to thwart spam; 
email administrators put an additional reverse record lookup to make 
sure it is not a "rogue" mail server put online by just anyone.  They 
have to contact their ISP and get a reverse record.  It's like adding 
locks to your door.  It isn't a guarantee - just a deterrent.  That one 
reverse record is for the mail server.  Otherwise RR's are not necessary 
and so it doesn't matter if there are multiple domains.  You can CNAME 
the domains MX records to the actual server name.
</p>

<p>
Dunno for sure but thats my 2 cents.
</p>

<p>
Mike
</p>

<p>

</p>

<hr />


<!-- Thread anchor: Talkback:140/dyckoff.html --><a name='talkback_140_dyckoff_html'></a>
<h3>Talkback:140/dyckoff.html</h3>
<p><b>[ In reference to "<a href='../140/dyckoff.html'>Away Mission: Sem-Tech 07 Conference, May 2007, San Jose, CA</a>" in LG#140 ]</b></p><p>
<b><p>
Neil Youngman [ny at youngman.org.uk]
</p>
</b><br />
<b>Mon, 2 Jul 2007 16:42:02 +0100</b>
</p>

<p>
We seem to have a broken link in this article, just below the screenshot it 
says "To view a demonstration of Cognition's technology, please visit 
www.CognitionSearch.com" and links to 
<a href="../current/">http://linuxgazette.net/current/</a>'<a href="http://cognitionsearch.com">http://cognitionsearch.com</a>
</p>

<p>
There's probably also a small typo "Hogkin's disease" should probably 
be "Hodgkin's disease"
</p>

<p>
Neil
</p>

<p>

</p>

<p><b>[  <a name="mb-talkback_140_dyckoff_html"></a> <a href="misc/lg/talkback_140_dyckoff_html.html">Thread continues here (2 messages/1.85kB)</a>  ]</b></p>
<hr />


</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:141/lg_talkback.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Published in Issue 141 of Linux Gazette, August 2007
</p>

</div>
</div>


<div class="content lgcontent">

<a name="lg_tips"></a>
<h1>2-Cent Tips</h1>

</b>
</p>

<p>

<!-- Thread anchor: 2 cent tip: Guide to Ubuntu on the Intel Apple iMac --><a name='2_cent_tip__guide_to_ubuntu_on_the_intel_apple_imac'></a>
<h3>2 cent tip: Guide to Ubuntu on the Intel Apple iMac</h3>
<p>
<b><p>
Peter Knaggs [peter.knaggs at gmail.com]
</p>
</b><br />
<b>Thu, 12 Jul 2007 21:41:01 -0700</b>
</p>

<p>
I've been gathering together all the hardware setup info needed to
get Ubuntu working (surprisingly well) with the Intel Apple iMac
</p>

<p>
  <a href="http://www.penlug.org/twiki/bin/view/Main/LinuxHardwareInfoAppleiMac24">http://www.penlug.org/twiki/bin/view/Main/LinuxHardwareInfoAppleiMac24</a>
</p>

<p>
There's not much original work on that page, but at least having
all the hardware setup info one place is handy, so I thought I'd
post it as a 2 cent tip.
</p>

<p>

</p>

<hr />


<!-- Thread anchor: 2-cent tip: Summing up file sizes --><a name='2_cent_tip__summing_up_file_sizes'></a>
<h3>2-cent tip: Summing up file sizes</h3>
<p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Thu, 12 Jul 2007 16:32:00 -0400</b>
</p>

<p>
I just got an iPod Shuffle, and am about to load it up with my favorite
tunes; however, I didn't want to dink around with multiple reloads of
the song list if it was too big. Since flash devices only have so many
write cycles before they start losing their little brains, minimizing
writes is a Good Thing. So, I needed to figure out the total size of the
files in my M3U playlist - and given the kind of questions that often
come up in my shell scripting classes and on LG's Answer Gang lists, I
thought that our readers would find some of these techniques (as well as
the script itself) useful. Herewith, the "m3u_size" script.  Enjoy!
</p>

<p>
<pre class="code">
#!/bin/bash
# Created by Ben Okopnik on Thu Jul 12 15:27:45 EDT 2007
 
# Exit with usage message unless specified file exists and is readable
[ -r "$1" ] || { printf "Usage: ${0##*/} &lt;file.m3u&gt;\n"; exit; }
 
# For the purposes of the loop, ignore spaces in filenames
old=$IFS
IFS='
'
# Get the file size and sum it up
for n in `cat "$1"`; do s=`ls -l "$n" | cut -d ' ' -f 5`; ((total+=$s)); done
# Restore the IFS
IFS=$old
  
# Define G, M, and k
Gb=$((1024**3)); Mb=$((1024**2)); kb=1024
# Calculate the number of G+M+k+bytes in file list
G=$(($total/$Gb))
M=$((($total-$G*$Gb)/$Mb))
k=$((($total-$G*$Gb-$M*$Mb)/$kb))
b=$((($total-$G*$Gb-$M*$Mb-$k*$kb)))
 
echo "Total: $total (${G}G ${M}M ${k}k ${b}b)"
</pre>
<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>

<p><b>[  <a name="mb-2_cent_tip__summing_up_file_sizes"></a> <a href="misc/lg/2_cent_tip__summing_up_file_sizes.html">Thread continues here (13 messages/20.54kB)</a>  ]</b></p>
<hr />


</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:141/lg_tips.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Published in Issue 141 of Linux Gazette, August 2007
</p>

</div>
</div>


<div class="content lgcontent">

<a name="lg_bytes"></a>
<h1>NewsBytes</h1>
<p id="by"><b>By <a href="../authors/dyckoff.html">Howard Dyckoff</a> and <a href="../authors/bisbee.html">Samuel Kotel Bisbee-vonKaufmann</a></b></p>

</b>
</p>

<p>

<div align="center">
<table cellpadding="7" summary="">
  <tr>
    <td>
        <img title="News Bytes" alt=bytes src="../gx/bytes.gif" border="1">
    </td>
    <td>
      <h3><img src="../gx/bolt.gif" alt="thunderbolt" />Contents:</h3>
      <ul>
        <li><A href="#general">News in General</a>
        <li><A href="#events">Conferences and Events</a> 
        <li><A href="#distro">Distro News</a>
        <li><A href="#commercial">Software and Product News</a> 
      </ul>
    </td>
  </tr>
</table>
</div>

<p> Please submit your News Bytes items in plain text; other formats may be
rejected. A one- or two-paragraph summary plus a URL has a much higher
chance of being published than an entire press release. Submit items to <a
href="mailto:bytes@linuxgazette.net">bytes@linuxgazette.net</a>.
</p>

<hr>

<a name="general"></a>
<h2>News in General</h2>

<h3><img src="../gx/bolt.gif" alt="thunderbolt">Ian Murdock, Sun, Unveil Project Indiana</h3>

<p>
In effort to grow its open source Solaris community, Sun launched an
effort to make the Solaris OS more like Linux distros. Ian Murdock,
founder of the Debian distro and Linux company Progeny, explained the
details to Sun partners and the press in July, and in the process
revealed more about his role at Sun since being hired there.</p>

<p>
Murdock had spoken to a packed session at JavaOne, last May, and talked
in general terms of improving the packaging and content of Solaris to
make it friendlier to the Open Source community. Project Indiana is that
effort, and adds a road map for Solaris developers.</p>

<p>
Murdock spoke about the similarities and differences of Linux and Solaris,
noting the "distro" model as a Linux "innovation" for users and
developers.   Both Solaris and Linux support GNOME and the X Window
System, office and Web apps, he noted.  "Right now, it's confusing.  If
you want to compile Open Solaris, you need to use Solaris Express
Community Edition first," according to Murdock.  "Originally, Linux
didn't have a binary deliverable, so I and others stepped up."</p>

<p>
Project Indiana aims to combine the best of both the Solaris and
Linux worlds. With Murdock on-board, Sun has been looking at the distro
and package model that is now intrinsic to Linux.  Open Solaris will
have a developer community, while still having enterprise-grade support
from Sun and its partners. Sun hopes users of Open Solaris will
eventually convert to full enterprise support licenses.</p>

<p>
"We will provide Open Solaris as a binary distro, with a strong focus on
unique Solaris features and the famous Solaris compatibility." The main
features of Indiana will include an easier install, ZFS as default file
system, and Net-based package management.</p>

<p>
"We are competing in the same sense that RH competes with Debian, or
Ubuntu competes with Debian. We are growing the Open Source
environment," Murdock explained. "We can bridge the gap for developers,
but we have to give them some compelling reasons to cross over. We have
an opportunity here to provide the same level of choice that Linux has
provided, but without the fracture..." Murdock added.  </p>

<p>
Project Indiana will be for developers and early adopters, with short
6-month release cycles, while Enterprise Solaris will have long release
cycles. It will be a 2-tier model, with a dev tier more like the Linux
model.  The first Indiana release is planned for the Spring of 2008,
with an earlier test release to selected users in the Fall of 2007.</p>

<p>
Murdock was more closed when discussing the issue of open source
licenses.  First, he said, that was under consideration, and would 
be decided by others at Sun.  He did note that the criteria would
include what was best for Sun, its partners, and the Solaris community.
</p>

<p>
"We are very focused on the technology parts of this. I don't tend to
find licensing discussions very interesting. To me, licenses are a
necessary evil. The real question is... is the fact that Solaris is not
under GPL going to matter? My view on the license changes is that if it
benefits Sun and the OS community, we should do it.  Another way of
looking at this is...  we could take the device driver code in Linux,
and help make the driver support in Solaris much better than it is
today.  So, that to me is a reason to GPL Solaris. The ability to borrow
code could be an advantage, but then our code can also be used."
Murdock added that the decision process would have to include the
growing developer community for Open Solaris.  In May and June, Sun CEO
Jonathon Schwartz expressed a strong interest in the new GPL v3, but
seems to be backing away from that position more recently.  </p>

<h3><img src="../gx/bolt.gif" alt="thunderbolt">$100 Laptop for 3rd World Kids Delayed, but Intel Joins the Party</h3>

<p>
At the Usenix '07 Technical Conference in June, Mary Lou Jepson, CIO of
One Laptop Per Child (OLPC), detailed progress on the hardware and
software development of this new portable computing platform, and
explained that the first mass shipments to Third World countries was
planned for the end of summer.  That has now been pushed back to October,
to accommodate fixes and new features and to wait for the required 3
million-order threshold.  (The first generation units will cost
$150-175, but will approach the $100 goal over the next year.)</p>

<p>
Intel, whose executives had strongly criticized the non-standard
technology and novel approach of OLPC, buried the conflict with OLPC in
July and joined its board in July. Intel has what it sees as a competing
effort with its "Classmate" mini-laptop design, based on Windows and
Intel chipsets.  (There is also an option to use Mandriva.)  The target
price for the "Classmate" is about $300, but includes newly developed
educational software. A fuller description of this PC is here:  <a
href="http://www.classmatepc.com/">http://www.classmatepc.com</a>
</p>

<p>
In contrast, OLPC uses the open source development model, and is trying
to develop a system that Third World kids and their teachers can support
and fix themselves.  The OS is a reduced version of Red Hat Linux, and
many design changes have been incorporated from Third World pilot testers,
to support tropical environments, erratic power, the likelihood of
laptops falling on stone or concrete floors, etc. The keyboard is sealed
to allow use in rainy environments. New designs in the LCD design and
graphics controller allow the laptop screen to be scrolled without CPU
involvement, and used in direct sunlight with only .1 watt per hour power
consumption. That allows an OLPC unit with a charged battery to run all
day and all night. There are also hand-powered generators and solar
charging stations for classroom use.  (Who wouldn't want one of these
for camping or car trips?)  The XO is also the greenest PC, in terms of
manufacturing and recycling.  </p>

<p>
The OLPC "XO" units employs an efficient dual antenna, and automatically
creates an inter-meshed peer-peer network across a village, with line of
sight distances of up to half a kilometer between units. There is also
learning software for kids called "Sugar" that is still in development.
Linux kernel developers and software engineers are encouraged to join in
different parts of this community project.  Here is the link to sign up:
<a href="http://wiki.laptop.org">http://wiki.laptop.org</a>.  </p>

<p>
The OLPC keynote is now available on the Usenix conference archive (<a
href="http://www.usenix.org/events/usenix07/tech/slides/jepsen.pdf">http://www.usenix.org/events/usenix07/tech/slides/jepsen.pdf</a>).
Photos from the presentation are available here:  <a
href="http://picasaweb.google.com/howarddy/Usenix07OLPC">picasaweb.google.com/howarddy/Usenix07OLPC</a>.
OLPC is based on learning theories pioneered by Seymour Papert and Alan
Kay.  </p>


<h3><img src="../gx/bolt.gif" alt="thunderbolt">Intel Hosts Moblin.org
Site for Linux Mobility Developers</h3>

<p>
Moblin.org is the site that hosts the Mobile &amp; Internet Linux
Project open source community. The site allows developers to prototype
new ideas, and build community around them.  Intel has also garnered
support from vendors Canonical and Red Flag Linux.  </p>

<p>
Currently, <a href="http://www.moblin.org"
title="moblin.org">moblin.org</a> hosts a number of projects, including
an Image Creator, Browser, UI framework, power policy manager, and
various non-PC-oriented applications and software components.  </p>

<p>
"Linux on mobile devices has gotten a lot of traction in the last few
years," says Dirk Hohndel, Intel's chief Linux and open source
technologist. "Moblin.org will help to improve the integration of many
of the existing components targeted at such devices, and should help
foster development of new use models and new features. In particular,
both power management and UI need to be fine-tuned for these new form
factors, and that is an area where development at moblin.org has already
shown good results." </p>


<h3><img src="../gx/bolt.gif" alt="thunderbolt">Python Database Tool
First Open Source Component of Launchpad</h3>

<p>
Canonical has released Storm, a generic open source object relational
mapper (ORM) for Python. Storm is designed to support communication with
multiple databases simultaneously. Canonical is known for the popular
Ubuntu operating system and Launchpad, a Web-based collaboration
platform for open source developers.  </p>

<p>
"Storm is an ORM that simplifies the development of database-backed
applications in Python, especially for projects that use very large
databases or multiple databases with a seamless Web front-end", said
Gustavo Niemeyer, lead developer of Storm at Canonical. "Storm is
particularly designed to feel very natural to Python programmers, and
exposes multiple databases as /stores/ in a clean and easy-to-use
fashion." </p>

<p> The project has been in development for more than a year, and is now
publicly available under the LGPL license. This is the first complete
Launchpad component to be released as open source software. Launchpad
currently includes developer data for several thousand projects, and is
used by tens of thousands of developers and other free software
contributors.  </p>

<p> The Storm project welcomes participation, and has a new Web site at
<a href="http://storm.canonical.com/">http://storm.canonical.com</a>.
That site includes a tutorial, and links to allow developers to
download, report bugs, and join the mailing list.  </p>

<h3><img src="../gx/bolt.gif" alt="thunderbolt">Personal Package Archive
Service, Free Space for Ubuntu Developers</h3>

<p>
Canonical also announced the beta release of the Launchpad Personal
Package Archive (PPA) service, a new way for developers to build and
publish packages of their code, documentation, and other contributions
to free software.  </p>

<p>
Individuals and teams can each have a PPA, allowing groups to
collaborate on sets of packages, and solo developers to publish their
own versions of popular free software. Developers upload packages to a
PPA, and have it built for multiple architectures against the current
version of Ubuntu. Each user gets up to one gigabyte of Personal Package
Archive space, which works as a standard Ubuntu software package
repository. Free PPAs are available only for free ("libre") software
packages.  </p>

<p>
Mark Shuttleworth, founder of Ubuntu, explained the significance of
Launchpad Personal Package Archives for the Ubuntu community: "Many
developers want to modify existing packages, or create new packages of
their software. The PPA service allows anyone to publish a package,
without having to ask permission or join the Ubuntu project as a
developer. This is a tremendous innovation in the free software
community." </p>

<p>
The Launchpad PPA service is currently in beta. To participate in the
beta program, send an e-mail to ppa-beta@launchpad.net.  </p>

<p>
Launchpad PPA Service will be released for general use on August 22,
2007, and will be available at <a
href="https://launchpad.net/ubuntu/+ppas">https://launchpad.net/ubuntu/+ppas</a>
</p>

<h3>Animators to Compete for 'Viking Animator' Title at SIGGRAPH 2007</h3>

<p>
SIGGRAPH officials will hold the international <em>FJORG!</em>
competition - an "iron animator" event - where 15 competing teams from
around the world have 32 hours to create the world's best
character-driven animation in front of a live, "Gladiator-style"
audience and judging panel, at the San Diego Convention Center.  </p>

<p> To earn the title of "Viking Animator" and several other prizes,
each team will be tasked with creating a 15-second (or longer)
animation, based on a theme to be provided during the <em>FJORG!</em>
kickoff on Monday, 6 August 2007. In the spirit of true competition,
<em>FJORG!</em> will test contestants' skill, talent, creativity,
teamwork, and physical endurance - all throughout multiple staged
distractions such as live music, belly dancing, acrobatics, and martial
arts performances.  </p>

<p>
<em>FJORG!</em> contestants will be required to complete the animations
using their own talents and skills, along with technology assets
supplied at the event. (Outside resources are not permitted.) Competitors
will have access to the following applications: </p>

<ul>
  <li>
    Sound and voiceover selections
  </li>
  <li>
    Rigged model for Maya
  </li>
  <li>
    51 HP xw9400 Workstations with Dual Core AMD Opteron processors
  </li>
  <li>
    Autodesk 3ds Max and Maya
  </li>
  <li>
    Macromedia Flash
  </li>
  <li>
    Adobe Photoshop and After Effects
  </li>
  <li>
    Softimage|XSI
  </li>
</ul>

<p>
With support provided by AMD, DreamWorks Animation, and HP,
<em>FJORG!</em> is being held in conjunction with SIGGRAPH 2007, the
34th International Conference and Exhibition on computer graphics and
interactive techniques at the San Diego Convention Center, August 5-9,
2007. For video of the competition and the winning animations, visit <a
href="http://www.workstations.tv/">http://www.workstations.tv</a>,
throughout SIGGRAPH. For more information about the competition in
general, visit <a
href="http://www.siggraph.org/s2007/presenters/fjorg/">http://www.siggraph.org/s2007/presenters/fjorg/</a>.
</p>


<h3><img src="../gx/bolt.gif" alt="thunderbolt">Mozilla Firefox use up in Europe</h3>

<p>
Frances XiTi Monitor has reported solid gains for Mozilla-based Web
browsers. Overall share of the European market grew from 24.1% to 27.8%.
This growth includes Firefox's gain of <strong>nearly 7 points in the
last year.</strong> </p>

<p>
Slovenia has the highest Firefox use in Europe, with a rate that is now over
45%. Finland is close behind. For more details see: <a
href="http://www.xitimonitor.com/en-us/browsers-barometer/firefox-july-2007/index-1-2-3-102.html">http://www.xitimonitor.com/en-us/browsers-barometer/firefox-july-2007/index-1-2-3-102.html</a>.
</p>

<h3><img src="../gx/bolt.gif" alt="thunderbolt">The Commonwealth of
Massachusetts Adds Open XML Document Format</h3>

<p>
The great state of Massachusetts had come out of the wilderness like a
latter-day Solomon, and decided to support both Microsoft's Office Open
XML format and the OASIS Open Document Format that it has previously
supported.  The decision to adopt the ECMA version of Open XML was made
in early July, and is part of the Massachusetts Enterprise Technical
Reference Model (ETRM) 4.0, a specification for the state's IT
operations. The draft listed ECMA-376 as one of its major revisions.
Earlier, the ETRM specified only ODF as a standard, open format.  </p>

<p>
Since its original adoption of ODF, two of the state's CIOs have been
forced to resign.  </p>

<p>
Microsoft has been lining up Linux distros to be part of the Open XML
Translator project, which is hosted on SourceForge. Currently, these
include Linspire, Novell, and Xandros.  <a
href="http://www.networkworld.com/news/financial/sun.html">Sun</a> has
also released its own Plugin ODF translator for Office. The state IT
group took these efforts into account during its deliberations.  </p>

<p>
The Linux Foundation has challenged the decision, saying  "... it is
wrong for the ITD to conclude that a specification that helps to
perpetuate the dominance of a single product can be properly called a
true open standard", and it encouraged concerned members of the open
source community to send their comments to the Massachusetts ITD at <a
href="mailto:standards@state.ma.us">standards@state.ma.us</a>.  </p>

<p>
Linux Foundation board member Andrew Updegrove wrote to the
Massachusetts IT Department, asking them to consider the public and
historical impacts, "It is also important to the future of open document
formats in the wider world, as well. The impact of Massachusetts in
rectifying the historical situation has already been profound. But it
has not been sufficient.  Earlier this decade, Microsoft properly looked
out for its stockholders' best interests by declining to participate in
the OASIS working group that created ODF, thereby increasing the
likelihood that ODF would die, and increasing the likelihood that its
dominance would continue. That decision was not, of course, the best
decision for end-users, including government purchasers, because it
perpetuated a situation where long-term access to important documents
was in the control of a single vendor. </p>

<p>
"But ODF did not die. Instead, it was completed, although it received
little attention, either publicly or among potential implementers. Only
with the announcement of the ITD's decision and the realization that a
market for ODF-compliant products might develop did interest broaden and
deepen. Microsoft is hardly to be blamed for lending no support to the
success of ODF. But neither should it be rewarded for launching a
competing, self-serving standard as a next-best defense against erosion
of its dominant position." </p>

<p>
This issue is important in itself for the future of open standards, but
the main battle is the September ballot in ISO on supporting OOXML. That
could derail ODF and a wider range of open standards.  </p>

<p>
Here is a link to articles discussing the position of the Linux
Foundation: <a
href="http://lxer.com/module/newswire/byuser.php?user=Andy_Updegrove">http://lxer.com/module/newswire/byuser.php?user=Andy_Updegrove</a>.
</p>


<a name="events"></a>
<h2>Events</h2>

<h3>July</h3>

<p>
FiXs West Coast Conference<br/> July 31 - August 1; Monterey,
California; <a
href="http://www.fixs.org/events.htm">http://www.fixs.org/events.htm</a>
</p>

<h3>August</h3>

<p>
Black Hat Security Conference<br/>
July 31 - August 2; Las Vegas, NV; <a href="http://www.blackhat.com/html/bh-link/briefings.html">http://www.blackhat.com/html/bh-link/briefings.html</a>
</p>

<p>
Security '07 / HotSec '07<br/>
August 6-10; Boston, MA; <a href="http://www.usenix.org/events/hotsec07/">http://www.usenix.org/events/hotsec07/</a>
</p>
  
<p>
MetriCon 2.0, Workshop on Security Metrics<br />
August 7; Boston, MA; <a
href="http://www.securitymetrics.org/content/Wiki.jsp?page=Metricon2.0">http://www.securitymetrics.org/content/Wiki.jsp?page=Metricon2.0</a>
</p>
  
<p>
SIGgraph 2007<br/> 
August 5 - 9; San Diego, CA; <a href="http://www.siggraph.org/s2007/attendees/schedule/">http://www.siggraph.org/s2007/attendees/schedule/</a>
</p>

<p>
Linux World - 2007<br/> 
August 6 - 9; San Francisco, CA; <a href="http://www.linuxworldexpo.com/dev/12/events/12SFO07A">Linux World - 2007</a>
</p>

<p>
Real-World Java Seminar<br/> 
August 13; Roosevelt Hotel, New York City; <a href="http://realworldjava.com/">http://realworldjava.com/</a>
</p>

<h3>September</h3>

<p>
Linux Kernel '07 Developers Summit<br/>
September 4 - 6; Cambridge, UK; <a href="http://www.usenix.org/events/kernel07/">http://www.usenix.org/events/kernel07/</a>
</p>

<p>
1st International LDAPv3 Conference<br/>
September 6 - 7; Cologne, Germany; <a href="http://www.guug.de/veranstaltungen/ldapcon2007/">http://www.guug.de/veranstaltungen/ldapcon2007/</a>
</p>

<p>
Rich Web Experience Conference<br/>
September 6 - 8; Fairmont Hotel, San Jose, CA; <a href="http://www.therichwebexperience.com/">http://www.therichwebexperience.com/</a>
</p>

<p>
BEAWorld 2007 - San Francisco<br/>
September 10 - 12; Moscone Convention Center, San Francisco, CA; <a href="http://www.bea.com/beaworld/us/">http://www.bea.com/beaworld/us/</a>
</p>

<p>
RailsConf Europe 2007<br/>
September 17 - 19; Berlin, Germany; <a href="http://www.railsconfeurope.com/">http://www.railsconfeurope.com/</a>
</p>

<p>
Gartner Open Source and Web Innovation Summits<br/> 
September 17 - 21; Las Vegas, NV; <a href="https://www.gartner.com/EvReg/evRegister?EvCd=OS3">https://www.gartner.com/EvReg/evRegister?EvCd=OS3</a>
</p>

<p>
Intel Developer Forum - 2007<br/>
September 18 - 20; Moscone Center West, San Francisco, CA; <a href="http://developer.intel.com/IDF/">http://developer.intel.com/IDF/</a>
</p>

<p>
Software Development Best Practices 2007 and Embedded Systems Conference<br/>
September 18 - 21; Boston, MA; <a href="http://www.sdexpo.com/2007/sdbp/">http://www.sdexpo.com/2007/sdbp/</a>
</p>

<p>
RFID World Boston<br/>
September 19 - 20; Boston, MA; <a href="http://www.shorecliffcommunications.com/boston/">http://www.shorecliffcommunications.com/boston/</a>
</p>

<p>
AJAXWorld Conference West<br/>
September 24 - 26; Santa Clara, CA; <a href="http://www.ajaxworld.com/">http://www.ajaxworld.com/</a>
</p>

<p>
Semantic Web Strategies Conference 2007<br/>
September 30 - October 1; San Jose Marriott, San Jose, CA; <a href="http://www.semanticwebstrategies.com/" >http://www.semanticwebstrategies.com/</a>
</p>

<h3>October</h3>

<p>
Ethernet Expo 2007<br/>
October 15 - 17, 2007; Hilton, New York, NY; <a href="http://www.lightreading.com/live/event_information.asp?survey_id=306" >http://www.lightreading.com/live/event_information.asp?survey_id=306</a>
</p>

<p>
ISPCON FALL 2007<br/>
October 16 - 18; San Jose, CA; <a href="http://www.ispcon.com/" >http://www.ispcon.com/</a>
</p>

<p>
Interop New York<br/>
October 22 - 26; <a href="http://www.interop.com/">http://www.interop.com/</a>
</p>

<h3>November</h3>

<p>
CSI 2007<br/>
November 3 - 9; Hyatt Regency Crystal City, Washington, DC; <a href="http://www.csiannual.com/">http://www.csiannual.com/</a>
</p>

<p>
Interop Berlin<br/>
November 6 - 8; <a href="http://www.interop.eu/">http://www.interop.eu/</a>
</p>

<p>
Oracle OpenWorld San Francisco<br/>
November 11 - 15; San Francisco CA; <a href="http://www.oracle.com/openworld/">http://www.oracle.com/openworld/</a>
</p>

<p>
Supercomputing 2007<br/>
November 9 - 12; Tampa, FL; <a href="http://sc07.supercomputing.org/">http://sc07.supercomputing.org/</a>
</p>

<a name="distro"></a>
<h2>Distros</h2>

<p>
The latest stable version of the Linux kernel is
<a href="http://kernel.org/pub/linux/kernel/v2.6/patch-2.6.22.1.bz2">2.6.22.1</a>
</p>

<h3><img src="../gx/bolt.gif" alt="thunderbolt">Linspire 6.0, Open XML Translator Released</h3>

<p>
In late July, Linspire released the updated version of its distro,
including the Microsoft media codecs and file filters. Version 6.0,
based on the Ubuntu 7.04 distro, includes KDE 3.5, SMP (Dual Core)
kernel support, improved boot times, and support for the current Windows
Media 10 audio and video codecs. Linspire 6 will have Microsoft TrueType
fonts, including Arial, Georgia, Times New Roman, and Verdana.  </p>

<p>
The Open XML Translator enables bi-directional compatibility, so that
files saved in Open XML can be opened by OpenOffice.org users, and files
created by OpenOffice.org can be saved in Open XML format. As a result, end
users of Microsoft Office and <a
href="http://www.openoffice.org/">OpenOffice.org</a> will now be able to
more easily share files, as documents will better maintain consistent
formats, formulas, and style templates across the two office
productivity suites.  </p>

<p class="editorial">
[ Except... not really. Once you get past the hype of the Linspire press
release, please see "The Commonwealth of Massachusetts adds Open XML
Document Format" on this very NewsBytes page. -- Ben ] </p>

<p>
The open source Open XML/ODF Translator project can be viewed here: <a
href="http://sourceforge.net/projects/odf-converter/">http://sourceforge.net/projects/odf-converter/</a>.
</p>

<h3><img src="../gx/bolt.gif" alt="thunderbolt">Sun Donates High Availability Cluster Code</h3>

<p>
Sun Microsystems has announced it will release the <a
href="http://www.sun.com/software/solaris/cluster/">Solaris Cluster</a>
source code through the <a
href="http://www.opensolaris.org/os/community/ha-clusters/">HA (High
Availability) Clusters community</a> on the <a
href="http://www.opensolaris.org/os/">OpenSolaris</a> site.  Sun's first
contributions are application modules, or agents, which enable open
source or commercially available applications to become highly available
in a cluster environment.  </p>
  
<p> The Open HA Cluster code will be made available in three phases,
beginning at the end of June and continuing over the next 18 months. In
the first phase of Open HA Cluster, Sun will deliver code for most of
the high-availability agents offered with the Solaris Cluster product.
Solaris Cluster's high-availability agents allow developers to
cluster-enable applications to run as either scalable or failover
services. Sun is also making available the source code for the Solaris
Cluster Automated Test Environment (SCATE), along with agent-related
documentation, to assist in testing new agents. The test framework and
first test suite will be contributed at the end of June 2007.  Among
these agents are the Solaris Containers agent, the BEA Weblogic agent,
and PostgreSQL.  </p>
  
<p> Agents written using Open HA Cluster will also run on Solaris
Cluster version 3.2 on the Solaris 10 OS. Subsequent phases of Open HA
Cluster will include delivery of the code for the recently released
Solaris Cluster Geographic Edition - software that enables multi-site
disaster recovery by managing the availability of application services
and data across geographically dispersed clusters. Later, Sun will
release the code for the core Solaris Cluster infrastructure, again with
SCATE infrastructure tests and documentation.  Sun is using the Common
Distribution and Development Licence (CDDL) for the code, as it
currently does for Solaris.  </p>
  
<p> To learn more about Open High Availability Cluster and the HA
Clusters community on OpenSolaris, and to review a complete list of the
high-availability agents offered with the Solaris Cluster product,
please visit: <a
href="http://www.opensolaris.org/os/community/ha-clusters/">http://www.opensolaris.org/os/community/ha-clusters/</a>
</p>

<h3><img src="../gx/bolt.gif" alt="thunderbolt">Samba Switches to GPL v3</h3>

<p>
Samba.org has announced that future distros will be released under the
new GPL v3.  Prior versions of Samba will remain under the GPL v2.  </p>

<p>
In the July announcement, Samba.org specified that all versions of Samba
numbered 3.2 and later will be under the GPLv3, whereas all versions of
Samba numbered 3.0.x and before will remain under the GPLv2.  But there
is a caveat, as noted in the FAQ about the licensing change: </p>

<p>
<em>"The Samba Team releases libraries under two licenses: the GPLv3 and
the LGPLv3. If your code is released under a "GPLv2 or later" license,
it is compatible with both the GPLv3 and the LGPLv3 licensed Samba
code.</em> </p>

<p>
<em>"If your code is released under a 'GPLv2 only' license, it is not
compatible with the Samba libraries released under the GPLv3 or LGPLv3,
as the wording of the 'GPLv2 only' license prevents mixing with other
licenses. If you wish to use libraries released under the LGPLv3 with
your 'GPLv2 only' code, then you will need to modify the license on your
code."</em> </p>

<a name="commercial"></a>
<h2>Software and Product News</h2>

<h3><img src="../gx/bolt.gif" alt="thunderbolt">Zenoss Releases Enterprise Edition</h3>

<p>
Zenoss, known for its existing free IT monitoring product Zenos Core 2
(released in June under GPL2, with all sub-projects under GPL or
compatible licenses), has released Zenoss Enterprise Edition 2.0 on top
of its Core 2 platform. Enterprise Edition 2.0 will provide all the same
features as its free, open source relative, but will provide "end-user
experience monitoring" for Web, e-mail, and database applications. This
allows IT managers to receive integrated user reports along with the
standard network and equipment statuses, while providing more depth to
the IT person's understanding of users' habits.  Zenoss is also
releasing custom ZenPacks - a plug-in framework that allows community
members to write their own features, skins, etc. - that will be
published only under their Enterprise Subscription. For more information on
Zenoss's products, subscriptions, and how they all relate to one another,
visit <a
href="http://www.zenoss.com/product/overview">http://www.zenoss.com/product/overview</a>.
</p>

<p>
Zenoss runs on Linux (GNU build environment required; Red Hat
Enterprise, Fedora Core, Ubuntu, and SUSE are known to work), FreeBSD,
and Mac OS X. VMplayer and Zenoss Virtual Appliance are required to run
Zenoss products on Windows.  </p>

<p>
Zenoss's homepage: <a
href="http://www.zenoss.com">http://www.zenoss.com</a> </p>

<h3><img src="../gx/bolt.gif" alt="thunderbolt">The Everex IMPACT GC3502 Features OpenOffice.org on Windows Vista</h3>

<p>
On July 28th, Everex announced that its IMPACT GC3502 PC will feature
the open-source office suite OpenOffice.org. Aimed at back-to-school
students, Everex is hoping to offer potentially low-budget consumers a
powerful, fully featured package. Most PC producers offer either 
proprietary, high-cost office suites, which sharply increases the cost
of the PC, or pre-installed trial software, which dissatisfies customers
and does not look good for the producer. Everex believes that by keeping
its prices low and its offers large it will get a jump start on its
competitors, this quickly approaching school year.  </p>

<p>
Everex IMPACT GC3502 specifications: <a
href="http://www.everex.com/gc3502/">http://www.everex.com/gc3502/</a><br/>
OpenOffice.org applications included: Writer, Calc, Impress, Draw, Base, Math<br/>
<br/>
OpenOffice.org's homepage: <a href="http://www.openoffice.org">http://www.openoffice.org</a><br/>
Everex's homepage: <a href="http://www.everex.com">http://www.everex.com</a><br/>
</p>

<p class="editorial">
[Editor's note: While I think it is wonderful that OpenOffice.org is
receiving more attention and more companies are seeing open source as
more than a server side solution, I am not entirely sure that
generations of Windows Office-trained users are ready for the switch.
Having used OpenOffice.org on Windows and Linux platforms in two
separate educational environments, now - both times as a student - I
remember experiencing some formatting issues when exporting .doc files,
which all my professors wanted, and printing. While I was easily able to
overcome them, this just added another level of frustration that may
prove too much for the average computer user. Also, most school's IT
departments that I have experienced will not support non-standard Mac
and Windows software. While OpenOffice.org has come leaps and bounds in
solving these transitional issues, I would not want to deploy a solution
in an office - or my grandparent's house - unless the basic
functionality translated seamlessly. -- S. Bisbee] </p>

<h3><img src="../gx/bolt.gif" alt="thunderbolt">Network Engines Integrates Appliance-Optimized Linux Distribution</h3>

<p>
Network Engines, Inc., a provider of storage and security appliance
products and services, announced two software initiatives to help
software developers deliver applications as appliances. First, Network
Engines announced Appliance Certified Edition Linux (ACE Linux), the
first Linux distribution to deliver integrated lifecycle management for
server appliances.  Second, the company also announced that Network
Engines Web-based Services  -  NEWS - its hardware and image management
system for Windows-based application appliances, is now available for
ACE Linux.  </p>

<p>
Hardware and image management integration with an appliance-proven Linux
release should lower costs and speed time to market for Linux-based
appliances.  "As the appliance market grows, ISVs recognize that they
need to focus on adding value to their application, not the underlying
components like the OS and management plane," said Hugh Kelly, 
Vice-President of Marketing for Network Engines.  "We developed our Network
Engines Web-based Services to provide these functions for Windows
applications, and now we are making them available for Linux." </p>

<p>
ACE Linux is a Conary-based Linux distribution that will be customized
for each customer's platform.  Network Engines will provide full
platform support including drivers, which are commonly a drain on
development resources if ISVs attempt to implement these tools
themselves.  In addition, NEI will provide performance qualification for
all of its hardware platforms.  This combination of reduced footprint
and technical support will enable ISVs to deliver more secure appliances
with much lower development cost.  </p>

<p>
Kelly said that integrating NEWS with the company's own distribution of
Linux enables ISVs to save time, money, and engineering resources,
because Network Engines eliminates the burden of qualifying new OS
releases and the associated expense of update and patch distribution.
The NEWS infrastructure will enable all of the software on an appliance,
including the OS, applications, and management plane, to be managed by a
robust update management system.  </p>

<p>
Network Engines is the only company with a full suite of
appliance-related services for both Linux and Windows environments,
Kelly said.  "While ISVs focus on their core competencies, we provide
all the services they need to deliver their solutions as true appliances
in much less time," Kelly said.  </p>


<h3><img src="../gx/bolt.gif" alt="thunderbolt">XenSource Surpasses 500 Virtualization Customers</h3>

<p>
There are now over 500 commercial customers for XenEnterprise and
XenServer - a doubling of their customer base in the past quarter. New
customers include AmerisourceBergen, Cimex Media UK, Harvard University,
Intuit, Investcorp, KBC Clearing, the Miami Herald, Moen, NASDAQ, Palm,
Rollins, Inc., and Sankyo. Additionally, their free product, XenExpress
has had more than 100,000 downloads, illustrating intense interest in
virtualization solutions.  </p>
  
<p> "Enterprises are realizing that they have a choice in virtualization
and XenSource offers great products that are easy to use, offer high
performance for Windows and Linux, and are very innovative and open,"
said John Bara, VP of marketing for XenSource.  </p>

<p> XenSource will introduce the newest release of its XenEnterprise
product, version 4.0, in late August 2007.  </p>

<h3><img src="../gx/bolt.gif" alt="thunderbolt">Borland, VMware Advance Testing of Virtual Lab Environments</h3>

<p>
<a href="http://www.borland.com/">Borland Software Corporation</a>
announced an integration with VMware software, to enable enterprise
software development organizations to more cost-effectively perform
multi-configuration and cross-platform software application testing, by
maximizing the use of virtualization for test environments.  </p>

<p>
Borland will deliver native support for virtual lab environments, by
integrating its SilkCentral Test Manager, a core component of Borland's
Lifecycle Quality Management (LQM) solution, with VMware Lab Manager
version 2.5.  SilkCentral Test Manager 2007 will extend test management
with seamless access to virtualized test environments in VMware Lab
Manager, addressing a critical challenge to fully test applications
across multiple configurations and platforms.  </p>

<p>
With the ability to call VMware Lab Manager environments directly from
SilkCentral Test Manager, users can create and assign individual tests
to run on many different configurations or platforms, without the high
costs of hardware or time needed to administer physical systems.
</p>

<p> Expected availability for Borland SilkCentral Test Manager 2007 is
the third calendar quarter of 2007.  </p>

<p>
For more information on Borland SilkCentral Test Manager and related
technologies, please visit: <a
href="http://www.borland.com/us/products/silk/silkcentral_test/">http://www.borland.com/us/products/silk/silkcentral_test/</a>.
</p>


</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:141/lg_bytes.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<!-- *** BEGIN bio *** -->
<hr>
<p>

<img align="left" alt="Bio picture" src="../gx/authors/dyckoff.jpg" class="bio">

<em>

Howard Dyckoff is a long term IT professional with primary experience at
Fortune 100 and 200 firms. Before his IT career, he worked for Aviation
Week and Space Technology magazine and before that used to edit SkyCom, a
newsletter for astronomers and rocketeers. He hails from the Republic of
Brooklyn [and Polytechnic Institute] and now, after several trips to
Himalayan mountain tops, resides in the SF Bay Area with a large book
collection and several pet rocks.

</em>
<br clear="all">
<!-- *** END bio *** -->

	<hr>
<p>
<img align="left" alt="[BIO]" src="../gx/authors/bisbee.jpg" class="bio">
</p>

<em>
<p>
Sam was born ('87) and raised in the Boston, MA area. His interest in all
things electronic was established early by his electrician father and database
designer mother. Teaching himself HTML and basic web design at the age of 10,
Sam has spiraled deeper into the confusion that is computer science and the
FOSS community. His first Linux install was Red Hat, which he installed on a
Pentium 233GHz i686 when he was about 13. He found his way into the computer
club in high school at Northfield Mount Hermon, a New England boarding school,
which was lovingly named GEECS for Electronics, Engineering, Computers, and
Science. This venue allowed him to share in and teach the Linux experience to
fellow students and teachers alike. Late in high school Sam was abducted into
the Open and Free Technology Community, had his first article published, and
became more involved in the FOSS community as a whole. After a year at 
Boston University he decided the experience was not for him, striking out on
his own as a software developer and contractor.
</p>
</em>

<br clear="all">


<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2007, <a href="../authors/dyckoff.html">Howard Dyckoff</a> and <a href="../authors/bisbee.html">Samuel Kotel Bisbee-vonKaufmann</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 141 of Linux Gazette, August 2007
</p>

</div>
</div>


<div class="content lgcontent">

<a name="anonymous"></a>
<h1>GRUB, PATA and SATA</h1>
<p id="by"><b>By <a href="../authors/anonymous.html">Anonymous</a></b></p>

</b>
</p>

<p>
<h3>1   The Problem</h3>

<p>
Start your search engine and search for the following
keywords both on the Web and in the newsgroups:
</p>

<pre>
GRUB SATA confused
</pre>

<p>
You will get reports galore of GRUB failing with SATA, most
certainly failing when facing a mix of old IDE (PATA) and
new SATA disks. I'm submitting this as evidence, but I
personally don't need any, since GRUB cost me some 10 hours
of toiling in exactly this context.
</p>

<p>
You will find less evidence for Lilo, but Lilo will also
fail. Xandros 4.0, which relies on Lilo, gave me a bad time.
Further, Lilo is on its way out; the major distros rely on
GRUB.
</p>

<p>
So what is to be done, if you have a mix of PATA and SATA, and
want to have a few GNU/Linux distros ready to boot, and need
to have Windows anyway? Repeat after me: <em>do not install GRUB
to the Master Boot Record on the disk carrying the Windows C
partition.</em>
</p>

<p>
It is assumed the problem does not stem from the hardware,
because there are indeed mobos with both SATA and PATA, and
are able to handle them together.
</p>


<h3>2   Accessing the GRUB boot record</h3>

<p>
If we write a GRUB boot record to a partition, how do we
activate it? Not from GRUB's Master Boot Record (MBR); we
don't have one, given our earlier decision. The options are:
</p>

<p>
    (i)     from Windows's MBR
</p>

<p>
    (ii)    from a real floppy, from a (DOS floppy emulation)
            CD, or from a (DOS floppy-emulation) USB stick.
</p>

<p>
Although frequently recommended, (i) is not a solution. You
will fail if you install GRUB to a boot sector, extract that
boot sector to a file, and append an entry for that file in
boot.ini: It will not work with a mix of PATA and SATA.
Something else is needed.
</p>

<p>
Maybe you belong to the lucky ones who never had problems
with GRUB. Otherwise, here follow solutions A and B, for your
attention. They will work in most cases, but still there
might be hardware constellations where they will fail. There are
too many variations to test, and I certainly do not have
access to the hardware.
</p>

<p>
The instructions given here target Windows XP. They can
probably be adjusted for Vista.
</p>


<h3>3   Terminology</h3>

<p>
Some terminology first. Under Windows, the boot drive is the
one that carries <code>boot.ini</code> and the loader, <code>ntldr</code>. The system
drive is the one that carries the Windows directory. They
can be different, although often they're not. We say
"boot directory" to indicate the directory containing
<code>boot.ini</code>. For Windows XP, the boot partition has to be the
first one seen by the operating system - with the drive
letter C slapped on it. Apparently, Vista accepts any
partition, and just re-arranges the drive letters.
</p>

<p>
When installing or restoring or updating your Linux distro, make
sure that the Master Boot Record for the Windows boot drive
is not touched. If it is, you will have to go through some
rescue operations. That MBR is reserved for Windows - and GRUB
should keep its GRUBby fingers off it. Let it install a boot record
to the boot sector in the partitions - e.g., to /dev/hda4 - but
never to /dev/hda.
</p>

<p>
Be aware that the installation routine may seem set to
comply with your instructions, but then it may go ahead and
modify the MBR anyway. It's difficult to say whether the blame is
with GRUB directly or with the install routines, but it happens.
Be ready for a fight, be ready for emergencies. You might even
consider trying the following trick: let the boot record be
installed to a floppy, then do everything else by
hand. The trick works even if there is no physical floppy.
</p>

<p>
The trouble stems from one simple fact: when GRUB starts
from the Master Boot Record and is showing its menu, it sees
the available devices in an order which <em>may</em> differ from
the order it sees after launching <code>initrd</code> and then the
kernel. In other words, its device map changes on you
without any warnings or compliments. After activating
the required entry from the menu, all hell breaks loose because
essential files are not found. I.e., kernel panic.
</p>

<p>
Ubuntu is trying to handle the problem using UUIDs for the
hard disks. UUID means Universally Unique IDentifier, and is
intended to be the immovable rock in the sea storm of boot
loader, <em>initrd</em>, and kernel. So you will see Ubuntu's GRUB
configuration showing lines like:
</p>

<pre>
    kernel /vmlinuz root=UUID=f0bfe866-2449-4d75-8222-b444ff564876
</pre>

<p>
Long story short - it does not help. This is my empirical
finding. Some theory from Linus Torvalds himself:
</p>

<p>
<a href="http://lwn.net/Articles/65209/">http://lwn.net/Articles/65209/</a>
</p>


<h3>4   Method A</h3>

<p>
Method A uses the 'hide/unhide' feature in GRUB, to hide away
all boot partitions that are contributing confusion - in the
extreme case, all boot partitions but the one that has a
specific operating system. At that moment, there can be no
confusion in the device map, since only one entry is left.
</p>

<p>
This approach is explained here:
</p>

<p>
<a href="http://www.justlinux.com/forum/showthread.php?t=143973">http://www.justlinux.com/forum/showthread.php?t=143973</a>
</p>

<p>
It is based on a GRUB floppy, either as a real floppy or as a
floppy image burnt to a CD. It is not quick to set up, since
it requires extensive trial and error but - to be fair - the
other method is not much quicker.
</p>

<p>
Note that with this method, when you are running a distro,
Windows may be hidden away. If you want to transfer files
from the distros to Windows, you will have to have a FAT
partition accessible at all times - or have access to Linux
file systems from Windows (available only for ext2/ext3, as
far as I know).
</p>


<h3>5   Method B</h3>

<p>
Go and download a modified version of GRUB, including <code>grldr</code>
(mnemonics: 'grub loader'). Put <code>grldr</code> in c:\ and add the
following line to <code>boot.ini</code>:
</p>

<pre>
    c:\grldr = "sundry distros"
</pre>

<p>
You also need GRUB's <code>menu.lst</code> in the same directory. Edit it
as appropriate for each of your distros, and you are done.
</p>

<p>
Fine - but what is <code>grldr</code>, and where do you get it from? <code>grldr</code>
is a GRUB console that gets along with Windows booting, and
weighs less than 190K. It is part of a free software project
with a puzzling name:
</p>

<p>
<a href="http://grub4dos.sourceforge.net/">http://grub4dos.sourceforge.net/</a>
</p>

<p>
You really only need that one file, <code>grldr</code>; the adventurous
can play around with the rest.
</p>


<h3>6   The Correct Device Map</h3>

<p>
As stated above, GRUB's problem with a mix of PATA and SATA
is that its device map shifts while the boot is in process. So,
if you opt for method B, what device map are you going to
throw at it? None: you do not need a <code>device.map</code> file for
method B. But you still need to edit <code>menu.lst</code>, and thus you
need to know what to call your available disks according to
both GRUB and kernel conventions.
</p>

<p>
Start with a <code>menu.lst</code> file that might be right. When the
GRUB menu pops up, go to the GRUB's command line and type
</p>

<pre>
    root (
</pre>

<p>
pressing 'Tab' to get a list of all possible completions, as
seen by GRUB here and now. This is the list of the available
devices. For each one of them repeat the trick, e.g.,
</p>

<pre>
    root (hd0,
</pre>

<p>
plus a 'Tab' will list all partitions in hd0. This way, you get
a list of all partitions on all disks with the correct GRUB
denomination and the partition type. Partitions from the
Unix world will be recognized; so will FAT partitions. NTFS will
not be seen in pure GRUB, but <code>grub4dos</code> can manage. That should be
sufficient for you to identify the hardware.
</p>

<p>
The hard part of the exercise is giving the devices correct
names according to kernel conventions. Would hd0 be <code>/dev/hda</code>,
or what? The shift in the device map occurs here, and you may
have to rely on trial and error. When you have the mapping
of the devices right, the mapping of the partitions is
trivial.
</p>


<h3>7   State of the Nation</h3>

<p>
All this looks like a royal hassle - and it is. Installing an
operating system should not affect operating systems already
installed. Windows has bad manners in this respect, but
does that mean that GRUB should also have bad manners? It should not;
actually, it's supposed to be friendly and co-operative.
</p>

<p>
Well, if you go to the GRUB Web site
(<a href="http://www.gnu.org/software/grub/">http://www.gnu.org/software/grub/</a>),
you'll learn that there is a <em>discontinued</em> GRUB version (0.97)
and a new GRUB version (1.95, as of the time of writing.) The former is called legacy
GRUB; the latter is called GRUB-2 and has been in the works
for 4-5 years. The FAQ for the new GRUB asks straight away
why there is a need for a fresh rewrite of GRUB. Excerpt
from the answer:
</p>

<pre>
    Because GRUB Legacy has become unmaintainable,
    due to messy code and design failures.
</pre>

<p>
It also says that the new version is "usable". Obviously, it
is not, since the major distros rely on legacy GRUB
(customized for their own purposes) and ignore the new one -
I was unable to find even one single distro that uses it.
</p>

<p>
Let's hope GRUB-2 will come up to speed very soon, and that it is
not going to turn into a repeat of the Hurd saga. The
situation is unbearable. While a Windows install takes over
the MBR and impedes booting other operating systems, Windows
still manages to pull itself up by its boot straps. With a
mix of PATA and SATA, installing SUSE or Ubuntu destroys the
Windows MBR, and then fails to boot ANY operating system at
all!
</p>


</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:141/anonymous.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<!-- ============================================================= -->
<!-- *** BEGIN bio *** -->
<hr>

<p>
<img ALIGN="LEFT" ALT="Bio picture" SRC="../gx/2002/note.png" class="bio">
<em>

A. N. Onymous has been writing for LG since the early days - generally by
sneaking in at night and leaving a variety of articles on the Editor's
desk. A man (woman?) of mystery, claiming no credit and hiding in
darkness... probably something to do with large amounts of treasure in an
ancient Mayan temple and a beautiful dark-eyed woman with a snake tattoo
winding down from her left hip. Or maybe he just treasures his privacy. In
any case, we're grateful for his contribution.<br>
 -- Editor, Linux Gazette

</em>
<br CLEAR="all">

<!-- *** END bio *** -->

<!-- ============================================================= -->


<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2007, <a href="../authors/anonymous.html">Anonymous</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 141 of Linux Gazette, August 2007
</p>

</div>
</div>


<div class="content lgcontent">

<a name="brownss"></a>
<h1>An NSLU2 (Slug) Reminder Server</h1>
<p id="by"><b>By <a href="../authors/brownss.html">Silas Brown</a></b></p>

</b>
</p>

<p>

<p>
LG #138 contained an <a href="../138/kapil.html">article</a>, "Debian on a Slug", in which Kapil
Hari Paranjape described how to install Debian on an NSLU2
device, so that it can be made into a general-purpose server (a
firewall, backup server, Web server, etc.) He also added a
sound device so that it can be used to play music.
</p>

<p>
Another application for such a device is as an alarms and
reminders system.  This is more than a simple alarm clock or
PDA, because the Slug is much more programmable: once you have
installed a lightweight speech synthesizer, the Slug can be made
not only to generate verbal reminders (which few PDAs can do), but
also to check for information on the Internet and adjust its
announcements, accordingly. It is a useful alternative to
leaving a PC switched on for long periods just to do that job
(or possibly forgetting something because the computer was
switched off).  If you are attempting to learn a foreign
language, the Slug reminders system can help with that,
also.
</p>

<h3>Speech Synthesis</h3>

<p>
The <a href="http://espeak.sourceforge.net/">eSpeak</a>
speech synthesizer is lightweight enough to run on the Slug, and
is also available as a Debian package, although if you are
running Debian stable then you will likely find that installing
from source will get you a significantly improved version.  (Be
sure to completely remove the Debian package and its libraries,
before compiling from source.) eSpeak produces very clear
English speech in several accents, and also supports quite a few
other languages (some better than others). Installation is
straightforward, except you might find that the audio output
doesn't work; if this is the case, then simply write audio to a
file or pipe and play using <tt>aplay</tt> in the
<tt>alsa-utils</tt> package. (Note, however, that some old versions
of eSpeak won't write to a pipe when asked. If it's just a short
reminder, you can write to a file in <tt>/dev/shm</tt> and
delete it after playing.)
</p>

<p>
Personally I use eSpeak with my language-practice program <A
HREF="http://people.pwf.cam.ac.uk/ssb22/gradint/">Gradint</a>,
which I have adapted to run on the Slug.  Besides generating
vocabulary-practice sessions using "graduated-interval recall",
Gradint can also be made to produce other speech-reminder
"patterns" such as:
</p>

<ul>
<li>Graduated wrap-up alarms ("10 minutes", "5 minutes", "3
minutes", etc.).</li>
<li>"Nags" to get you out of bed in the morning or to do other things
when you are tired.</li>
<li>Timed second-language or multilingual instructions for performing various
aspects of personal-care activities, etc., in order to practice
the language and also allowing you to optimise the
schedule for maximum energy-efficiency or whatever.
(This could also be of <em>some</em> use for people with disabilities
that create a need for prompting.)</li>
</ul>

<p>
For languages that eSpeak cannot yet produce well, if all the
possible utterances are known in advance, then Gradint allows you
to generate them on another system and transfer them across (or
simply use recorded sounds instead).
</p>

<h3>Dealing with Unreliable Sound</h3>

<p>
It appears that some USB sound adapters will fail, from time to
time, especially if they are plugged into an unpowered hub.
This is probably due to their highly variable power consumption.
The symptoms are that the sound stops, and the system behaves as
though the sound adapter has disappeared from the USB bus. To
restore sound functionality, the adapter may need to be
unplugged and re-inserted, or perhaps even the hub it has been
connected to may need to be unplugged and re-inserted.  For this
reason, never put the sound adapter on the same hub as the
storage device(s).  (Some kernel versions won't be able
to use certain sound adapters, if they are attached to USB 2
hubs, anyway; they need to be attached to USB 1.1 hubs or directly to
the NSLU2, unless you update your kernel.) It is also advisable
to minimise the number of devices the sound adapter shares
an unpowered hub with, or even connect it directly to the NSLU2,
if you don't have many other USB devices to connect. (You
<em>could</em> connect it to a powered hub, but I am trying to avoid the
use of powered hubs in an attempt to minimise extra power
consumption and reduce the amount of wiring.)
</p>

<p>
In order to avoid missing your reminders due to the sound
adapter having failed, it is advisable to periodically run a
script that ensures the sound device is still present, and alerts
you if it is not. The alert can be by means of a console beep.
(Unfortunately, there does not yet appear to be an NSLU2
equivalent of the PC-speaker kernel patches that allow the
speaker to generate more than a simple beep.) The following
script will do this:
</p>

<pre class="code">
#!/bin/bash
if ! amixer scontents &gt;/dev/null; then
  # sound adapter has somehow gone down
  cd
  if test -e .soundcheck-beeping; then exit; fi
  touch .soundcheck-beeping
  while true; do
    for N in 1 2 3 4 5; do echo $'\a' &gt; /dev/tty1; sleep 1; done
    if amixer scontents &gt;/dev/null; then break; fi # came back
  done
  rm .soundcheck-beeping
fi
</pre>

<p>
The script should be run as root, so that it can access
<tt>/dev/tty1</tt> to make the NSLU2 beep.  It should be run at
various times (perhaps from <tt>crontab</tt>), but take care not
to run it in the middle of the night, unless you want to be
awoken whenever the sound happens to fail. (It may be better to
wait until just before the time the morning alarm would have
happened.)
</p>

<p>
It will likely help to use <tt>amixer</tt> to set the sound
level low, so as to reduce peaks in USB-bus current.  If using
unpowered speakers (which is a good idea because powered
speakers can be more susceptible to picking up annoying noise
from mobile phones, etc., and anyway they take more power),
consider attaching only one speaker, if stereo sound is not
necessary, since this should further reduce the current, and it
also means you can salvage an unpowered speaker from an unused
pair of powered speakers (one of which is usually unpowered),
rather than having to obtain new ones.  Then experiment with
different levels, to find how low you can go whilst still being
able to hear it clearly.  This will vary with the sound adapter
and the speakers.
</p>

<p>
Wireless headphones may need the level set lower still; cheap
FM cordless headphones can easily be overloaded and lose the
signal, if the input sound peaks too loudly. You may find that
the lowest setting of <tt>amixer</tt> (the resolution of which
is limited to that of the sound adapter) is still too high, in
which case you need to ensure that the sound data itself is not
too loud.  This is one of the functions I had to add to
Gradint.
</p>

<h3>Using the Power Button</h3>

<p>
The NSLU2 has only one built-in input device: the power
button.  Thankfully, this can be re-programmed, so, for example,
you can use it to acknowledge an alarm without having to connect
some other input device or connect across the network.  On
Debian at least, the NSLU2 power button sends a "Ctrl-Alt-Delete"
event to <tt>init</tt>, so you can edit <tt>/etc/inittab</tt>
and change the <tt>ctrlaltdel</tt> line to run whatever script
you want.  Since it's rather long-winded to script an automatic
edit of <tt>/etc/inittab</tt> followed by a signal to
<tt>init</tt>, every time one of your scripts wants to change the
function of the power button, it makes sense to point
<tt>inittab</tt> to a shell script somewhere, which you can then
modify at will.  Personally, I use something like the following:
</p>

<pre class="code">
#!/bin/bash
cd ~user   # note: we are root
if test -e .powerbutton.pid; then
  kill $(cat .powerbutton.pid)
elif test -e .powerbutton2.pid; then
  kill $(cat .powerbutton2.pid)
else
  if test -e .about-to-shutdown; then
    reminders.sh "en Shutting down."
    rm -f .powerbutton.pid .about-to-shutdown
    /sbin/halt
  fi
  echo $'\a' &gt; /dev/tty1
  touch .about-to-shutdown
  reminders.sh "en Press again to shut down." &amp;
  (sleep 10 ; rm .about-to-shutdown) &amp;   # the &amp; is important
fi
</pre>

<p>
This looks for a file called <tt>.powerbutton.pid</tt>, which
should, if it exists, contain the process ID of some process
that needs to be terminated when the power button is pressed
(for example, the alarm process).  If <tt>.powerbutton.pid</tt>
does not exist (and there is a check for
<tt>.powerbutton2.pid</tt> also, in case you need to run some
lower-priority reminder sequence at the same time as the
immediate one), then the power button will halt the machine, but,
before it does so, it will prompt the user to press again (within
10 seconds), in order to protect against accidents: if you
pressed the button half a second after the process happened to
terminate by itself, then you probably <em>don't</em> want to
shut down the machine.  The console beep is there so that, if
the sound or speech somehow fails, there is at least some
indication of response.  The line in the script marked "the
&amp; is important" is marked thus because the script needs to
return control to <tt>init</tt>, so that <tt>init</tt> can catch
the repeat press of the power button within the 10-second period;
otherwise, <tt>init</tt> may queue that event until after the
script finishes, when the 10 seconds are up, which will mean it
will not be possible to use the power button to halt the machine.
</p>

<h3>Other Remarks</h3>

<p>
If you want to reduce the amount of light given off by the
Slug (for example because you want to run it in a bedroom which
needs to be dark), you can turn off all the LEDs except the
Ethernet LED and the power button LED, by using the
<tt>leds</tt> command.  For example, you can put this in root's
crontab:
</p>

<pre>
@reboot sleep 5; for N in ready status disk-1 disk-2; do leds $N off; done
</pre>

<p>
The <tt>sleep 5</tt> is to avert a race condition with the
system init scripts, which will otherwise switch the LEDs back
on. You may still have too much light coming from the LEDs on
USB devices (most flash storage devices have bright LEDs that
may flicker during use), so you may have to position these
carefully, and/or point their LEDs downward onto a dark surface.
</p>

<p>
The NSLU2 seems to keep good clock time (better than many
PCs), but you might want to install <tt>ntp</tt> to keep the
clock synchronised.  To save RAM, you can prevent <tt>ntp</tt>
from running as a daemon by adding an <tt>exit</tt> command near
the start of <tt>/etc/init.d/ntp.conf</tt>, and instead run it
from root's crontab using something like<p>


<pre>
37 2 * * * /usr/sbin/ntpd -n -q -g &gt;/dev/null
</pre>

<p>
since updating once a day should easily be accurate enough.
(Note that it should run after 2am, if you want it to pick up
daylight-saving changes.)
</p>

<p>
If you do not have a PDA to connect as an NSLU2 terminal, you
may also like to try using a screenreader with eSpeak in place
of a display.  An NSLU2 with a screenreader and a USB keyboard
could be enough to make a simple workstation for a blind user,
although it does require some setting up.
</p>


</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:141/brownss.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<hr>
<p>
<img align="left" alt="[BIO]" src="../gx/authors/brownss.jpg" class="bio">
</p>

<em>
<p>
Silas Brown is a legally blind computer scientist based in Cambridge UK.
 He has been using heavily-customised versions of Debian Linux since
 1999.
</p>



</em>

<br clear="all">


<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2007, <a href="../authors/brownss.html">Silas Brown</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 141 of Linux Gazette, August 2007
</p>

</div>
</div>


<div class="content lgcontent">

<a name="kapil"></a>
<h1>Who is using your Network?</h1>
<p id="by"><b>By <a href="../authors/kapil.html">Kapil Hari Paranjape</a></b></p>

</b>
</p>

<p>
<h2>1&nbsp;&nbsp;The Evil That Lurks Within</h2>
<p>Securing a local network (LAN) usually means creating firewall rules, host
access rules and proper configuration of Mail, DNS and web servers. All these
methods are primarily based on the assumption that the threat to your network
is from the big bad internet. In this article I will take a reverse point of
view&mdash;that is, users of the local network who are (possibly) the bad guys.</p>
<p>Such an assumption may be justified in the following contexts:</p>
<ul>
<li>The network is large one, like a campus-wide network.
The network administrator has no knowledge of or control over all the different
computers that are connected to the LAN.</li>
<li>A typical wireless network where everyone in your
neighbourhood can use your internet link.</li>
<li>You are in a cut-throat corporate office (perhaps this
applies to some academic departments as well) where you do not really trust
your colleagues&mdash;and all of them wear black-hats.</li>
</ul>
<p>Although I spoke about &ldquo;users&rdquo; above, the <em>real</em> actors
in a computer network are computers. By making the (often false!) assumption
that each computer is doing <em>exactly</em> what its user wants it to do, I
will reduce the question to the following:</p>
<blockquote class="quote">How can computer Abdul that wants to talk to computer
Chin be reasonably confident that computer Betaal is not able to intercept the
conversation and/or impersonate Chin?</blockquote>

<h2>2&nbsp;&nbsp;Not a telephone network</h2>
<p>In order to understand why a slightly sophisticated solution is required, we
need to realise that a LAN is a not like a telephone network and an IP address
is not an identifying label in the same sense that a telephone number is.</p>
<p>The more sophisticated reader will know about &ldquo;hardware&rdquo;
addresses (also known as Ethernet or MAC addresses) which are built into the
hardware unlike IP addresses. However, the description of the network given in
the paragraph below is equally appropriate if you replace &ldquo;IP
address&rdquo; with &ldquo;MAC address&rdquo;.</p>
<p>A typical LAN is a packet based network. Each computer is always
&ldquo;connected&rdquo; to all other computers. Conversations are based on
packets which are sent &ldquo;via the wire&rdquo; and are &ldquo;heard&rdquo;
by all the computers on the network. Each packet is labelled by the recipient's
IP address so that the relevant party can &ldquo;listen to it&rdquo; (in other
words, <em>copy it</em> to its memory). The packet also contains sender's IP
address so that the recipient knows where to send replies.</p>
<p>Computer Betaal (as the ghost in the machine) can collect (copies of)
packets meant for any destination and can also inject packets with any label(s)
desired.</p>
<p>So, Abdul must (digitally) sign every packet sent out and encrypt it so that
only Chin can read it. If Abdul only signed the packets, then Betaal (still)
could collect them. Since there are a <em>lot</em> of packets that make up any
conversation, Betaal could re-send the packets later in a more profitable
order&mdash;thus delivering a blow to Chin. If Abdul only encrypted the packets
then Betaal could inject his own encrypted packets of gobble-de-gook
(illegible/undecipherable data) and disrupt the conversation between Abdul and
Chin.</p>
<p>Let me re-state this in jargon. "In a packet based network, secrecy and
authenticity go hand in hand."</p>

<h2>3&nbsp;&nbsp;Secure Shell</h2>
<p>When Tatu Ylonen originally wrote <code>ssh</code> it was thought of as a
replacement for <code>telnet</code> and <code>rsh</code> which are
programs/protocols for remote logins (for remote shell access). However,
<code>ssh</code> is a <em>network protocol</em>, so it can be used to create
secure conversations between computers.</p>
<p>Each SSH server has a private key&mdash;usually located at
<code>/etc/ssh/ssh_host_rsa_key</code>. Often, there is a second private key in
<code>/etc/ssh/ssh_host_dsa_key</code>. Network administrator's job is to
collect public keys associated to each of these private keys (in the same
place, with a <code>.pub</code> extension) and distribute them to <em>all</em>
computers on the network.</p>
<p>The simplest way to do this is to go to each computer and copy these files
to a USB stick:</p>
<pre>
   cp /etc/ssh/ssh_host_rsa_key.pub /media/usb/&lt;ip_addr&gt;.rsa.pub
   cp /etc/ssh/ssh_host_dsa_key.pub /media/usb/&lt;ip_addr&gt;.dsa.pub
</pre>
<p>Admin then creates a &ldquo;known hosts&rdquo; file:</p>
<pre>
   for type in rsa dsa 
   do
       for i in /media/usb/*.$type.pub
       do
         addr=$(basename $i .$type.pub)
         (echo -n "$addr "; cut -f1-2 -d' '&lt; $i)&gt;&gt; known_hosts
       done
   done
</pre>
<p>This <code>known_hosts</code> file is then copied to
<code>/etc/ssh/ssh_known_hosts</code> on <em>each</em> computer. Finally, we
set configuration parameters</p>
<pre>
   echo "StrictHostKeyChecking yes" &gt;&gt; /etc/ssh/ssh_config
</pre>
<p>on each computer. (Users on each computer may also need to modify the
configuration file <code>$HOME/.ssh/config</code> if it exists and remove/edit
<code>$HOME/.ssh/known_hosts</code> if it exists).</p>
<p>After this (admittedly) long-winded (but not difficult) procedure, Abdul and
Chin have each other's public keys. So, Abdul can encrypt packets which only
Chin can read and Chin can verify signatures made by Abdul. (The actual SSH
protocol is more complex and doesn't concern us here).</p>
<p>So, now, on Abdul one can do <code>ssh Chin</code> and be confident that it
<em>is</em> Chin who is answering. Chin will still ask for the password unless
all servers enable <code>HostBasedAuthentication</code> in
<code>/etc/ssh/sshd_config</code>. This procedure might be risky for Chin,
unless the <code>root</code> user on Abdul is to be considered equivalent to
the <code>root</code> user on Chin.</p>
<p>What about other (than SSH) types of data exchange? Luckily, this too has
been thought of. If Abdul wants to open TCP port (say) 80 on Chin now, then
Abdul runs</p>
<pre>
   ssh -q -f -N -L 8080:localhost:80 Chin
</pre>
<p>Now, opening <code>http://localhost:8080</code> on Abdul gives Chin's web
server.</p>
<p>What about securing <em>all</em> of data exchange? This has been thought of
as well. In fact, SSH provides at least two ways:</p>
<ol class="enumerate" type="1">
<li class="li-enumerate">Applications that can use the SOCKS protocol (like
Mozilla and Thunderbird) can use a tunnel created by this command:
<pre>
         ssh -q -f -N -D 1080 Chin
</pre>
There are also wrapper libraries like <code>tsocks</code> that can
&ldquo;teach&rdquo; any TCP application to use SOCKS.</li>
<li class="li-enumerate">One can also create a TCP <!-- previously,
IP, but I don't think that you can use UDP over SSH tunnels without
non-SSH workarounds - UCP>TCP and TCP>UDP translators -->
tunnel between the hosts:
<pre>
         ssh -q -f -N -w 0:any Chin
</pre>
With some additional network configuration at <em>each</em> end, this tunnel
can be used by all TCP applications, but not by applications that use UDP
instead of UDP for their transport.</li>
</ol>
<p>Despite these efforts, SSH is not always adequate for the problem we set out
to solve for the following reasons:</p>
<ol class="enumerate" type="1">
<li class="li-enumerate">The key distribution mechanism is complex.</li>
<li class="li-enumerate">It is not a great idea to tunnel TCP over TCP as
explained in an <a href=
"http://sites.inka.de/sites/bigred/devel/tcp-tcp.html">article by Olaf
Titz</a>.</li>
<li class="li-enumerate">We need to setup tunnels between each pair of hosts in
the network; and this is a pain.</li>
</ol>
<h2 class="section"><a name="htoc4" id="htoc4">4</a>&nbsp;&nbsp;OpenVPN</h2>
<p>OpenVPN can be thought of as SSH with a solution to all three problems noted
above.</p>
<p>One machine is configured as the <code>openvpn</code> server and all of the
other boxen - as clients. Server passively waits for the client to initiate a
connection. Once the connection is established, roles of the two computers in
the conversation are completely symmetric.</p>
<p>The server (respectively, client) can be configured using the sample
<code>server.conf</code> (<code>client.conf</code> for the client) file that
comes with the openvpn package (sample config files should be located at
<code>/usr/share/doc/openvpn-&lt;version&gt;/sample-config-files</code>, if
you're using a prepackaged version. Otherwise, they're located at
<code>sample-config-files</code> directory in the source tarball). In client's
configuration file one needs to edit only one line starting with
<code>remote</code> and put in the correct server to connect to. In server's
configuration file line that starts with <code>server</code> can to be edited
to put in some random network in form of <code>10.a.b.0</code> (you could also
use <code>172.16-32.a.0</code> or <code>192.168.a.0</code>) instead of the
default. Since we want clients to talk with each other, we also enable the
<code>client-to-client</code> option in server's configuration file. In
addition, we will edit these files to put in appropriate names like
<code>host.key</code> and <code>host.crt</code> for certificate and key files
(see below).</p>
<p>One nice feature of <code>openvpn</code> is that it can use certificates.
This completely simplifies key distribution &mdash; we no longer need to
distribute public key(s) of a new host to all other hosts. This gain comes at
the &ldquo;pain&rdquo; of setting up a Certificate Authority.</p>
<h3 class="subsection"><a name="htoc5" id="htoc5">4.1</a>&nbsp;&nbsp;The Stamp
of Authority</h3>
<p>First, we have to set up a Certificate Authority (CA) on one computer
(network administrator's personal computer, for example). There are a number of
ways to do this.</p>
<p>A simple way to setup a CA is provided with <code>openvpn</code>. We begin
by copying the &ldquo;Easy RSA 2.0&rdquo; directory to a suitable place.</p>
<pre>
   mkdir /root/openvpn_CA
   cd /root/openvpn_CA
   cp -a /usr/share/doc/openvpn/examples/easy-rsa/2.0 .
</pre>
<p>Next, we have to edit last few lines of the <code>vars</code> file in this
directory to reflect our organisation. The relevant lines:</p>
<pre>
   export KEY_SIZE=2048
   export KEY_COUNTRY=
   export KEY_PROVINCE=
   export KEY_CITY=
   export KEY_ORG=
   export KEY_OU=
   export KEY_EMAIL=
</pre>
<p>Then, we generate key of the Certificate Authority.</p>
<pre>
   . ./vars
   ./clean-all
   ./pkitool --initca 
</pre>
<p>Once all queries from the last command have been properly answered, we have
a bunch of files in the <code>keys</code> subdirectory.</p>
<h3 class="subsection"><a name="htoc6" id="htoc6">4.2</a>&nbsp;&nbsp;Give us a
sign</h3>
<p>Having set up the certificate authority, we have to sign keys of each host.
First, each host generates a signing request:</p>
<pre>
   ln -s /etc/ssh/ssh_host_rsa_key.pub /etc/openvpn/host.key
   cd /etc/openvpn
   openssl req -new -extensions v3_req \
         -key host.key -out host.csr
</pre>
<p>All of the queries should be answered carefully. In particular, it is a good
idea to use the fully qualified domain name for the common name (CN) entry.
Then, <code>host.csr</code> is copied to the <code>keys</code> directory, where
the certificate authority was installed with a name like
<code>&lt;hostname&gt;.csr</code>. The CA then verifies and signs the key with
the following commands:</p>
<pre>
   . ./vars
   ./pkitool --interact --sign &lt;hostname&gt;
</pre>
<p>Then, <code>ca.crt</code> and <code>&lt;hostname&gt;.crt</code> files from
the <code>keys</code> directory of CA are copied <em>back</em> to the original
host's <code>/etc/openvpn</code>; we also rename (or symlink)
<code>&lt;hostname&gt;.crt</code> to <code>host.crt</code>.</p>
<h3 class="subsection"><a name="htoc7" id="htoc7">4.3</a>&nbsp;&nbsp;Getting it
all together</h3>
<p>Now, to start the tunnel, we run</p>
<pre>
   /etc/init.d/openvpn start &lt;config&gt;
</pre>
<p>where the <code>&lt;config&gt;</code> is <code>server</code> or
<code>client</code> as appropriate. We can start multiple client versions which
are directed to the same server. Since we want the clients to talk with each
other, we enable the <code>client-to-client</code> option in the server's
configuration.</p>
<p>So, let us say that Octavio is the server and Abdul and Chin are two
clients. When Abdul and Chin have a conversation over <code>openvpn</code>
(which is ensured by Abdul by opening a connection to the
<code>10.a.b.x</code> address assigned to Chin) they can be reasonably
confident that no one&mdash;not even Octavio&mdash;can intercept this
conversation. Since <code>openvpn</code> asks for the certificate at the
start of the conversation, Abdul is also confident that it <em>is</em> Chin
at the other end of the conversation. At the very least Abdul is certain
that this has been certified by the Certificate Authority.</p>
<h3 class="subsection"><a name="htoc8" id=
"htoc8">4.4</a>&nbsp;&nbsp;Problems</h3>
<p>Have Abdul and Chin solved their problem? Can they communicate without
worrying about Betaal?</p>
<p>Some problems remain:</p>
<ol class="enumerate" type="1">
<li class="li-enumerate">All the parties depend on Octavio who is at the center
of the (virtual) network. This may be appropriate for some situations, like a
wireless network, where the natural shape (topology) of the network is
star-like, but is inappropriate in a campus-wide network.</li>
<li class="li-enumerate">Though routing is automated, it is still complex. Each
pair of machines is connected by the underlying local network as well as the
virtual network. To make the conversations secure, we must ensure that all data
is exchanged over the virtual network.</li>
</ol>
<h2 class="section"><a name="htoc9" id="htoc9">5</a>&nbsp;&nbsp;Repeat</h2>
<p>The solution to these two problems has essentially been described by Rene
Pfeiffer in his <a href="http://linuxgazette.net/125/pfeiffer.html">article
I</a> and <a href="http://linuxgazette.net/126/pfeiffer.html">article II</a> on
IPsec. We will vary from his prescription in two respects:</p>
<ol class="enumerate" type="1">
<li class="li-enumerate">Use the &ldquo;transport&rdquo; mode for IPsec rather
than the &ldquo;tunnel&rdquo; mode.</li>
<li class="li-enumerate">Use the &ldquo;use&rdquo; policy for IPsec rather than
the &ldquo;require&rdquo; policy.</li>
</ol>
<p>The first ensures that routing is &ldquo;automatic&rdquo;. The second allows
us to migrate to an IPsec network without disrupting existing connections. Once
all machines that need to speak securely to each other are configured we can
switch to the &ldquo;require&rdquo; mode to ensure that all conversations are
encrypted.</p>
<p>One difference between IPsec and <tt>openvpn</tt> is, that, in IPsec, a
separate daemon handles the key exchange and authentication. In GNU/Linux, this
is <tt>racoon</tt>. We configure the <code>/etc/racoon/racoon.conf</code> file
as follows. First of all, we put in the path to the certificates. This can be
the same as certificates generated for OpenVPN. Next we configure
authentication.</p>
<pre>
   remote anonymous {
      exchange_mode main;
      certificate_type x509 "$HOST_CERT" "$HOST_KEY";
      verify_cert on;
      my_identifier asn1dn;
      proposal {
            encryption_algorithm aes;
            hash_algorithm sha1;
            authentication_method rsasig;
            dh_group modp1024;
      }
   }
</pre>
<p>Here we have to replace <code>$HOST_CERT</code> and <code>$HOST_KEY</code>
with certificate and key locations, respectively. The next section in the
configuration file describes the encryption used after successful
authentication.</p>
<pre>
   sainfo anonymous {
        pfs_group modp768;
        encryption_algorithm 3des;
        authentication_algorithm hmac_md5;
        compression_algorithm deflate;
   }
</pre>
<p>Next, we instruct the kernel to use IPsec whenever possible. To do this, we
ensure that the following directives are loaded by command <code>setkey</code>.
Flush the security associations and security policies.</p>
<pre>
   flush;
   spdflush;
</pre>
<p>The policy is to use the ESP protocol and AH protocol for <em>all</em>
packets between this host and any other host on the network <em>if
possible</em>. In commands below, one needs to put in the correct
<code>$IP_ADDR</code> and <code>$NETMASK</code> values</p>
<pre>
   spdadd $IP_ADDR $NETMASK any -P out ipsec
      esp/transport//use
      ah/transport//use;

   spdadd $NETMASK $IP_ADDR any -P in ipsec
      esp/transport//use
      ah/transport//use;
</pre>
<p>This means that all hosts will use encrypted and authenticated traffic for
every <!-- reciprocating --> host in the LAN which supports encrypted traffic.
This allows one to enable this configuration on all hosts in LAN <em>one host
at the time</em> without disrupting the existing network in the process. Once
all hosts are configured for IPsec, this can be replaced with</p>
<pre>
   spdadd $IP_ADDR $NETMASK any -P out ipsec
      esp/transport//require
      ah/transport//require;

   spdadd $NETMASK $IP_ADDR any -P in ipsec
      esp/transport//require
      ah/transport//require;
</pre>
<h2>6&nbsp;&nbsp;Conclusion and Acknowledgements</h2>
<p>Now, it is relatively easy to configure machines to use encryption and
authentication in a local area network. Today, computers and networks are fast
enough, so extra calculations and extra network packets, that are required for
this, do not cause noticeable delays. Also, it is quite easy to implement such
a solution without bringing down the entire network until all machines are
reconfigured.</p>
<p>So! What are you waiting for? Choose a solution that is appropriate for your
use and put it into use!</p>
<p>In the IMSc network we have tested and implemented the <code>openvpn</code>
solution. A number of my colleagues here helped debug various aspects of this.
I thank them all for their help. The documentation of <code>ssh</code> and
<code>openvpn</code> is also very good. There is a number of great articles on
IPsec including those from LG that have been mentioned above. I thank the
authors of these documents for their assistance.</p>
<hr size="2">
<blockquote class="quote"><em>This document was translated from
L<sup>A</sup>T<sub>E</sub>X by</em> <a href=
"http://hevea.inria.fr/index.html"><em>H</em><em><font size=
"2"><sup>E</sup></font></em><em>V</em><em><font size=
"2"><sup>E</sup></font></em><em>A</em></a><em>.</em></blockquote>


</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:141/kapil.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<!-- *** BEGIN bio *** -->
<hr>
<p>
<img align="left" alt="Bio picture" src="../gx/authors/kapil.jpg" class="bio">
<em>

Kapil Hari Paranjape has been a ``hack''-er since his punch-card days.
Specifically, this means that he has never written a ``real'' program.
He has merely tinkered with programs written by others. After playing
with Minix in 1990-91 he thought of writing his first program---a
``genuine'' *nix kernel for the x86 class of machines. Luckily for him a
certain L. Torvalds got there first---thereby saving him the trouble
(once again) of actually writing code. In eternal gratitude he has spent
a lot of time tinkering with and promoting Linux and GNU since those
days---much to the dismay of many around him who think he should
concentrate on mathematical research---which is his paying job. The
interplay between actual running programs, what can be computed in
principle and what can be shown to exist continues to fascinate him.

</em>
<br clear="all">
<!-- *** END bio *** -->

<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2007, <a href="../authors/kapil.html">Kapil Hari Paranjape</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 141 of Linux Gazette, August 2007
</p>

</div>
</div>


<div class="content lgcontent">

<a name="lazar"></a>
<h1>Serving Your Home Network on a Silver Platter with Ubuntu</h1>
<p id="by"><b>By <a href="../authors/lazar.html">Shane Lazar</a></b></p>

</b>
</p>

<p>
<p>Linux has always been a good choice for a server OS. In practical
terms, however, this functionality has been out of reach for the
everyday computer user, mainly due to the technical know-how required to
manage a dedicated server OS. On the other hand, our homes today are
more filled with computers than ever before - and, in a multi-node
network, a server can provide many benefits. In this article, I am going
to try to guide you in setting up a useful server for your home
network, one that is headless (i.e., without monitor, keyboard, or
mouse) and can be stowed away neatly out of view.</p>

<p>This setup will be ideal for:</p>

<ul>
   <li>securely sharing a single Internet connection to multiple computers,</li>
   <li>streamlining of Internet traffic,</li>
   <li>providing a central file server in the home network,</li>
   <li>preventing bandwidth-hogging by P2P software,</li>
   <li>allowing easy remote administration of the server.</li>
</ul>

<p>Hardware required:</p>

<ul>
   <li>Any old computer with commonly available components (thus ensuring
driver availability for it). My own server is a Pentium III 800MHz with
256MB RAM, Intel chipset, an on-board graphic card, 2 Ethernet cards, 40GB
hard drive (the larger the better, obviously), and a CD-ROM. This is really
overkill - you can use a Pentium I with 64MB RAM (otherwise landfill
material), and it would run moderately well. The two network cards are
required. You can use a wireless card for your Local Area Network (LAN),
but make sure it has Linux drivers available.</li>
   <li>An Ethernet hub with at least as many ports as there are computers you
wish to connect (including your server). If you use a wireless LAN, you
will not need this.<li>
   <li>As much "straight through" Ethernet cable as you will need (not
required in a wireless setup).</li>
</ul>

<p>The basic setup:</p>

<p><tt>Internet &lt;--&gt; Ubuntu Server &lt;--&gt; Ethernet Hub
&lt;--&gt; LAN Machines</tt><p>

<p>Services we are going to be running on our system:</p>

<ul>
   <li><a href="http://webmin.com/">Webmin</a> for remote Web-based administration,</li>
   <li><a href="http://www.shorewall.net/">Shorewall firewall</a> setup up for Internet connection sharing or network address translation (NAT),</li>
   <li><a href="http://www.squid-cache.org/">Squid Proxy server</a> for caching of Internet content,</li>
   <li><a href="http://www.isc.org/index.pl?/sw/bind/">BIND DNS server</a> working hand-in-hand with Squid,</li>
   <li><a href="http://us1.samba.org/samba/">Samba file server</a>,</li>
   <li><a href="http://en.wikipedia.org/wiki/LAMP_(software_bundle)">LAMP Web server</a>, which we are going to use to run <a href="http://www.torrentflux.com/">TorrentFlux</a>, a Web-based Torrent client. You can use LAMP to serve your own Web sites, if you so wish.</li>
</ul>

<p>Let us dive right in!</p>

<h3>1. Getting Ubuntu</h3>

<p>Download the Ubuntu (currently at version 7.04, "Feisty Fawn") Server
CD image from 
<a href="http://www.ubuntu.com/getubuntu/download">Ubuntu's download
page</a>.</p>

<h3>2. Making the installation CD</h3>

<p>Burn the <tt>ubuntu-7.04-server-i386.iso</tt> image to a CD using
your favorite image-burning program. Remember, <em>burn the image</em>; do not
extract the files from the image file. If you are going to be using an
old CD-ROM, burn the CD at the slowest possible speed, for
reliability.</p>

<h3>3. Installing Ubuntu on Your Server</h3>

<p>Ubuntu is well known for having an easy installation process. For
now, plug in a monitor, keyboard, and the network cables (Internet and
LAN, both), put in the Ubuntu server CD, and boot up! You may need to
change your BIOS settings to allow booting from CD.</p>

<ol>
   <li>Select the hard disk installation, choose your desired language,
then pick your country and keyboard.</li>
   <li>Configure the network interface connected to the Internet, using one of
3 options: autoconfiguration, autoconfiguration with DHCP
(automatically assigned IP addresses), or manual. Which one you choose
really depends on your Internet connection; ask your ISP, if in doubt. If
you have to configure manually, configure your Internet connection on the eth0
network card, for simplicity's sake.</li>
   <li>For partitioning, I recommend "Guided - use entire disk", as it is a
no-brainer, and accept the settings, thereby writing changes to disk.</li>
   <li>Allow the system clock to be set to UTC.</li>
   <li>Create the system administrator's user account; enter the full user
name, account name, and administrator's password (which has to be
verified).</li>
   <li>Ubuntu will continue to install the base system.</li>
   <li>Enter your ISP's proxy server settings, if required.</li>
   <li>When you are asked to choose the software to install, select both DNS and LAMP server. You do this using the spacebar to check the boxes, cursor keys, and <tt>TAB</tt>, to navigate through the menu.</li>
   <li>Complete the installation, reboot, and you will be presented with a command-line interface (CLI) prompting you to log in. Use the administrator account name and password to do so.</li>
</ol>

<p>Before we continue, I did mention that I would try to make this as
simple as possible, and now you are probably wondering what you are doing
in a CLI. This is necessary, as we want our server to run as lean as
possible. After all, it is going to be stowed away in a closet, so who
needs a fancy GUI? I promise we won't be spending much longer on the
CLI. A couple of tips for new users:

<ul>
   <li>the cursor keys let you scroll through previous commands you entered,</li>
   <li>the <tt>TAB</tt> key is a God-send for its auto-complete function. Type a couple of keys, hit the <tt>TAB</tt> key, and it will auto-complete or show you the valid commands or paths!</li>
</ul>

<h3>4. Checking Internet connectivity</h3>

<p>First thing we will do on our new system is to check if we are
connected to the Internet. Do this simply by pinging Google.</p>

<tt>ping www.google.com</tt>

<p>Stop the pinging with <tt>Ctrl+C</tt>. If all went well, you should be
getting responses to your pings. If not, try switching the LAN and
Internet cables around. Most probably, you will get a ping response by
now. Keep in mind which card your Internet is configured on,
<tt>eth0</tt> or <tt>eth1</tt>, and modify the instructions accordingly.
In this guide, the Internet is on <tt>eth0</tt> and the LAN is on
<tt>eth1</tt>.

<h3>5. LAN network configuration</h3>

<p>Now, we will configure our LAN network card. We will do this using
<tt>vim</tt>, a CLI text editor.</p>

<p>Four simple commands you will use in <tt>vim</tt> are:</p>

<ul>
   <li>the <tt>I</tt> key, which will put you in Insert mode so you can edit the text file as in any other text editor,</li>
   <li>the <tt>Esc</tt> key, which exits you out of the Insert mode,</li>
   <li><tt>:w!</tt>, which saves/writes the file to disk,</li>
   <li><tt>:x</tt>, which exits the <tt>vim</tt> text editor.</li>
</ul>

<p>Let us open our network configuration file with administrative
privileges:</p>

<tt>sudo vim /etc/network/interfaces</tt>

<p>You will be asked to enter the administrator's password. Navigate
with the cursor key and add the following at the end of this file:</p>

<pre><tt>auto eth1
iface eth1 inet static
	address 192.168.0.1
	netmask 255.255.255.0
	broadcast 192.168.0.255</tt></pre>

<p>If you need to change the configuration of your Internet connection,
you should do this now in the <tt>eth0</tt> section. Restart your
network interfaces using:</p>

<tt>sudo /etc/init.d/networking restart</tt>

<h3>6. Update Ubuntu</h3>
<p>Install any available updates by:</p>

<tt>sudo apt-get update</tt>

<p>and then</p>

<tt>sudo apt-get upgrade</tt>

<h3>7. Installing Webmin</h3>

<p>Now we will install the packages required for Webmin, the Web-based
administration tool:</p>

<tt>sudo apt-get install libnet-ssleay-perl openssl libauthen-pam-perl
libio-pty-perl libmd5-perl</tt>

<p>Download Webmin:</p>

<tt>wget http://prdownloads.sourceforge.net/webadmin/webmin_1.350_all.deb</tt>

<p>If this does not work, there is probably a newer version of Webmin.
Get the link to the latest <tt>*.deb</tt> file from the <a
href="http://webmin.com/download.html">Webmin site</a>. 

<p>Install it:</p>

<tt>sudo dpkg -i webmin_1.350_all.deb</tt>

<p>You will get the following output:</p>

<p>Webmin install complete. You can now login to
https://your-server-name:10000/ as root with your root password, or as
any user who can use sudo to run commands as root.</p>

<p>And that's it! We are done with the CLI. Log out:</p>

<tt>exit</tt>

<p>Now, you can disconnect the monitor and keyboard, stow your server
away, and continue from your desktop machine on a beautiful Web-GUI!</p>

<h3>8. Configure your LAN machines</h3>

<p>However, before you do that, you will have to configure your desktop
machine's network card. Set it up as follows:</p>

<pre><tt>IP address: 192.168.0.2
Subnet mask: 255.255.255.0
Gateway: 192.168.0.1
DNS server: 192.168.0.1</tt></pre>

<p>Your other machines would have incrementing IP addresses, e.g., <tt>192.168.0.3</tt>, <tt>192.169.0.4</tt>,...</p>

<h3>9. Upgrade Webmin</h3>

<p>Open your favorite Web browser and navigate to
<tt>https://192.168.0.1:10000</tt>. Enter the administrator's user name
and password. Welcome to the powerful Webmin!</p>

<p>On the tree menu on the left, go to <tt>Webmin &gt; Webmin
Configuration</tt>. Click <em>Upgrade Webmin</em>, and, with <em>"Latest
version from www.webmin.com"</em> selected, click the <em>Upgrade</em>
button. If there is an upgrade available, it will be installed for
you.</p>

<h3>10. Shorewall Firewall</h3>

<p>To install the Shorewall firewall, go to <tt>System &gt; Software
Packages</tt> and in the <em>"Install a New Package"</em> section,
select <em>"Package from APT"</em>, enter <tt>shorewall</tt>, and click
<em>Install</em>. This may take some time, depending on your Internet
connection, but Shorewall will be installed.</p>

<p>Now, go to <tt>Networking &gt; Shorewall Firewall</tt>, and we'll begin
setting up your firewall. Do <em>not</em> start the firewall yet, or you might
lock yourself out of the server. We will configure Shorewall section by
section.</p>

<p><strong>Network Zones:</strong> This section defines zones to which
we will assign "levels of trust". We will create three zones: the
firewall, Internet, and local zones.</p>

<p>Click <em>Add a new network zone</em>. You will be provided with a
number of options. We are interested in the <em>Zone ID</em> field and
the <em>Zone type</em> list. For each zone, enter the options as follows,
and click <tt>Create</tt> before returning to the page to create the
next.</p>

<ul>
   <li><em>Zone ID</em> = <tt>fwall</tt>; <em>Zone type</em> = <tt>Firewall system</tt></li>
   <li><em>Zone ID</em> = <tt>net</tt>; <em>Zone type</em> = <tt>IPv4</tt></li>
   <li><em>Zone ID</em> = <tt>loc</tt>; <em>Zone type</em> = <tt>IPv4</tt></li>
</ul>

<p><strong>Network Interfaces:</strong> This section tells the firewall
which Ethernet card is connected to the Internet, and which one to the
LAN. In our case, we have only two interfaces.</p>

<p>Click <em>Add a new network interface</em>, and again you will be
presented with a vast array of options. We will define only 
<em>Interface</em>, <em>Zone name</em>, and <em>Broadcast address</em>.
Here, also, you will have to setup one interface at a time, clicking
<em>Create</em> before returning to configure the next. Configure as
follows:</p>

<ul>
   <li><em>Interface</em> = <tt>eth0</tt>; <em>Zone name</em> = <tt>net</tt>; <em>Broadcast address</em> = <tt>Automatic</tt></li>
   <li><em>Interface</em> = <tt>eth1</tt>; <em>Zone name</em> = <tt>loc</tt>; <em>Broadcast address</em> = <tt>Automatic</tt></li>
</ul>

<p><strong>Default Policies:</strong> The default policies tell the
firewall what to do with packets coming from various sources. We will
set it to drop all requests from the Internet, and accept all from the
LAN and the firewall itself. Click <em>Add a new default policy</em>. As
before, we will define one policy at a time, clicking <tt>Create</tt>
before proceeding. Configure the policies as follows:</p>

<ul>
   <li><em>Source zone</em> = <tt>net</tt>; <em>Destination zone</em> = <tt>Any</tt>; <em>Policy</em> = <tt>DROP</tt></li>
   <li><em>Source zone</em> = <tt>fwall</tt>; <em>Destination zone</em> = <tt>Any</tt>; <em>Policy</em> = <tt>ACCEPT</tt></li>
   <li><em>Source zone</em> = <tt>loc</tt>; <em>Destination zone</em> = <tt>Any</tt>; <em>Policy</em> = <tt>ACCEPT</tt></li>
</ul>

<p><strong>Firewall Rules:</strong> This section defines specific rules
for specific services. We will enable them as the need arises, later.</p>

<p><strong>TOS:</strong> This section optimizes Web browsing as much as
you can on your end. Click <em>Add a new type of service</em>, and we
will proceed to configure the services one by one.</p>

<ul>
    <li><em>Source zone</em> = <tt>Any</tt>; <em>Destination zone</em> = <tt>Any</tt>; <em>Protocol</em> = <tt>TCP</tt>, <em>Source ports</em> = <tt>Any</tt>; <em>Destination ports</em> = with the <em>Ports or ranges</em> radio button selected enter <tt>www</tt>; <em>Type of service</em> = <tt>Maximize-Throughput</tt></li>
   <li><em>Source zone</em> = <tt>Any</tt>; <em>Destination zone</em> = <tt>Any</tt>; <em>Protocol</em> = <tt>TCP</tt>, <em>Source ports</em> = <tt>Any</tt>; <em>Destination ports</em> = with the <em>Ports or ranges</em> radio button selected enter <tt>www</tt>; <em>Type of service</em> = <tt>Minimize-Delay</tt></li>
   <li><em>Source zone</em> = <tt>Any</tt>; <em>Destination zone</em> = <tt>Any</tt>; <em>Protocol</em> = <tt>TCP</tt>, <em>Source ports</em> = with the <em>Ports or ranges</em> radio button selected enter <tt>www</tt>; <em>Destination ports</em> = <tt>Any</tt>; <em>Type of service</em> = <tt>Maximize-Throughput</tt></li>
   <li><em>Source zone</em> = <tt>Any</tt>; <em>Destination zone</em> = <tt>Any</tt>; <em>Protocol</em> = <tt>TCP</tt>, <em>Source ports</em> = with the <em>Ports or ranges</em> radio button selected enter <tt>www</tt>; <em>Destination ports</em> = <tt>Any</tt>; <em>Type of service</em> = <tt>Minimize-Delay</tt></li>
</ul>

<p><strong>Masquerading:</strong> This tells the server to forward
requests from the LAN to the Internet, which is required for Internet
connection-sharing. Click <em>Add a new masquerading rule</em>, and enter
the following rule.</p>

<ul>
<li><em>Outgoing interface</em> = <tt>eth0</tt>; <em>Network to masquerade</em> = with <em>Subnet on interface</em> selected, choose <tt>eth1</tt>; leave the rest unchanged</li>
</ul>

<p><strong>When Stopped:</strong> This allows machines whose IP
addresses are specified to access the server even when the firewall is
not running. No other IP addresses will have access. Add as many as you
want, but there should be at least one, just in case. In the example
below, I have allowed access from two IP addresses on the LAN. Click
<em>Add a new stopped address</em>, and configure as follows:</p>

<ul>
<li><em>Interface</em> = <tt>eth1</tt>; select <em>Listed addresses and networks</em>, and enter <tt>192.168.0.2,192.168.0.3</tt> and/or any other addresses you wish.</li>
</ul>

<p>We don't need to add any other settings.</p>

<p>Back on the Shorewall main page, click <em>"Check Firewall"</em>. You
should get the thumbs up. Note that an "OK" result here does <em>not</em>
guarantee the firewall will work properly, or will work at all. It
simply checks the rules syntax.</p>

<p>There is a security feature that prevents an unconfigured Shorewall
from being started up, when booting. This has to be changed manually. For
this, you will need a Java-enabled Web browser to do it using Webmin, or
you could resort to using vim from the CLI.</p>

<p>What you have to do is change the line</p>

<tt>startup=0</tt>

<p>in the file <tt>/etc/default/shorewall</tt> to</p>

<tt>startup=1</tt>

<p>In Webmin, go to <em>Others &gt; File Manager</em>. This will give you a
nice Java-based file manager. Navigate to the above mentioned file, and
click the <em>"Edit"</em> button at the top. A text editor window will
pop up. (Disable pop-up blocker.) Make the change, and then save and 
close.</p>

<p>Again, using this browser, browse to the file
<tt>/etc/shorewall/shorewall.conf</tt>, click <em>"Edit"</em>, and find
the line  <tt>IP_FORWARDING=Keep</tt>. Change the value from
<tt>Keep</tt> to <tt>On</tt>. Save and close.</p>

<p>Now, let us make sure that Shorewall is set to start at bootup. Go to
<em>System &gt; Bootup and Shutdown</em>, look for <tt>shorewall</tt> in the
list. Tick the checkbox, and click <em>"Start Now and On Boot"</em> at
the bottom. Go back to the <em>Networking &gt; Shorewall Firewall</em> page,
and you should see six buttons where there were previously only two. Click
<em>"Show Status"</em>, to verify that all is running well. Your Internet
connection sharing should be set up, now. Try it out!</p>

<h3>11. BIND DNS Server</h3>

<p>Ubuntu server pretty much does all the configurations necessary for a
working BIND DNS server. There is, however, one thing we can do to make
the lookups faster. We can tell our server to forward unknown requests
to your ISP's DNS server.<a href="#1"><strong>[1]</strong></a></p>

<p>Go to <em>Servers &gt; BIND DNS Server</em>, click on <em>"Forwarding and
Transfers"</em>, and, in the fields marked <em>"Server to forward queries
to"</em>, enter the IP addresses of your ISP's DNS servers. Save, and
click <em>"Apply Changes"</em> in the main BIND DNS server page.</p>

<h3>12. Squid Proxy Server</h3>

<p>Now, we will move on to installing and setting up Squid as your <a
href="http://en.wikipedia.org/wiki/Proxy_server#Caching_proxy_server">caching
proxy server</a>. Go to <em>Servers &gt;Squid Proxy Server</em>. Webmin will
inform you that Squid is not installed on your system, and provide you
with an option to install it using APT. Click on the link (labelled
<em>"Click here"</em>) provided, to install Squid. Webmin will keep you
informed of the progress and, once completed, will give you some
information on the installed packages.</p>

<p>Go back to the main page for Squid, and now you should have a host of
configuration tools available. I will not explain all the options
available, but, if you require more clarification, help is available at
the top left of the tool's page. (You will have to disable your browser's
popup blocker.)</p>

<p><strong>Ports and Networking:</strong> Here we will tell Squid which
port it will be listening on. The default is port 3128. We will stick to
this, but you can change it.  In the <em>"Options for port"</em> field,
enter <tt>transparent</tt>. This will make Squid a transparent proxy
server, which eliminates the need to configure machines on your LAN. Save
the changes.</p>

<p><strong>Memory Usage:</strong> Here, you can define memory usage limits
for Squid, or choose to go with the default settings. I would draw
attention to the <em>"Maximum cached object size"</em> option. Here, you can
define the maximum size of cached files.</p>

<p><strong>Cache Options:</strong> The option I would recommend you
changing here is the <em>"Cache Directories"</em> one. Squid defaults to a
100MB cache, which is pretty minuscule for our caching proxy objective.
Decide how much of your hard disk you wish to use for the cache; I
use 5GB out of my 40GB hard disk. In the <em>"Directory"</em> field,
enter <tt>/var/spool/squid</tt>, <em>"Type"</em> as <tt>UFS</tt>, in
<em>"Size (MB)"</em>, enter however much you decided on in megabytes, for
the 1st- and 2nd-level directories, enter one of the following numbers;
<tt>16,32,64,128</tt> or <tt>256</tt> (defaults being <tt>16</tt> and
<tt>256</tt>, respectively). These numbers basically define the file
structure of your cache. Read the help documentation, for more
information on this and other options. Save your changes.</p>

<p><strong>Helper Programs:</strong> In the <em>"DNS server
addresses"</em> field, enter <tt>192.168.0.1</tt>, select the radio
button, and save. This tells Squid to send DNS requests to the BIND DNS
server running on your server.</p>

<p><strong>Access Control:</strong> Here, we will define which LAN
machines will be able to use Squid, by their IP addresses. At the bottom
of the <em>"Access Control Lists"</em> section, select <tt>Client
Address</tt> from the drop down list, and click <em>"Create new
ACL"</em>. In the page that appears, enter a name of your choice in the
<em>"ACL Name"</em> field (e.g., <tt>Local_Network</tt>), define the
range of IP addresses you wish to grant access to, and the Netmask, e.g.,
<em>From</em> = <tt>192.168.0.2</tt>, <em>To</em> =
<tt>192.168.0.7</tt>, <em>Netmask</em> = <tt>255.255.255.0</tt>. If you
would like to grant access to all machines on your LAN, enter as
follows; <em>From</em> = <tt>192.168.0.0</tt>, <em>To</em> = *leave
blank*, <em>Netmask</em> = <tt>255.255.255.0</tt>. Save your
changes.</p>

<p>Having defined the machines on our LAN, we will now tell Squid what to
do with requests from these machines. Click <em>"Add proxy
restriction"</em> in the <em>"Proxy Restrictions"</em> section. Select
the <em>"Allow"</em> action, and the ACL you just created
(<tt>Local_Network</tt>) from the <em>"Match ACLs"</em> list. Save your
changes.</p>

<p>Your new restriction will be at the bottom of the restrictions list,
and, since they are effectuated in order, you will have to move your new
rule up the list to third place. Do this using the <em>"Move"</em>
arrows, to the right of the defined restrictions.</p>

<p>For security reasons, we will create a new user named <tt>squid</tt>
who will run squid. Go to <em>System>Users and Groups</em>. Click
<em>"Create a new user"</em>, and enter the following;</p>

<ul>
   <li><em>Username</em> = <tt>squid</tt>,</li>
   <li><em>Real name</em> = <tt>squid</tt>,</li>
   <li><em>Password</em> = <tt>No login allowed</tt>,</li>
   <li><em>Primary group</em> = <tt>New group with same name as user</tt>,</li>
   <li><em>Create home directory</em> = <tt>No</tt>,</li>
   <li><em>Copy files to home directory</em> = <tt>No</tt>,</li>
   <li><em>Create user in other modules</em> = <tt>Yes</tt>,</li>
</ul>

<p>Leave the rest unchanged. Click <em>"Create"</em>.</p>

<p>Now, we will grant permissions to the user <tt>squid</tt> to write to
our cache. Go to <em>Others &gt; Command Shell</em>, and execute the
following command:</p>

<tt>chown -R squid:squid /var/spool/squid/</tt>

<p>Return to the Squid Proxy Server page.</p>

<p><strong>Administrative Options:</strong> In the <em>"Run as Unix
user"</em> field, click the browse button, and select <tt>squid</tt> from
the list of users. In the <em>"Visible hostname"</em> field, enter the
name of your server. This you can find out from the <em>"System
Information"</em> page in Webmin, as <em>"System hostname"</em>. Save the
changes.</p>

<p>Click <em>"Initialize Cache"</em>. Once this terminates successfully,
return to the Squid main page and click <em>"Start Squid"</em>.

Since we are making a transparent proxy server, we need to add some
rules in the firewall, to redirect requests to pass through Squid. Go to
<em>Networking>Shorewall Firewall>Firewall Rules>Manually Edit
File</em>, and paste the following rule:</p>

<pre><tt>#squid transparent proxy redirect
REDIRECT	loc	3128	tcp	www</tt></pre>

<p>If you changed the port Squid listens to, earlier on, use that
port in this rule, instead of 3128. Save the changes, and <em>Apply
Configuration</em>.</p>

<p>Test if your desktop machines have access to the Internet. The
difference between a simple Internet connection sharing and using a
caching proxy is that frequently visited Web sites will load faster, as
some content is stored on your server.</p>

<h3>13. Samba file sharing</h3>

<p>Now, we'll move on to installing and setting up Samba for file sharing
to both Linux and Windows machines. Go to <em>Servers &gt; Samba Windows File
Sharing</em>. As was the case with Squid, Webmin detects that Samba is
not installed, and provides an easy link to install it using APT. Go
ahead and click the link, to download and install Samba. Once this is
done, we will now configure file sharing.</p>

<p>Since we are sharing on a trusted network, we will setup our file
server with read and write permissions for everybody.</p>

<p>Return to <em>Servers &gt; Samba Windows File Sharing</em>, and, in the
first section, click <em>"Create a new file share"</em>, then complete as
follows:</p>

<ul>
   <li><em>Share name</em> = enter whatever you would like to identify the share as (I am using <tt>public</tt>),</li>
   <li><em>Directory to share</em> = <tt>/home/public</tt>,</li>
   <li><em>Automatically create directory</em> = <tt>Yes</tt>,</li>
   <li><em>Create with owner</em> = <tt>root</tt>,</li>
   <li><em>Available</em> = <tt>Yes</tt>,</li>
   <li><em>Browseable</em> = <tt>Yes</tt>,</li>
   <li><em>Share Comment</em> can be whatever you wish.</li>
</ul>

<p>This will create the share <tt>public</tt>, with Read-only permissions
for all. Using <em>Others &gt; File Manager</em>, navigate to <tt>/home</tt>,
select the folder <tt>public</tt>, and click <em>Info</em>. In the info
window that opens, in the <em>Permissions</em> section, select all the
checkboxes for <em>User</em>, <em>Group</em>, and <em>Other</em>, thereby
giving permission to everybody to read and write to this folder.</p>

<p>Now, navigate to <tt>/etc/samba</tt>, select <tt>smb.conf</tt>, and
click <em>Edit</em>. Look for the line</p>

<tt>; security = user</tt>

<p>and change it to</p>

<tt>; security = share</tt>

<p>Scroll down to the end of the file, to find the section which
describes the share we just created, and edit it to it look like
this:</p>

<pre><tt>[public]
	comment = public
	path = /home/public
	public = yes
	writable = yes
	create mask = 0777
	directory mask = 0777
	force user = nobody
	force group = nogroup</tt></pre>

<p>Save and close. If you need to change your Workgroup, do that from
the <em>Windows Networking</em> tool in the <em>Global
Configuration</em> section on the <em>Samba Windows File Sharing</em>
page. Samba's default workgroup is, ironically, <tt>MSHOME</tt>. Click
<em>Restart Samba Server</em>, and verify that you have access to the
shared folder with read and write permission from your desktop machine,
by creating and deleting a file in the share. The only settings you will
have to enter on your LAN machine to gain access are:</p>

<ul>
   <li>the correct workgroup (Samba defaults to MSHOME),</li>
   <li>the server's address which is <tt>192.168.0.1</tt> in our setup,</li>
   <li>the name of the share which is <tt>public</tt> in our setup,</li>
   <li><b>no</b> username or password is required.</li>
</ul>


<h3>14. TorrentFlux</h3>

<p>For those of us who use Bittorrent for peer-to-peer file sharing, we
will install TorrentFlux, which is a Web-based Bittorrent client. Some of
the advantages of using TorrentFlux include;</p>

<ul>
  <li>running all Torrents on a single machine, so your workstations do not bear that load,</li>
  <li>other machines need not be left running solely for the Torrent connections,</li>
  <li>automatically sharing the downloaded files across the LAN,</li>
  <li>limiting of bandwidth usage of Torrent downloads,</li>
  <li>queueing of Torrent connections.</li>
</ul>

<p>In your Web browser, go to the <a
href="http://www.torrentflux.com">TorrentFlux Web site</a>, and download
the latest version of TorrentFlux. In Webmin, go to <em>Others &gt; Upload and
Download</em>. In the <em>"Upload files to server"</em> section, browse
to the <tt>torrentflux_2.x.tar.gz</tt> file you just downloaded in the
<em>"Files to upload"</em> field. In the field <em>"File or directory to
upload to"</em>, enter <tt>/var/www</tt>. Select in the <em>Extract ZIP
or TAR files</em> options the <tt>Yes, then delete</tt> radio button.
Click <em>"Upload"</em> to upload, and unpack TorrentFlux.</p>

<p>Using <em>Other>File Manager</em>, browse to the
<tt>/var/www/torrentflux_2.x</tt> directory, and double-click the
<tt>INSTALL</tt> file, to open it in your browser. Read the instructions
carefully.</p>

<p>First, and very important, we will set the root password for our
MySQL database. Note that this root user is different from the system
root user. The same applies to all MySQL users.</p>

<p>Go to <em>Servers &gt; MySQL Database Server</em>, and click <em>User
Permissions</em> from the <em>Global Options</em> section. From the list
of users, click on any of the instances of root. In the password field,
select <em>Set to..</em>, and enter a password for the MySQL root user.
You may be asked to log in, after setting the password. Repeat for all
the other instances, with the same password.</p>

<p>TorrentFlux uses MySQL for its database features. So, let us go ahead
and create a database for TorrentFlux. On the main MySQL page, click
<em>Create a new database</em>. In the <em>"Database name"</em> field,
enter <tt>torrentflux</tt> and don't make any other changes. Click
<em>Create</em>.</p>

<p>To create the required tables, click on the torrentflux database we
just created, then click the <em>"Execute SQL"</em> button. In the
second section, which says <em>"Select an SQL commands file to execute on
database"</em>, select <em>"From local file"</em>, and browse to the file
<tt>/var/www/torrentflux_2.x/sql/mysql_torrentflux.sql</tt>, click
<em>Ok</em>, and then <em>Execute</em>. Now, if you return to the table
list, you will see that some tables have been created.</p>

<p>For security reasons, we will create a MySQL user specifically for
TorrentFlux. On the MySQL main page, click <em>"User Permissions"</em>,
and then <em>"Create new user"</em>. Enter the following, and make sure
to select the appropriate radio buttons:</p>

<ul>
   <li><em>Username</em> = <tt>torrentflux</tt>,</li>
   <li><em>Password</em> = *enter a password which you will add to the config.php file later*,</li>
   <li><em>Hosts</em> = <tt>localhost</tt>,</li>
</ul>

<p>Don't select any of the permissions, and Save.</p>

<p>Now, we will allow this new user to modify the torrentflux database,
only. Back on the MySQL main page, click on <em>"Database
Permissions"</em>, and then on <em>"Create new database permissions"</em>.
Remembering to select the appropriate radio buttons, select the
following;</p>

<ul>
   <li><em>Databases</em> = <tt>torrentflux</tt> (from the drop-down menu),</li>
   <li><em>Username</em> = <tt>torrentflux</tt>,</li>
   <li><em>Hosts</em> = <tt>localhost</tt>.</li>
</ul>

<p>For the permissions, hold the Ctrl key, and select the following;</p>

<ul>
<tt>
   <li>Select table data,</li>
   <li>Insert table data,</li>
   <li>Update table data,</li>
   <li>Delete table data,</li>
   <li>Create tables,</li>
   <li>Drop tables,</li>
   <li>Alter tables.</li>
</tt>
</ul>

<p>That's it; we're done with MySQL!</p>

<p>Now, we will tell TorrentFlux about the database settings we have just
implemented. Using the Java browser, navigate to
<tt>/var/www/torrentflux_2.x/html</tt>, select the <tt>config.php</tt>
file, and click <em>"Edit"</em>. Modify the <em>"Your database connection
information"</em> section, entering the correct settings. Hints are
provided. It should look something like this:</p>

<p><tt>$cfg["db_type"] = "mysql";	// mysql, postgres7 view adodb/drivers/<br>
$cfg["db_host"] = "localhost";		// DB host computer name or IP<br>
$cfg["db_name"] = "torrentflux";	// Name of the Database<br>
$cfg["db_user"] = "torrentflux";	// username for your MySQL database<br>
$cfg["db_pass"] = "</tt>*password for MySQL user torrentflux*"<tt>; // password for database</tt></p>

<p>Save and close.</p>

<p>Now, we will tell the Web server, Apache httpd, to serve TorrentFlux on port
80. Go to <em>Servers &gt; Apache Web server</em>. You should have a Default
Server and a Virtual Server, set up for you already. Click on the
<em>Virtual Server</em>, and, at the bottom, in the <em>"Virtual Server
Details"</em> section, make the following changes;</p>

<ul>
   <li><em>Address</em> = <tt>Any</tt>,</li>
   <li><em>Port</em> = <tt>80</tt> (don't forget to select the radio button),</li>
   <li><em>Document Root</em> = <tt>/var/www/torrentflux_2.x/html</tt> (replace the "x" with your version number),</li>
</ul>

<p>and Save. Then, on the Apache server page, click <em>"Apply
Changes"</em> at the top right.</p>

<p>Now, in your browser, navigate to <tt>http://192.168.0.1</tt>, and you
should get the TorrentFlux login page. Note that the username and
password you enter here will create the administrator's account
settings. Don't forget these. Choose wisely, and proceed to login.</p>

<p>You will be taken to the settings page, where we will change a few
things.</p>

<ul>
   <li><em>Path</em> = <tt>/home/public</tt></li>
   <li><em>Max Upload and Download rates</em>: set these to your liking. If you have broadband, I would suggest setting the max upload rate to 5% of your total Internet bandwidth, and your max download rate to 40%. This should allow modest bandwidth for Web browsing, even with two Torrent downloads running. Ultimately, the choice is yours.</li>
   <li><em>Port Range</em> = <tt>40000</tt> - <tt>40010</tt></li>
</ul>

<p>Have a look at the other settings, and change them as you wish. You
can change them later, as well. Click <em>"Update Settings"</em>. There
are "lights" that indicate problems in your settings. All should be
green. Notice that TorrentFlux will download directly to our shared
folder, giving instant access over the LAN.</p>

<p>A nice feature of TorrentFlux is queueing. Click on <em>"queue"</em>
at the top, and choose if you want to enable it, and define how many
torrent connections you want to allow to run in total (server threads)
and per user (user threads). Click <em>"Update Settings"</em>. Going
with the 40% max download bandwidth per Torrent and allowing two
connections total to run at a time still leaves 20% of the bandwidth for
Web browsing.</p>

<p>Use the <em>"new user"</em> page to create normal or admin users for
any one you want to grant access to. Other settings include search
engine options and filters, external links, rss feeds, and database
backups.</p>

<p>Adding torrents is done either by uploading from your desktop
machine, pasting the URL of the torrent file, or searching using the
available search engines. Files will be saved in folders according to
TorrentFlux usernames in the shared folder.</p>

<p>Now, we will open ports 40000-40010 in Shorewall for the Torrent
software to work properly. Go to <em>Networking &gt; Shorewall
Firewall &gt: Firewall Rules &gt; Manually Edit File</em>, and paste
this rule at the end:</p>

<pre><tt>#torrentflux
ACCEPT	net	$FW	tcp	40000:40010</tt></pre>

<p>If you wish to access your TorrentFlux from the Internet, e.g., while
at work, and have a static external IP address, simply open port 80 on
the external firewall, by adding this rule:</p>

<pre><tt>#Apache Web server
ACCEPT	net	$FW	tcp	80</tt></pre>

<p>Click <em>Save</em>, and then <em>Apply Configuration</em> in the
Shorewall main page. You can then access TorrentFlux from anywhere, by
browsing to <tt>http://*your external IP address*</tt></p>

<p>If you have a dynamic IP address, then you will also have to use a
service such as that provided by <a
href="http://www.dyndns.com/services/dns/dyndns">Dynamic DNS</a>, which
is free. Instructions for this are available on <a
href="http://ubuntuguide.org/wiki/Ubuntu:Feisty#How_to_assign_Hostname_to_local_machine_with_dynamic_IP_using_free_DynDNS_service">ubuntuguide.org</a>.
Although they are meant to be done at the actual machine, you can do
them through Webmin, running the sudo commands in <em>Others &gt; Command
Shell</em> and editing the <tt>dyndns_update</tt> file in the Java file
manager tool.</p>

<p>One thing to be wary of is completely filling up your hard disk. This
will inevitably cause problems. So, just make sure you have enough
space, before you decide to run your Torrent session.</p>

<h3>15. System logs</h3>

<p>Speaking of space, although system log files are useful in diagnosing
problems, they sometimes occupy a whole lot of space. We will now limit
the size of the log files.</p>

<p>Go to <em>System &gt; Log File Rotation>Edit Global Options</em> and set
the <em>"Maximum size before rotating"</em> to <tt>50M</tt> (for 50MB)
and the <em>"Number of old logs to keep"</em> to <tt>4</tt>. This should
allow you to have decent system logs, without eating up all your disk
space. For a few days under normal use, keep an eye on the size of log
files in <tt>/var/log</tt> using the Java file manager. See which logs
are huge, fiddle with their settings in <em>System>Log File
Rotation</em> and <em>System>System Logs</em>. Bear in mind that all
that logging might be due to a real problem in your system. In general,
though, the debug logs are pretty massive, and not very important for our
purpose, especially the ones that debug network traffic.</p>

<h3>16. Backing up Webmin configurations</h3>

<p>Once you have set everything up, and all is working fine, it would be
wise to backup your settings, in case you get too adventurous trying to
fiddle around and break something, or even if you decide to change your
server machine. This will enable you to restore all your settings.</p>

Go to <em>Webmin &gt; Backup Configuration Files</em>. In the <em>"Modules to
backup"</em> list, select all of them (using the <tt>Shift</tt> key); for
the <em>Backup destination</em> choose <em>Local file</em>, and enter a
path, e.g., <tt>/home/*admin username*/backup-*date*.tar</tt>; in the
<em>"Include in backup"</em> section, check <em>"Webmin module
configuration files"</em> and <em>"Server configuration files"</em>, and
click <em>"Backup Now"</em>. I recommend naming your backup files
including the date, as choosing which one to restore from becomes
easier.</p>

<p>If you wish, you can set up Webmin to periodically backup your
configurations automatically, in the <em>"Scheduled backups"</em>
section. I set mine to backup up daily and weekly. Previous scheduled
backups are replaced, and only the latest one is kept. Restoring is
simply a matter of choosing which modules to restore, from which backup
file, and whether the configurations should be applied.</p>

<h3>17. Updating your server</h3>

<p>Once in a while, it would be wise to update your server to get the
latest fixes and patches. Do this by going to <em>System>Software
Packages</em>, and in the <em>"Upgrade all Packages"</em> section,
select:</p>


<ul>
<li><em>Resynchronize package list</em> = <tt>Yes</tt>,</li>
<li><em>Upgrade mode</em> = <tt>Normal upgrade</tt>,</li>
<li><em>Only show which packages would be upgraded</em> = <tt>No</tt>.</li>
</ul>

<p>Click <em>"Upgrade Now"</em>, and it will all be done automagically,
giving all the information about the upgrade. Also periodic upgrades to
Webmin, as we did at the beginning of this guide, are advisable.</p>

<h3>Other tools</h3>

<p>Some examples of other functionality you may be interested in including:</p>

<ul>
   <li><a href="http://ubuntuguide.org/wiki/Ubuntu:Feisty#SSH_Server">SSH</a> will allow you log in to your server in CLI over any network, while keeping everything encrypted and secure. I would recommend this, as everything else pales in comparison to the CLI, when it comes to control over your system.</li>
   <li>using <a href="http://ubuntuguide.org/wiki/Ubuntu:Feisty#Apache_HTTP_Server">Apache</a> HTTPd, to host your own Web sites in tandem with services like Dynamic DNS,</li>
   <li><a href="http://coppermine-gallery.net/">Coppermine Gallery</a> or <a href="http://ubuntuguide.org/wiki/Ubuntu:Feisty#Image_Gallery_Server">Image Gallery Server</a>, for sharing your photos online while keeping them off public services,</li>
   <li>hosting an <a href="http://ubuntuguide.org/wiki/Ubuntu:Feisty#FTP_Server">FTP server</a>, to share files over the Internet</li>
   <li>using <a href="http://www.ubuntugeek.com/send-and-receive-your-hotmail-messages-through-evolution.html">hotway</a> and <a href="http://www.freepops.org/en/">freepops</a>, to get your Hotmail and Yahoo mail right in your e-mail client,</li>
   <li>The list is endless, really, from disk quotas to clusters, and to think we have only used a fraction of the features in Webmin! Poke around! It only gets more interesting, and besides... what are backups for, anyway?</li>
</ul>

<p>As you may have gathered by now, administering a Linux server is not
a brain-twisting business, as some may have you think. Once you have
everything set up to meet your needs, your LAN server/gateway should run like
clockwork, requiring only occasional upgrades and maybe a pat on the
back. Moreover, Webmin makes it a pleasant point-and-click affair,
although, like everything else, you have to know what it is you want to
do. This is where the vast documentation and help from the Linux
community is priceless and indispensable.</p>

<p>With luck, everything has worked as expected, so far, and you now
benefit from a free (as in free speech), powerful, flexible,
easy-to-manage, easy-to-use, and cheap solution to your home networking
needs. This, dear friends, is the brilliance of free and open source
software!</p>

<hr />

<p> [1] <span class="editorial">Rick Moen comments</span>: On balance,
I'd recommend against forwarding queries to one's ISP's nameservers. At
minimum, users should understand the tradeoff.</p>

<p>Declaring forwarders in BIND declares a list of nameserver IPs to
consult on all lookups BIND cannot answer from its local cache, in
preference to the various nameservers that are authoritative for those
domains, that would have been otherwise consulted.  In the very short
term, this gives you faster responses on early queries (i.e., for a
short while after your local BIND instance fires up).  After a while,
most likely queries will be hits on the local cache, making your
relatively high speed of access to the ISP's nameservers mostly
irrelevant.</p>

<p>What you lose, through such forwarding, constitutes in my view a very
serious drawback: you are making your DNS rely on the performance,
reliability, and security of your ISP's nameservers. Most ISP
nameservers turn out to have poor performance, are seriously lacking in
performance and reasonableness of configuration, and have alarming
security problems.

<p>To be specific about some of those things:  Many ISPs appear to have 
deliberately overridden the Time to Live (TTL) values that would
normally be shared with cached DNS records, to artificially prolong the
life of that cached information even though it may have been sent with
low TTL values to keep cached copies from being used after becoming
obsolete.  The ISPs presumably do this, subverting the intended
DNS-updating mechanisms, to reduce (needed) network traffic.

<p>The worst problem is the security one:  Studies such as Dan
Kaminsky's have shown that a large number of ISP nameservers are
vulnerable to cache poisoning, a form of attack in which they are
induced to cache fraudulent data. This is especially true when those
nameservers are <a
href="http://www.circleid.com/posts/so_you_think_youre_safe_from_dns_cache_poisoning/">used
by forwarding nameservers</a> in the fashion that this article describes.

<p>The alternative, of course, is to just simply run your own
recursive-resolver nameserver without forwarders, which by default will
then get its data from the root zone and the immense tree of
authoritative servers it defines.  Fortunately, this works just
perfectly without touching anything at all, out of the box, and imposes
no performance deficiencies once you've started loading the cache.

<p>I guess, overall, there's an instinctive tendency in people to think
"Let's rely on the ISP's services, because they're the pros, and we
should get better performance and reliability, that way."  However, it
turns out that, most often, none of those things is actually the case,
and you can do much better on your own, with surprising ease.

<p>One last suggestion:  BIND9 is certainly a respected do-it-all
nameserver, but is vastly overfeatured when all you need is a simple
recursive resolver (and, in particular, aren't serving up locally defined 
domains of your own, which is called authoritative service).  Also,
BIND9 is infamously bloated and slow.   

<p>For all of those reasons, I long ago started keeping a catalogue of 
<a href=http://linuxmafia.com/faq/Network_Other/dns-servers.html">alternatives
to BIND9</a>, and in all honesty cannot say which of them I'd recommend,
since I've stayed with BIND9 despite reservations, but you might browse
the alternatives.


</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:141/lazar.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<!-- *** BEGIN bio *** -->
<hr>
<P>
<img ALIGN="LEFT" ALT="[picture]" SRC="../gx/authors/lazar.jpg" WIDTH="180" HEIGHT="200" class="bio">
<em>
Shane is a Medical Resident in Romania. He has been a ardent user of FOSS and Linux since 2004. He spends a sizeable amount of time on Linux forums learning about it and helping others where he can. Currently his favorite distro is Ubuntu, while he has used Mandrake/Mandriva in the past on his desktop and still does for his home network server.
</em>
<br CLEAR="all">
<!-- *** END bio *** -->
<br clear="all">


<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2007, <a href="../authors/lazar.html">Shane Lazar</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 141 of Linux Gazette, August 2007
</p>

</div>
</div>


<div class="content lgcontent">

<a name="pfeiffer"></a>
<h1>One Volunteer Per Child - GNU/Linux and the Community</h1>
<p id="by"><b>By <a href="../authors/pfeiffer.html">Ren&eacute; Pfeiffer</a></b></p>

</b>
</p>

<p>
<p>
The gory details of code and hardware often hide the details of the
"wetware", the human beings using technology. GNU/Linux software is
driven by hundreds and thousands of people who in turn make the
experience of using a computer a pleasant task for many more. There are
many projects out there that want to reach the non-technical groups of
our society. One of these projects is the One Laptop per Child (OLPC)
organisation.</p>

<h3>
Technology and Teaching
</h3>

<p>
Technology as seen in software and hardware doesn't fall from the sky.
There have to be skilled designers and developers at work who think,
code, build, test, and produce. These people don't fall from the sky,
either. They have to learn their skills at school or at university.
Learning to write a computer program is a lot easier when you have
access to computers and tools that enable you to get your code to run.
This sounds a bit like a vicious circle. Indeed, this is true, and can be
readily experienced by teaching, let's say, Perl programming without a
Perl interpreter. This is how bleak it looks in many schools around
the world. It is especially true in developing countries and rural
areas; in short, anywhere where there is not much budget for
equipment.</p>

<p>
This is where the One Laptop per Child project comes into play. It tries
to address two issues.  First, it supplies a tool that can be used for
teaching. The <em>XO-1</em>, formerly known as the $100 laptop, is a
laptop that can be used inside and outside school. It can be used for
reading, for writing, for playing, for collaborating and lots of other
activities. In fact, <em>activities</em> is the rather fitting name for
applications on the XO-1. The XO-1 is networked by using wireless
technology, thus finessing any need to integrate the One Crossover Cable 
per Child project (or worse) into the OLPC initiative. The second issue
is giving access to technology for children in a playful way.  If your
only association with computers is a big black mainframe sitting in a
air-conditioned dungeon, being ready to devour you alive, then you
won't have much fun working with computers, and probably are not very
keen on learning any technological skills. Managing a first contact
situation without traumata is one of the primary goals of teachers.</p>

<p>
So, all in all, the XO-1 is a wonderful thing - and it runs Linux! Which
is all the better.</p>

<h3>First Contact with the XO-1</h3>

<p>
My first contact with a XO-1 machine went without shock, but with even more
curiosity. It happened during the Linuxwochen in Vienna, an
annual event presenting talks, workshops, and companies working with Free
Software at locations throughout Austria. Aaron Kaplan held a talk about
the OLPC project, and managed to bring two XO-1s for anyone to try. The
laptops fell prey to the visitors, so you had to be quick to get a glance
at the system. However it was easy to hear them, for many played with the
musical sequencer software called <em>TamTam</em>. The XO-1s desktop is
tailored for children. This means that you navigate by using symbols. The
desktop also tries to express its messages by means of graphics and
icons.</p>

<p>
The hardware is tuned for low power consumption, in order to make
deployment in areas with an unstable electrical power grid easier.
Recharging can be done via solar panels or mechanical generators, such as
a spindle that can be operated by pulling a rope. The display is either
backlit or operates in a contrast mode; the latter allows for reading
text on the screen in broad sunlight. The CPU of the newer models is
capable of displaying video clips on-screen. Networking is done by using
wireless network cards. The laptops autoextend the range of the network
by using mesh technology; this means that every XO-1 acts as a wireless
client and as a mesh router. Mesh routing is also done while the system
is not being used - i.e., in sleep mode.</p>

<p>
There is a lot of design in this laptop. Which brings me to the people
who thought of all this.</p>

<h3>Getting Involved and Being Part of the Community</h3>

<p>
So, how did the two XO-1s end up at the Linuxwochen, and how did Aaron get
involved in all of this?  Since curiosity is part of my job description,
I asked Aaron a couple of questions.</p>

<ul>
     <li><p><strong>Hello, Aaron! What is your background in terms of computing 
     and Unix systems?</strong></p>

     <p>I started in around '92 or so with BSD. I think it was still
     called BSD-Lite, and it came on 3.5" floppy discs.  These were chunks, and
     you had to literally "cat" them together.  Anyway, at around the
     same time, I was telneting myself from hop to hop via the MCI
     systems to my first real e-mail account / Unix box on the Net: well.com.
     Remember, that was ~19200 baud modem times. So, like many people from
     that time who experienced the Net for the first time, I got totally
     hooked on all the online Unix services that were out there, just
     waiting to be explored.  Then, for some time, I was a bit active in the
     FreeBSD community.</p>

     <p>I guess from then on, just one interesting topic just kept
     coming in after the previous. So that is where I am now :) It was
     always a playful experience all the way.</p> 
     </li>

     <li><p><strong>How did you get involved with the OLPC project?</strong></p>

     <p>
     First of all, I have to state that I am a volunteer in olpcaustria.org - 
     a local grassroots organization that is not directly affiliated with 
     OLPC per se. However, we are in good contact with OLPC, and try to help 
     the core team and the whole idea.</p>

     <p>
     About the history:
     I got intensively involved only recently. Before, in 2006, I was looking
     for a job and a colleague of mine pointed me to the Google Summer of Code
     for OLPC. So, I tried to apply myself, but eventually ended up as a mentor
     for Arthur Wolf. The SoC project could have gone better, but - hey - all
     students need to get some experience. Actually, it would be interesting to
     look at Arthur's ideas again, and re-implement them.
     Then, for some time, I was not involved in any activity related to 
     OLPC.</p>

     <p>
     Recently, I had the chance to visit OLPC and MIT, and only then did I
     realise how close the works of funkfeuer.at (a wireless community network,
     similar to freifunk) had been networkwise to OLPC. Both projects work on
     mesh networks, and I do believe these mesh networks could complement each
     other.</p>
     </li>

     <li><p><strong>Where do you see the biggest benefits to education, in the
     countries that deploy the OLPC?</strong></p>

     <p>
     Personally, I believe that, in the short term, these countries will
     have a lot of good engineers who will understand computers,
     maths, physics, biology, etc., very intuitively.  However, please 
     remember, this is only the first step. You still need good education 
     and teachers later (as a university student). The whole topic is quite 
     big, not simple, but I still believe: if you give kids a chance to 
     program their own games, to playfully explore maths, physics, and the 
     like, then this is the very first and most important step.  Will every 
     kid with a laptop be an engineer? No! Of course not, luckily, not.</p>

     <p>
     So, I think OLPC is a great, great grand vision, and it will only be the
     first step in revolutionizing education.
     There are other new approaches as well, such as 
     <a href="http://cnx.org/">Connexions</a>, MIT open courseware, etc.</p>
     </li>

     <li><p><strong>A lot of companies support the OLPC project. What is the 
     role of the volunteer developers? Which skills are needed for which 
     tasks?</strong></p>
     

     <p>
     As far as I know - everything is needed. The core system is currently
     heavily in development.  Personally, I see the power of the open source 
     community primarily in creating cool mesh-enabled activities.</p>

     <p>(<em>"Activities" are the applications of the XO-1.</em>)</p>
     </li>

     <li><p><strong>A lot of people say that they aren't programmers, and 
     lack highly technical skills. Are there any opportunities for these 
     volunteers, as well?</strong></p>

     <p>
     Sure! Lobby for the OLPC project at your local government. :)
     Program some activities. And most important: test with kids!</p>

     <p>
     But for a better explanation, I would like to redirect the humble reader 
     to: <a href="http://wiki.laptop.org/go/Getting_involved_in_OLPC">Getting_involved_in_OLPC</a></p>
     </li>

     <li><p><strong>How is the development organised? Who reviews the code 
     and who decides which improvements go into the production 
     version?</strong></p>

     <ol>
        <li> IRC freenode.net, #olpc.</li>
        <li> Lots of discussions.</li>
        <li> We have to distinguish between the base system and other 
        contributions.  The base system is the most active development area,
        presently.</li>
        <li> git (<em>a source control management system, also used for the 
        Linux kernel</em>).</li>
     </ol>
     </li>

     <li><p><strong>Do you think other communities can benefits from the 
     OLPC project, as well?</strong></p>

     <p>
     Yes!  There is a wealth of really cool tech coming out of OLPC.
     Take, for example, the microsleep mode: The laptop has a DCON chip that
     keeps the image stable but everything else can go to sleep, including the
     CPU.  So that the CPU does not have to wake up every HZ ticks, there is,
     according to my knowledge, a special adaption to the timing code. In other
     words: the CPU can go to sleep in milliseconds, and wake up again in
     milliseconds. This drastically increases the battery hours you get out of
     the laptop. I think this change will come to mainstream Linux laptops. It
     is just too good to leave out.</p>
     </li>
</ul>

<h3>Our Turn</h3>

<p>
I deliberately wrote "our turn": Teaching children the ways of the world
can be done (and should be done) by any of us. Teaching children what
Free Software can do may be something to start with.  You don't need
programming skills to do that. All you need is to write, to read, and to
talk, as Aaron said. And the OLPC project is not the only one of its
kind. There are lots of efforts in progress to achieve similar goals,
and most of them go nowhere without a community.</p>

<h3>Useful Links</h3>

<p>
I picked some links to the OLPC project, volunteer Web sites, and things
I mentioned in the article. I don't wish to rate any efforts by
omission; I just mention a few, and probably missed many.</p>

<ul>
   <li> <a href="http://www.gnu.org/education/education.html">Free Software in Education</a></li>

   <li> <a href="http://www.linuxwochen.at/">Linuxwochen Austria</a> (in
German)</li>

   <li> <a href="http://www.olpcaustria.org/">OLPC Austria</a></li>

   <li> <a href="http://olpcnepal.org/">OLPC Nepal</a></li>

   <li> <a href="http://www.olpc.tv/">OLPC Videos</a></li>

   <li> <a href="http://www.laptop.org/">One Laptop Per Child (OLPC)</a></li>

   <li> <a href="http://www.infodev.org/en/Publication.107.html">Quick
   guide to low-cost computing devices and initiatives for the developing
   world</a></li>

   <li> <a href="http://wiki.laptop.org/go/TamTam">TamTam</a></li>

   <li> <a href="http://www.germany.fsfeurope.org/projects/education/argumentation.en.html">Why give precedence to Free Software at school?</a></li>
</ul>

</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:141/pfeiffer.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<!-- *** BEGIN bio *** -->
<hr>
<p>

<img align="left" alt="Bio picture" src="../gx/authors/pfeiffer.jpg" class="bio">

</p>
<em>

<p>
Ren&eacute; was born in the year of Atari's founding and the release of the game Pong.
Since his early youth he started taking things apart to see how they work. He
couldn't even pass construction sites without looking for electrical wires that
might seem interesting. The interest in computing began when his grandfather
bought him a 4-bit microcontroller with 256 byte RAM and a 4096 byte operating
system, forcing him to learn assembler before any other language.
</p>

<p>
After finishing school he went to university in order to study physics. He then
collected experiences with a C64, a C128, two Amigas, DEC's Ultrix, OpenVMS and
finally GNU/Linux on a PC in 1997. He is using Linux since this day and still
likes to take things apart und put them together again. Freedom of tinkering
brought him close to the Free Software movement, where he puts some effort into
the right to understand how things work. He is also involved with civil liberty
groups focusing on digital rights.
</p>

<p>
Since 1999 he is offering his skills as a freelancer. His main activities
include system/network administration, scripting and consulting. In 2001 he
started to give lectures on computer security at the Technikum Wien. Apart from
staring into computer monitors, inspecting hardware and talking to network
equipment he is fond of scuba diving, writing, or photographing with his digital
camera. He would like to have a go at storytelling and roleplaying again as soon
as he finds some more spare time on his backup devices.
</p>

</em>
</p>
<br clear="all">
<!-- *** END bio *** -->

<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2007, <a href="../authors/pfeiffer.html">Ren&eacute; Pfeiffer</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 141 of Linux Gazette, August 2007
</p>

</div>
</div>


<div class="content lgcontent">

<a name="collinge"></a>
<h1>HelpDex</h1>
<p id="by"><b>By <a href="../authors/collinge.html">Shane Collinge</a></b></p>

</b>
</p>

<p>
<p>
<em>These images are scaled down to minimize horizontal scrolling.</em>
<p>


<p>
<a href="http://linuxgazette.net/124/misc/nottag/flash.html"><b>Flash problems?</b></a>
<br>

<div class="cartoon">

<object>
<embed src="misc/collinge/623inheritance.swf" bgcolor="#ffffff" width="600" />
</object>

<a href="misc/collinge/623inheritance.swf"><p>Click here to see the full-sized image</p></a>

</div>
<div class="cartoon">

<object>
<embed src="misc/collinge/633hassle.swf" bgcolor="#ffffff" width="600" />
</object>

<a href="misc/collinge/633hassle.swf"><p>Click here to see the full-sized image</p></a>

</div>

<p> All HelpDex cartoons are at Shane's web site,
<a href="http://www.shanecollinge.com/">www.shanecollinge.com</a>.

<script src="http://www.google-analytics.com/urchin.js"
type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-1204316-1";
urchinTracker();
</script>


</p>

<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:141/collinge.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
	<!-- *** BEGIN bio *** -->
<hr>
<P>
<img ALIGN="LEFT" ALT="Bio picture" SRC="../gx/2002/note.png" class="bio">
<em>
Part computer programmer, part cartoonist, part Mars Bar. At night, he runs
around in his brightly-coloured underwear fighting criminals. During the
day... well, he just runs around in his brightly-coloured underwear. He
eats when he's hungry and sleeps when he's sleepy.
</em>
<br CLEAR="all">
<!-- *** END bio *** -->

<!-- *** END author bio *** -->

<div id="articlefooter">


<p>
Copyright &copy; 2007, <a href="../authors/collinge.html">Shane Collinge</a>. Released under the
<a href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>


<p>
Published in Issue 141 of Linux Gazette, August 2007
</p>

</div>
</div>


<img src="../gx/tux_86x95_indexed.png" id="tux" alt="Tux"/>

<br />

<script src="http://www.google-analytics.com/urchin.js"
type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-1204316-1";
urchinTracker();
</script>

</body>
</html>

