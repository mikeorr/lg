<!DOCTYPE html PUBLIC '-//W3C//DTD XHTML 1.0 Transitional//EN'
		 'http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd'>
<html xmlns='http://www.w3.org/1999/xhtml' lang='utf-8' xml:lang='utf-8'>
<head>
<title>Talkback:135/pfeiffer.html</title>
<meta http-equiv='Content-Type; charset=utf-8' />
<link rel='stylesheet' type='text/css' href='../../../lg.css' />
</head>
<body>
<a href="../../../"><img alt="Linux Gazette" src="../../../gx/2003/newlogo-blank-200-gold2.jpg" id="logo" /></a><img alt="Tux" src="../../../gx/tux_86x95_indexed.png" id="tux" /><p id="fun">...making Linux just a little more fun!</p><div class='content articlecontent'><a name="top"></a><h3>Talkback:135/pfeiffer.html</h3>
<p><b>[ In reference to "<a href='../../../135/pfeiffer.html'>TCP and Linux' Pluggable Congestion Control Algorithms</a>" in LG#135 ]</b></p><p>
<b><p>
Ren&eacute; Pfeiffer [lynx at luchs.at]

</p>
</b><br />
<b>Sun, 30 Mar 2008 23:37:14 +0200</b>
</p>

<p>
I forward this request since I asked for 
</p>

<p>
----- Forwarded message from Ren&eacute; Pfeiffer &lt;lynx@luchs.at&gt; -----
</p>

<pre>
From: Ren&eacute; Pfeiffer &lt;lynx@luchs.at&gt;
Date: Sun, 30 Mar 2008 20:26:00 +0200
To: Erik van Zijst &lt;erik.van.zijst@layerstream.com&gt;
Subject: Re: Scalable TCP Tuning
Message-ID: &lt;20080330182600.GC4927@nephtys.luchs.at&gt;
</pre>In-Reply-To: &lt;47EF3432.9090307@layerstream.com&gt;
Hello, Erik!
</p>

<p>
I'll answer to in private, but please let me know if I can send this
answer also to The Answer Gang mailing list. We like to keep all
feedback there, so our readers can find it.
</p>

<p>
On Mar 29, 2008 at 2333 -0700, Erik van Zijst appeared and said:
</p>

<pre>
&gt; Hi Rene,
&gt;
&gt; I read some of your Linux Gazette articles, specifically the one on TCP's 
&gt; pluggable congestion control and maybe you can give me a little push in the 
&gt; right direction.
&gt;
&gt; I'm in a startup doing streaming video over TCP (rather than UDP with 
&gt; forward error correction), but TCP is giving me some latency headaches. To 
&gt; ensure uninterrupted playback, we use a chunky client-side playback buffer, 
&gt; but in its relentless quest for throughput-optimization, often under high 
&gt; bandwidth and transcontinental RTT's, TCP manages to introduce enough 
&gt; latency to underrun any buffer.
&gt;
&gt; With streaming video, keeping latency within bounds is often more important 
&gt; than squeezing out a few percent extra throughput. I've looked at the 
&gt; pluggable congestion control algorithms which are great, but pretty much all 
&gt; of them focus on high throughput, rather than latency.
</pre>

<p>
Yes, most of the algorithms deal with increasing throughput on fat pipes
and high latency. Only the algorithms TCP Veno and Westwood deal with
other scenarios (frequent packet loss on wireless links). Apart from
that maybe Interactive TCP (iTCP) seems to be interesting, but this
isn't available as module (yet).
<a href="http://www.medianet.kent.edu/itcp/main.html">http://www.medianet.kent.edu/itcp/main.html</a>
</p>


<pre>
&gt; TCP maintains the dynamic send buffer between user- and kernel-space and in 
&gt; order to minimize context switches, Linux seems to have a tendency to making 
&gt; these really large. On high-latency, transcontinental connections, I often 
&gt; get 1MB+ send buffers that can easily contain over 10 seconds of video. From 
&gt; what I see, the kernel modules mostly seem to tune only the size of the cwnd 
&gt; within the send buffer, rather than the send buffer as a whole, but since 
&gt; this is probably the main cause for increased latency, I'm looking for a way 
&gt; to tune this and always keep it as small as possible. Linux already seems to 
&gt; increase the send buffer's capacity when the cwnd increases, but never seems 
&gt; to shrink it again.
&gt;
&gt; Would you have any tips for a Linux-based startup when it comes to 
&gt; low-latency TCP tuning?
</pre>

<p>
The only thing I noticed are the following settings in /proc.
</p>

<pre>
 - /proc/sys/net/ipv4/tcp_low_latency controls if the data is forwarded
   directly through the TCP stack to the application buffer (=1) or not
   (=0). I have never benchmarked or compared this setting, thought it's
   always on on my laptop (as I noticed just now, I must have fiddled
   with sysctl.conf here).
 
 - The application keeps its own buffer, but you can also influence the
   maximum socket buffers of the TCP stack in the kernel.
   <a href="http://dsd.lbl.gov/TCP-tuning/linux.html">http://dsd.lbl.gov/TCP-tuning/linux.html</a> describes the maximum size
   of send/receive buffers. You could try reducing this, but maybe you
   can't influence both sides of the connection.
</pre>

<p>
That's all that I can think of right now, but be aware that this is
isn't complete. <img src="../gx/smile.png" alt=":)">
</p>

<p>
Best regards,
Ren&eacute;.
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_talkback.html#mb-talkback_135_pfeiffer">Back</a><hr width="50%" align="left" /><p><br /></p></div>
</body>
</html>