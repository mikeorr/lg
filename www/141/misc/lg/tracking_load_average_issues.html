<!DOCTYPE html
	PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
	 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
<title>Tracking load average issues</title>
<link rel="stylesheet" type="text/css" href="../../../lg.css" />
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
</head>
<body>
<a href="../../../"><img alt="Linux Gazette" src="../../../gx/2003/newlogo-blank-200-gold2.jpg" id="logo" /></a><img alt="Tux" src="../../../gx/tux_86x95_indexed.png" id="tux" /><p id="fun">...making Linux just a little more fun!</p><div class='content articlecontent'><a name="top"></a><h3>Tracking load average issues</h3>
<p>
<b><p>
Neil Youngman [ny at youngman.org.uk]
</p>
</b><br />
<b>Wed, 18 Jul 2007 09:27:12 +0100</b>
</p>

<p>
I was asked to look at a system that had a consistent load average around 5.3 
to 5.5. Now I know very little about how to track down load average issues, 
so I haven't been able to find much. The CPU usage is about 90% idle, so it's 
not CPU bound.
</p>

<p>
I googled for "load average", "high load average" and "diagnose load average" 
and I found very little of use. the one thing I found was that if it's 
processes stuck waiting on I/O "ps ax" should show processes in state "D". 
There are none visible on this box.
</p>

<p>
Do the gang know of any good resources for diagnosing load average issues or 
have any useful tips?
</p>

<p>
Neil Youngman
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-tracking_load_average_issues">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Thomas Adam [thomas at edulinux.homeunix.org]
</p>
</b><br />
<b>Wed, 18 Jul 2007 19:27:37 +0100</b>
</p>

<p>
On Wed, Jul 18, 2007 at 09:27:12AM +0100, Neil Youngman wrote:
</p>

<pre>
&gt; I was asked to look at a system that had a consistent load average around 5.3 
&gt; to 5.5. Now I know very little about how to track down load average issues, 
&gt; so I haven't been able to find much. The CPU usage is about 90% idle, so it's 
&gt; not CPU bound.
</pre>

<p>
Which means it can only be I/O related.
</p>


<pre>
&gt; I googled for "load average", "high load average" and "diagnose load average" 
&gt; and I found very little of use. the one thing I found was that if it's 
&gt; processes stuck waiting on I/O "ps ax" should show processes in state "D". 
&gt; There are none visible on this box.
</pre>

<p>
No, that's completely bogus.  All active processes can be I/O bound -- look
at communication between sockets and pipes, for example.
</p>


<pre>
&gt; Do the gang know of any good resources for diagnosing load average issues or 
&gt; have any useful tips?
</pre>

<p>
I sometimes help diagnose this sort of thing at work, and it can be tricky.
I suppose you could look at some of your processes and attach strace to them
to see what they're up to, but then only you would know which processes.
</p>

<p>
Sometimes, a high load average can be hardware related as well, such as
having apic turned on in the BIOS, for instance.  You could also look at
lsof to make sure nothing out of the ordinary is left open, etc.  But at
that point, you're really clutching at straws.
</p>

<pre>-- 
Thomas Adam
"He wants you back, he screams into the night air, like a fireman going
through a window that has no fire." -- Mike Myers, "This Poem Sucks".
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-tracking_load_average_issues">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Jim Jackson [jj at franjam.org.uk]
</p>
</b><br />
<b>Wed, 18 Jul 2007 22:19:05 +0100 (BST)</b>
</p>

<p>
On Wed, 18 Jul 2007, Neil Youngman wrote:
</p>


<pre>
&gt; I was asked to look at a system that had a consistent load average around 5.3
&gt; to 5.5. Now I know very little about how to track down load average issues,
&gt; so I haven't been able to find much. The CPU usage is about 90% idle, so it's
&gt; not CPU bound.
</pre>

<p>
Have you run the "top" command? It shows which processes are most active.
I believe there are such things as gtop, ktop? etc for the those suffering
command line timidity^H^H^H^H^H^H^H^Htemerity (timidity is a midi
application <img src="../gx/smile.png" alt=":-)">.
</p>

<p>
A load ave of 5 means that there were on ave 5 processes read to run for
the whole of the averaging period - the usual ave periods are 1min 5min
and 15min.
</p>


<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-tracking_load_average_issues">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ren&eacute; Pfeiffer [lynx at luchs.at]
</p>
</b><br />
<b>Wed, 18 Jul 2007 23:57:54 +0200</b>
</p>

<p>
On Jul 18, 2007 at 0927 +0100, Neil Youngman appeared and said:
</p>

<pre>
&gt; I was asked to look at a system that had a consistent load average around 5.3
&gt; to 5.5. Now I know very little about how to track down load average issues,
&gt; so I haven't been able to find much. The CPU usage is about 90% idle, so it's
&gt; not CPU bound.
</pre>

<p>
"htop" is a nice tool for browsing the process table. It has some nice
features and a better screen output than "top". Modern "procinfo"
variants can also distinguish between user, nice, system, waiting for
I/O, interrupt and idle time. This helps to get a better view at the
load.
</p>

<p>
Thomas already noticed that you are probably I/O bound on this machine.
Look at everything that shuffles or waits for data; databases, network
applications, file processing. "iostat", "netstat" and "vmstat" can help
you to see I/O rates.
</p>

<p>
If this system is attached to a monitoring system (such as Nagios or
Munin, <a href="http://munin.sourceforge.net/">http://munin.sourceforge.net/</a>, for example) you might also get an
impression about what the system is doing.
</p>

<p>
Best wishes,
Ren&eacute;.
</p>


<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-tracking_load_average_issues">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Neil Youngman [ny at youngman.org.uk]
</p>
</b><br />
<b>Thu, 19 Jul 2007 09:09:04 +0100</b>
</p>

<p>
On or around Wednesday 18 July 2007 22:19, Jim Jackson reorganised a bunch of 
electrons to form the message:
</p>

<pre>
&gt; On Wed, 18 Jul 2007, Neil Youngman wrote:
&gt; &gt; I was asked to look at a system that had a consistent load average around
&gt; &gt; 5.3 to 5.5. Now I know very little about how to track down load average
&gt; &gt; issues, so I haven't been able to find much. The CPU usage is about 90%
&gt; &gt; idle, so it's not CPU bound.
&gt;
&gt; Have you run the "top" command? It shows which processes are most active.
&gt; I believe there are such things as gtop, ktop? etc for the those suffering
&gt; command line timidity^H^H^H^H^H^H^H^Htemerity (timidity is a midi
&gt; application <img src="../gx/smile.png" alt=":-)">.
</pre>

<p>
Yes, top was the first thing I ran, which is where I got the 90% idle figure.
</p>


<pre>
&gt; A load ave of 5 means that there were on ave 5 processes read to run for
&gt; the whole of the averaging period - the usual ave periods are 1min 5min
&gt; and 15min.
</pre>

<p>
That much I found, although apparently "ready to run" includes waiting for 
I/O, otherwise they would soak up that 90% CPU idle time. What I haven't 
found yet is a way to tell which processes are waiting on I/O and why?
</p>

<p>
Neil
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-tracking_load_average_issues">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Jim Jackson [jj at franjam.org.uk]
</p>
</b><br />
<b>Thu, 19 Jul 2007 22:57:07 +0100 (BST)</b>
</p>

<p>
On Thu, 19 Jul 2007, Neil Youngman wrote:
</p>


<pre>
&gt; On or around Wednesday 18 July 2007 22:19, Jim Jackson reorganised a bunch of
&gt; electrons to form the message:
&gt; &gt; On Wed, 18 Jul 2007, Neil Youngman wrote:
&gt; &gt; &gt; I was asked to look at a system that had a consistent load average around
&gt; &gt; &gt; 5.3 to 5.5. Now I know very little about how to track down load average
&gt; &gt; &gt; issues, so I haven't been able to find much. The CPU usage is about 90%
&gt; &gt; &gt; idle, so it's not CPU bound.
&gt; &gt;
&gt; &gt; Have you run the "top" command? It shows which processes are most active.
&gt; &gt; I believe there are such things as gtop, ktop? etc for the those suffering
&gt; &gt; command line timidity^H^H^H^H^H^H^H^Htemerity (timidity is a midi
&gt; &gt; application <img src="../gx/smile.png" alt=":-)">.
&gt;
&gt; Yes, top was the first thing I ran, which is where I got the 90% idle figure.
&gt;
&gt; &gt; A load ave of 5 means that there were on ave 5 processes read to run for
&gt; &gt; the whole of the averaging period - the usual ave periods are 1min 5min
&gt; &gt; and 15min.
&gt;
&gt; That much I found, although apparently "ready to run" includes waiting for
&gt; I/O,
</pre>

<p>
It sort of depends on <em>how</em> the process is "waiting" for i/o. Doing it the
sensible way and the process should be sleeping untill i/o, i.e. doing a
blocking read or using select or similar. However bad design spinning on a
non blocking read would possibly account for it.
</p>


<pre>
&gt;From the ps manual page
</pre>

<pre>
      PROCESS STATE CODES
       D   uninterruptible sleep (usually IO)
       R   runnable (on run queue)
       S   sleeping
       T   traced or stopped
       Z   a defunct ("zombie") process
</pre>
If the process is runnable then the userland code is ready to run, and
that is what load measures.
</p>


<pre>
&gt; otherwise they would soak up that 90% CPU idle time. What I haven't
&gt; found yet is a way to tell which processes are waiting on I/O and why?
</pre>

<p>
It does indeed seem strange. Everytime I've investigated high persistant
loads there have been obvious culprits.
</p>

<pre>
&gt;From top get the process numbers of likely suspect and watch them with suitable
</pre>

<pre>
 while true do
  ps uax | grep PID
  sleep 1
 done
</pre>
to see what they are doing. You may need to customise the ps flags.
</p>



<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-tracking_load_average_issues">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Thomas Adam [thomas at edulinux.homeunix.org]
</p>
</b><br />
<b>Thu, 19 Jul 2007 23:01:45 +0100</b>
</p>

<p>
On Thu, Jul 19, 2007 at 10:57:07PM +0100, Jim Jackson wrote:
</p>

<pre>
&gt; It sort of depends on <em>how</em> the process is "waiting" for i/o. Doing it the
&gt; sensible way and the process should be sleeping untill i/o, i.e. doing a
&gt; blocking read or using select or similar. However bad design spinning on a
&gt; non blocking read would possibly account for it.
</pre>

<p>
Maybe, but that's slightly going in the wrong direction.  I suspect if Neil
can confirm if this is a persistent issue or not, that it's going to be
hardware-related, and not software I/O (the mark of a suspect program, for
instance.)  In which case, going with the noapic suggestion both in the
kernel and in the BIOS (as I suggested) is still something worth trying.
</p>


<pre>
&gt;  while true do
&gt;   ps uax | grep PID
&gt;   sleep 1
&gt;  done
</pre>

<p>
Let's do this properly using watch(1), please.
</p>

<pre>-- 
Thomas Adam
"He wants you back, he screams into the night air, like a fireman going
through a window that has no fire." -- Mike Myers, "This Poem Sucks".
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-tracking_load_average_issues">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Mulyadi Santosa [mulyadi.santosa at gmail.com]
</p>
</b><br />
<b>Fri, 20 Jul 2007 12:38:55 +0700</b>
</p>

<p>
Hi Neil...
</p>

<pre>
&gt; That much I found, although apparently "ready to run" includes waiting for
&gt; I/O, otherwise they would soak up that 90% CPU idle time. What I haven't
&gt; found yet is a way to tell which processes are waiting on I/O and why?
</pre>

<p>
I don't know if doing kernel compilation is possible in your env, but
perhaps.. if you can, maybe blktrace can help you. You can find it
here:
<a href="http://git.kernel.dk/?p=blktrace.git;a=summary">http://git.kernel.dk/?p=blktrace.git;a=summary</a>
</p>

<p>
I don't know how to fetch git repository, so the best I can do is just
showing you the tool. You also need to turn on a kernel config...I
forgot the exact name, try to grep for "block trace" in your kernel
config.
</p>

<p>
Another thing you can try is system tap. However, that also needs
kprobe support. What you need to hook is probably any fs related
operation (read, write, readahead, flush and so on).
</p>

<p>
Goin' in reverse way, you said the load is about 5? IIRC, that means
in average there are 5-6 runnable process running on each of your CPU
(cores). And they likely sleep on something. If you can confirm via
iostat that it's indeed block device access, the at least you get a
lead. If not, it can be anything: busy waiting on socket, busy ping
pong between Unix domain sockets, etc.
</p>

<p>
I think, what you can do now... is to hook to every suspicious process
id using strace. it takes time, but the result could be more
satisfying than just counting on statistics.
</p>

<p>
regards,
</p>

<p>
Mulyadi
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-tracking_load_average_issues">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Neil Youngman [ny at youngman.org.uk]
</p>
</b><br />
<b>Fri, 20 Jul 2007 10:53:07 +0100</b>
</p>

<p>
On or around Thursday 19 July 2007 23:01, Thomas Adam reorganised a bunch of 
electrons to form the message:
</p>

<pre>
&gt; On Thu, Jul 19, 2007 at 10:57:07PM +0100, Jim Jackson wrote:
&gt; &gt; It sort of depends on <em>how</em> the process is "waiting" for i/o. Doing it
&gt; &gt; the sensible way and the process should be sleeping untill i/o, i.e.
&gt; &gt; doing a blocking read or using select or similar. However bad design
&gt; &gt; spinning on a non blocking read would possibly account for it.
&gt;
&gt; Maybe, but that's slightly going in the wrong direction.  I suspect if Neil
&gt; can confirm if this is a persistent issue or not, that it's going to be
&gt; hardware-related, and not software I/O (the mark of a suspect program, for
&gt; instance.)  In which case, going with the noapic suggestion both in the
&gt; kernel and in the BIOS (as I suggested) is still something worth trying.
</pre>

<p>
It's intermittent rather than persistent. The affected servers will run with a 
load average around 0.3 normally for weeks, then the load average will ramp 
up and settle at a relatively high level for an hour or so before returning 
to normal. 
</p>

<p>
There is no obvious degradation of service, so we're not panicking about it, 
but we are wondering if the systems are trying to tell us something.
</p>

<p>
These aren't systems on which I can easily mess with kernel and BIOS options, 
but I'll look into whether we can do something with the NOAPIC option.
</p>

<p>
Neil
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-tracking_load_average_issues">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Kapil Hari Paranjape [kapil at imsc.res.in]
</p>
</b><br />
<b>Fri, 20 Jul 2007 16:03:06 +0530</b>
</p>

<p>
Hello,
</p>

<p>
On Fri, 20 Jul 2007, Neil Youngman wrote:
</p>

<pre>
&gt; It's intermittent rather than persistent. The affected servers will run with a 
&gt; load average around 0.3 normally for weeks, then the load average will ramp 
&gt; up and settle at a relatively high level for an hour or so before returning 
&gt; to normal. 
&gt; 
&gt; There is no obvious degradation of service, so we're not panicking about it, 
&gt; but we are wondering if the systems are trying to tell us something.
</pre>

<p>
This seems to point to something like a cron job which raises the
load but is "nice" about it. 
</p>

<p>
I suppose you have looked at the output of ps and tried to find
processes that make a lot of use of I/O resources. 
</p>

<p>
One thing that does use I/O resources extensively but does not
otherwise load the system is any program that uses "polling" to
get interactive input. One non-interactive such routine is a ppp
dialer. Other expect-send routines are equally suspect. 
</p>

<p>
Until recently I used to use "cat /dev/xconsole" to get log messages
on one of my "screen"s. This tends to use up significant I/O
resources as compared with "inotail -f /dev/xconsole".
</p>

<p>
I suppose you can also look for process that need enough memory as to
cause significant swapping.
</p>

<p>
Regards,
</p>

<p>
Kapil.
--
</p>


<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-tracking_load_average_issues">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Raj Shekhar [rajlist2 at rajshekhar.net]
</p>
</b><br />
<b>Fri, 27 Jul 2007 20:40:28 +0530</b>
</p>

<p>
in infinite wisdom Neil Youngman spoke thus  On 07/18/2007 01:57 PM:
</p>


<pre>
&gt; I googled for "load average", "high load average" and "diagnose load average" 
&gt; and I found very little of use. the one thing I found was that if it's 
&gt; processes stuck waiting on I/O "ps ax" should show processes in state "D". 
&gt; There are none visible on this box.
</pre>

<p>
I doubt you can fix it without any monitoring.  There are lots of light 
weight monitoring scripts that you can use (I use nagios, and I cannot 
remember any names for light weight scripts from the top of my head. 
Nagios would be an overkill for you).  In generic terms, what you need 
to do is
</p>

<p>
- install a monitoring script
</p>

<p>
- all monitoring systems have hooks that allow you to insert your own monitoring scripts
</p>

<p>
- monitor for system load - the bash oneliner should give you the system load for the past 1 minute
</p>

<pre>
uptime  |perl -lane 'if (m/.+ (load average: )(.+), (.+), (.+)/) {print $2}'
</pre>
- when the number goes above UPPER-LIMIT, then do a 'ps auxww &gt;&gt; LOG_FILE' from your monitoring script itself (where UPPER-LIMIT and LOG_FILE are your user supplied values)
</p>

<p>
- study the log file deeply to find a pattern.
</p>

<p>
- fix the problem
</p>

<p>
You can also use sar to monitor system load. sar is part of the sysstat 
package and you need to enable it explicitly through cron (man sar, sadc 
,sa1 for more details).  Once you have sar running, check its data to 
see if you can find a pattern in spikes and then dig further.
</p>

<pre>-- 
raj shekhar
facts: <a href="http://rajshekhar.net">http://rajshekhar.net</a> | opinions: <a href="http://rajshekhar.net/blog">http://rajshekhar.net/blog</a>
I dare do all that may become a man; Who dares do more is none.
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-tracking_load_average_issues">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Fri, 27 Jul 2007 14:10:00 -0400</b>
</p>

<p>
On Fri, Jul 27, 2007 at 08:40:28PM +0530, Raj Shekhar wrote:
</p>

<pre>
&gt; in infinite wisdom Neil Youngman spoke thus  On 07/18/2007 01:57 PM:
&gt; 
&gt; &gt; I googled for "load average", "high load average" and "diagnose load average" 
&gt; &gt; and I found very little of use. the one thing I found was that if it's 
&gt; &gt; processes stuck waiting on I/O "ps ax" should show processes in state "D". 
&gt; &gt; There are none visible on this box.
&gt; 
&gt; I doubt you can fix it without any monitoring.  There are lots of light 
&gt; weight monitoring scripts that you can use (I use nagios, and I cannot 
&gt; remember any names for light weight scripts from the top of my head. 
&gt; Nagios would be an overkill for you).  In generic terms, what you need 
&gt; to do is
&gt;   - install a monitoring script
&gt;   - all monitoring systems have hooks that allow you to insert your own 
&gt; monitoring scripts
&gt;   - monitor for system load - the bash oneliner should give you the 
&gt; system load for the past 1 minute
&gt; "
&gt; uptime  |perl -lane 'if (m/.+ (load average: )(.+), (.+), (.+)/) {print $2}'
&gt; "
&gt;   - when the number goes above UPPER-LIMIT, then do a 'ps auxww &gt;&gt; 
&gt; LOG_FILE' from your monitoring script itself (where UPPER-LIMIT and 
&gt; LOG_FILE are your user supplied values)
</pre>

<p>
You could easily combine the two tasks and automate them, perhaps by
running a cron job. Or, you could wrap a '{ ...; sleep 10; redo }'
construct around the statements below to get snapshots at 10-second
intervals.
</p>

<pre>
perl -we'`uptime`=~/average: ([^,]+)/;system "ps auxww&gt;&gt;LOG_FILE" if $1&gt;$LIMIT'
</pre>
Obviously, you'll want to modify the name of the log file and set $LIMIT
(or use an explicit value.)
</p>


<pre>
&gt;   - study the log file deeply to find a pattern.
</pre>

<p>
I think that this is the part that Neil was asking about, really. <img src="../gx/smile.png" alt=":)">
</p>


<pre>
&gt;   - fix the problem
</pre>

<p>
Once the above is done, that's most likely trivial.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-tracking_load_average_issues">Back</a><hr width="50%" align="left" /><p><br /></p></div>
</body>
</html>