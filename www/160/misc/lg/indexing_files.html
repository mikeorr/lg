<!DOCTYPE html PUBLIC '-//W3C//DTD XHTML 1.0 Transitional//EN'
		 'http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd'>
<html xmlns='http://www.w3.org/1999/xhtml' lang='utf-8' xml:lang='utf-8'>
<head>
<title>Indexing files</title>
<meta http-equiv='Content-Type; charset=utf-8' />
<link rel='stylesheet' type='text/css' href='../../../lg.css' />
</head>
<body>
<a href="../../../"><img alt="Linux Gazette" src="../../../gx/2003/newlogo-blank-200-gold2.jpg" id="logo" /></a><img alt="Tux" src="../../../gx/tux_86x95_indexed.png" id="tux" /><p id="fun">...making Linux just a little more fun!</p><div class='content articlecontent'><a name="top"></a><h3>Indexing files</h3>
<p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]

</p>
</b><br />
<b>Tue, 3 Feb 2009 21:59:16 -0500</b>
</p>

<p>
[ Karl, I hope you don't mind me copying this exchange to The Answer
Gang; I'd like for the error that you pointed out to be noted in our
next issue, and this is the best and easiest way to do it. If you have
any further replies, please CC them to 'tag@lists.linuxgazette.net'. ]
</p>

<p>
On Tue, Feb 03, 2009 at 02:59:57PM -0500, Karl Vogel wrote:
</p>

<pre>
&gt; Very cool follow-up article!
</pre>

<p>
Thanks, Karl; I appreciate that. That's a very, very fun program -
again, thanks for introducing me to it!
 
</p>

<pre>
&gt; &gt;&gt; In a previous message, you unhesitatingly continued with this missive:
&gt; 
&gt; B&gt; In practice, I've found that indexing HTML files with either "-ft" or
&gt; B&gt; "-fh" leads to exactly the same results - i.e., a working index for all
&gt; B&gt; the content - and so now I lump both of the above under "-ft".
&gt; 
&gt;    The display is different in the web interface.  I indexed the same small
&gt;    collection of HTML files as both plain text and HTML, and then looked for
&gt;    "samba troubleshooting".
&gt; 
&gt;    Search for the phrase when indexed as plain text:
&gt;      <a href='http://localhost/search/plain/estseek.cgi?phrase=samba+troubleshooting'>http://localhost/search/plain/estseek.cgi?phrase=samba+troubleshooting</a>
&gt; 
&gt;    Command used to index:
&gt;      estcmd gather -sd -ft plain /tmp/searchlmaiHg
&gt; 
&gt;    Display:
&gt;      SAMBA_Troubleshooting.htm 24428
&gt;        &lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2//EN"&gt; &lt;html&gt; &lt;head&gt; &lt;meta
&gt;        name="generator" content=" ...  me="Generator" content="Microsoft
&gt;        Word 97"&gt; &lt;title&gt;Troubleshooting Log for VOS Samba&lt;/title&gt; &lt;/head&gt;
&gt;        &lt;body link="#000 ...  /font&gt;&lt;/b&gt;&lt;/p&gt; &lt;p align="CENTER"&gt;&lt;b&gt;&lt;font
&gt;        size="6"&gt;Samba&lt;/font&gt;&lt;/b&gt;&lt;/p&gt; &lt;p align="CENTER"&gt;&lt;b&gt;&lt;font size="6" ...
&gt;        &gt;Troubleshooting&lt;/font&gt;&lt;/b&gt;&lt;/p&gt; &lt;p align="CENTER"&gt;&lt;b&gt;&lt;font size="6"
&gt;        ...  1516637"&gt;*&lt;/a&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="#_Toc531516638"&gt;Samba
&gt;        Symptoms, Causes and Resolutions &lt;a href="#_Toc531 ...
&gt;      <a href='http://localhost/search/docs/SAMBA_Troubleshooting.htm'>http://localhost/search/docs/SAMBA_Troubleshooting.htm</a> - [detail]
&gt; 
&gt;    Click the "details" link and check the attributes:
&gt;      @type: text/plain
&gt;
&gt;    Now do a search for the same phrase when indexed as HTML:
&gt;      <a href='http://localhost/search/hyper/estseek.cgi?phrase=samba+troubleshooting'>http://localhost/search/hyper/estseek.cgi?phrase=samba+troubleshooting</a>
&gt; 
&gt;    Command used to index:
&gt;      estcmd gather -sd -fh hyper /tmp/searchlmaiHg
&gt; 
&gt;    Display:
&gt;      Troubleshooting Log for VOS Samba 25592
&gt;        Samba Troubleshooting Guide Version 2.0.7 Paul Green May 22, 2002 -
&gt;        2001, 2002 Paul Green.  Permi ...  ree Documentation License".  Contents
&gt;        Terminology * Samba Symptoms, Causes and Resolutions * Introduction
&gt;        * ...  and Editing Host Files from a PC * Miscellaneous * Samba Web
&gt;        Access Tool (SWAT) * Troubleshooting * GNU Fre ...  t that I am unable
&gt;        to offer personal assistance in troubleshooting specific problems.
&gt;        Installation This section lists ...  hat arise during installation
&gt;        and configuration of Samba.  Symptom: Cannot add a new HOST machine
&gt;        to an NT D ...
&gt;      <a href='http://localhost/search/docs/SAMBA_Troubleshooting.htm'>http://localhost/search/docs/SAMBA_Troubleshooting.htm</a> - [detail]
&gt; 
&gt;    Click the "details" link and check the attributes:
&gt;      @type: text/html
</pre>

<p>
I just ran a careful, step-by-step manual retest of the above, and
you're absolutely right. I must have lost track of what I did during
which test - it does indeed make a difference.
</p>

<p>
On the other hand - please bear with me while I think "out loud" about
this - since the only place that difference shows up is in the cited
"hit context" paragraphs in Hyperestraier and not in the content itself,
I'm not sure how much extra effort this deserves. In order to make that
small change - i.e., not have the HTML markup appear in the cited
paragraph, which only shows up for a second or so during the process -
you'd have to split the files into two streams, index each of them
individually, then do 'extkeys/optimize/purge' on both... pretty much
double the processing time and seriously increase the complexity of the
build script. Doesn't seem like much of a payback for a whole lot of
work.
</p>

<p>
I suppose you could use "-fx" to keep the modifications really simple:
just add something like '-fx htm* H@"lynx -dump -nolist"' to the "estcmd
gather" line... but if you're going to do that, you might as well set up
processing for all the other "interesting" types of files: PDFs, RTFs,
OpenOffice files, etc. (I was going to write about that, too, but
figured it would become too complex at that point.) I guess it's a
question of deciding where the cutoff point is and building the indexer
to reflect that.
</p>

<p>
Overall, I don't think that doing major hackery just to fix the context
paragraph is worthwhile. For myself, I'm going to leave it just as it
is until I decide to start processing the other filetypes.
 
</p>

<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_talkback.html#mb-indexing_files">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>

</p>
</b><br />
<b>Thu, 5 Feb 2009 13:08:06 -0500 (EST)</b>
</p>

<pre>
&gt;&gt; On Tue, 3 Feb 2009 21:59:16 -0500, 
&gt;&gt; Ben Okopnik &lt;ben@linuxgazette.net&gt; said:
</pre>

<p>
B&gt; On the other hand - please bear with me while I think "out loud" about
B&gt; this - since the only place that difference shows up is in the cited
B&gt; "hit context" paragraphs in Hyperestraier and not in the content
B&gt; itself, I'm not sure how much extra effort this deserves.
</p>

<p>
   Yup, this only starts to matter if you're searching lots of different
   filetypes.  I was trying to index as much content on a fileserver as I
   could, to assist in records-office searches.
</p>

<p>
B&gt; [...] you'd have to split the files into two streams, index each of
B&gt; them individually, then do 'extkeys/optimize/purge' on both.
</p>

<p>
   No, the extkeys/etc stuff only has to be done once if you're building
   one index to hold more than one type of files.
</p>

<p>
B&gt; I suppose you could use "-fx" to keep the modifications really simple:
B&gt; just add something like '-fx htm* H@"lynx -dump -nolist"' to the
B&gt; "estcmd gather" line... but if you're going to do that, you might as
B&gt; well set up processing for all the other "interesting" types of files:
B&gt; PDFs, RTFs, OpenOffice files, etc.
</p>

<p>
   And this is where I found the memory problem mentioned in the original
   article, not to mention all sorts of MS/Adobe files which aren't
   handled well by rtf2txt, antiword, xls2csv, and pdftotext.  I finally
   had to resort to running "strings" on lots of things and hoping for
   the best.  That's what the "locword" entry does in the example below.
</p>

<p>
   The approach that worked best (failed least) was to run "file -i" on a
   fileset to get the MIME types, and then make a few passes through the
   resulting list to index what I could.  Here's part of the script.
</p>

<pre>-- 
Karl Vogel                      I don't speak for the USAF or my company
</pre>                 --Washington Post "alternate definitions" contest
</p>

<p>
# --------------------------------------------------------------------------
# $ftype holds output from "file":
#
#   /tmp/something.xls| application/msword
#   /tmp/resume.pdf| application/pdf
#   /tmp/somedb.mdb| application/x-msaccess
</p>

<p>
opts="-cl -sd -cm -xh -cs 128"
</p>

<p>
(
    # -------------------------------------------------------------------
    # Plain text files.  The mimetypes file looks like this:
    #   application/x-perl
    #   application/x-shellscript
    #   message/news
    #   message/rfc822
    #   text/html
    #   text/plain
    #   text/rtf
    #   text/troff
    #   text/x-asm
    #   text/x-c
    #   text/x-mail
    #   text/x-news
    #   text/x-pascal
    #   text/x-tex
    #   text/xml
</p>

<p>
    logmsg starting plain text
    mimetypes='/usr/local/share/mime/plain-text'
    fgrep -f $mimetypes $ftype |
        cut -f1 -d'|' |
        estcmd gather $opts -ft $dbname -
</p>

<p>
    # -------------------------------------------------------------------
    # Word files
</p>

<p>
    logmsg starting Word
    exten=".doc,.msg,.xls,.xlw"
    grep 'application/msword' $ftype |
        cut -f1 -d'|' |
        estcmd gather $opts -fx "$exten" "T@locword" -fz $dbname -
</p>

<p>
    # -------------------------------------------------------------------
    # Access DBs
</p>

<p>
    logmsg starting Access
    exten=".mdb,.mde,.mdt,.use"
</p>

<p>
    grep 'application/x-msaccess' $ftype |
        cut -f1 -d'|' |
        estcmd gather $opts -fx "$exten" "T@locword" -fz $dbname -
</p>

<p>
    # -------------------------------------------------------------------
    # Excel files with different MIME type
</p>

<p>
    logmsg starting remaining Excel
    exten=".xls,.xlw"
    grep 'application/vnd.ms-excel' $ftype |
        cut -f1 -d'|' |
        estcmd gather $opts -fx "$exten" "T@locword" -fz $dbname -
</p>

<p>
    # -------------------------------------------------------------------
    # PDF files
</p>

<p>
    logmsg starting PDF
    exten=".pdf"
    grep 'application/pdf' $ftype |
        cut -f1 -d'|' |
        estcmd gather $opts -fx "$exten" "H@estfxpdftohtml" -fz $dbname -
</p>

<p>
    # -------------------------------------------------------------------
    # Index cleanup for searching.
</p>

<p>
    logmsg cleaning up index
    estcmd extkeys $dbname
    estcmd optimize $dbname
    estcmd purge -cl $dbname
</p>

<p>
) &gt; BUILDLOG 2&gt;&amp;1
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_talkback.html#mb-indexing_files">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]

</p>
</b><br />
<b>Fri, 6 Feb 2009 19:30:21 -0500</b>
</p>

<p>
On Thu, Feb 05, 2009 at 01:08:06PM -0500, Karl Vogel wrote:
</p>

<pre>
&gt; &gt;&gt; On Tue, 3 Feb 2009 21:59:16 -0500, 
&gt; &gt;&gt; Ben Okopnik &lt;ben@linuxgazette.net&gt; said:
&gt; 
&gt; B&gt; [...] you'd have to split the files into two streams, index each of
&gt; B&gt; them individually, then do 'extkeys/optimize/purge' on both.
&gt; 
&gt;    No, the extkeys/etc stuff only has to be done once if you're building
&gt;    one index to hold more than one type of files.
</pre>

<p>
You're right, of course.
 
</p>

<pre>
&gt; B&gt; I suppose you could use "-fx" to keep the modifications really simple:
&gt; B&gt; just add something like '-fx htm* H@"lynx -dump -nolist"' to the
&gt; B&gt; "estcmd gather" line... but if you're going to do that, you might as
&gt; B&gt; well set up processing for all the other "interesting" types of files:
&gt; B&gt; PDFs, RTFs, OpenOffice files, etc.
&gt; 
&gt;    And this is where I found the memory problem mentioned in the original
&gt;    article, not to mention all sorts of MS/Adobe files which aren't
&gt;    handled well by rtf2txt, antiword, xls2csv, and pdftotext.  I finally
&gt;    had to resort to running "strings" on lots of things and hoping for
&gt;    the best.  That's what the "locword" entry does in the example below.
</pre>

<p>
I did wonder about that. The way I saw it, trying to convert all the
PDFs at once would really play hell on my poor underpowered laptop. <img src="../gx/smile.png" alt=":)">
So, I didn't actually go into indexing all the PDFs and such, although I
did a couple of small test runs just to see what it would be like. 
 
</p>

<pre>
&gt;    The approach that worked best (failed least) was to run "file -i" on a
&gt;    fileset to get the MIME types, and then make a few passes through the
&gt;    resulting list to index what I could.  Here's part of the script.
</pre>

<p>
That certainly makes sense. I figured that for a simple indexing run,
all you needed was a pipe of the sort I put together - but for anything
more complicated, you'd need tempfiles, for exactly the reason you've
stated (multiple passes.) I actually played around with that quite a bit
('tmp=`mktemp /tmp/searchXXXXXX`' plus 'trap "/bin/rm -rf $tmp" 0' are
my friends!), and found it useful.
 
[snipping script]
</p>

<p>
Thanks, Karl - I was actually going to write something like this for
myself later. This will give me a good start on it; possibly, it'll be
of help to any of our readers who have been following along with this.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_talkback.html#mb-indexing_files">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>

</p>
</b><br />
<b>Sat, 7 Feb 2009 20:22:53 -0500 (EST)</b>
</p>

<pre>
&gt;&gt; On Fri, 6 Feb 2009 19:30:21 -0500,
&gt;&gt; Ben Okopnik &lt;ben@linuxgazette.net&gt; said:
</pre>

<p>
B&gt; The way I saw it, trying to convert all the PDFs at once would really play
B&gt; hell on my poor underpowered laptop. <img src="../gx/smile.png" alt=":)"> So, I didn't actually go into
B&gt; indexing all the PDFs and such, although I did a couple of small test runs
B&gt; just to see what it would be like.
</p>

<p>
   My ideal setup (not there yet, but I'm inching closer) is to have two
   distinct filetrees on a workstation or server.  The first tree would
   be /, /usr, /src -- all the junk we know and love.  The second tree
   (call it /shadow for now) would have drafts for most files under the
   first tree.  (If you didn't see the first Estraier article, drafts -- or
   ".est" files -- are the guts of the system; they hold the stuff that's
   actually indexed.)
</p>

<p>
   I don't much like databases for search/retrieval because it's not a
   really great fit.  I don't like millions of tiny files, either; if you
   go down hard and have to run fsck, you not only have time for coffee,
   you can go to Columbia and pick the beans.  My compromise looks like this:
</p>

<p>
   * Create 256 directories under /shadow using hex digits 00-ff.
     Each directory has at most 256 zip files named the same way.
</p>

<p>
   * Create a draft for any regular file of interest.  One of the attributes
     in each draft will be the hash of the contents of the file being
     indexed.  The MD5 hash of the filename without newline determines
     where the draft file will go.  For example, we index /etc/motd like so:
</p>

<p>
       me% echo /etc/motd | tr -d '\012' | md5sum
       b3097c3f6cd13df91fac6e56735da0b6  -
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ &lt;-- draft-filename
       ^^^^  &lt;-- directory
</p>

<p>
       me% md5sum /etc/motd
       58d9f375623df94a2b26b0bcddb20e3d  /etc/motd
</p>

<p>
     The file /shadow/b3/09.zip will hold a draft called 7c3f...b6.est.
     7c3f...b6.est holds all the interesting stuff about /etc/motd: keywords,
     last modification time, and an attribute that holds a signature of the
     file contents:
</p>

<p>
       @sig=58d9f375623df94a2b26b0bcddb20e3d
</p>

<p>
   This way, we can go directly from any "interesting" file on the system
   to its corresponding draft by looking in no more than one zipfile,
   and the draft doesn't have to be updated or reindexed for searching
   unless the original file's contents have changed.
</p>

<p>
   I want something that will scale up to tens of millions of indexed files.
   I did a few experiments with this, and the 1.6 million files on my
   workstation could fit into 64k zipfiles with an average of 25 drafts per
   archive.  My home directory has ~17,400 files taking up ~300 Mbytes.
   Unpacking and zipping the equivalent draft files takes up 9.1 Mbytes
   (about 3% of the original file space) if you don't mind doing without
   phrase searches.
</p>

<p>
   The current fad seems to be "consolidating" people's working files on
   some massive central server for searching, which is dumb on so many
   levels; crossing a network to get files that should be local, having a
   nice juicy single point of failure, etc.  If you want to search files
   without generating enough heat to boil the nearest body of water,
   put the draft files on the central server and index <strong>those</strong> instead.
</p>

<pre>-- 
Karl Vogel                      I don't speak for the USAF or my company
The outpatients are out in force tonight, I see.        --Tom Lehrer
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_talkback.html#mb-indexing_files">Back</a><hr width="50%" align="left" /><p><br /></p></div>
</body>
</html>