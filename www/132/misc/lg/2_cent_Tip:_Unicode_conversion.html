<!DOCTYPE html
	PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
	 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
<title>Mailbag::Thread</title>
<link rel="stylesheet" type="text/css" href="../../../lg.css" />
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
</head>
<body>
<a href="../../../"><img alt="Linux Gazette" src="../../../gx/2003/newlogo-blank-200-gold2.jpg" id="logo" /></a><img alt="Tux" src="../../../gx/tux_86x95_indexed.png" id="tux" /><p id="fun">...making Linux just a little more fun!</p><div class='content articlecontent'><a name="top"></a><h3>2-cent Tip: Unicode conversion</h3>
<p>
<b>Benjamin A. Okopnik [ben at linuxgazette.net]</b><br />
<b>Mon, 4 Sep 2006 15:27:16 -0400</b>
</p>

<p>
A couple of years ago, I decided to stop wrestling with what I call
"encoding craziness" for various bits of non-English text that I have
scattered around my file system. Russian, for example, has at least four
different encodings that I've run into - and guessing which one a given
text file was written in was like a game of darts played in the dark. At
300 yards. With your hands tied behind you, so you had to use your toes.
Oh, and while you were severely drunk on Stoli vodka. <img src="../gx/smile.png" alt=":)"> UTF-8 (Unicode)
allowed me to, well, unify all of that into one single encoding that was
readable without scrambling for whichever character set I needed (and
may or may not have installed.) Better yet, Unicode usually displays
just fine in HTML browsers - no special entity encoding is required.
</p>

<p>
For some reason, though, good converters appear to be something of a
black art - and finding one that works, as opposed to all those that
<strong>claim</strong> to work, was rather frustrating. Therefore, I decided to write
one in my favorite language, Perl - only to find that the job has
already been done for me, via the 'encoding' pragma. In other words,
conversion from, say, KOI8-R to UTF-8 is no more complex than this:
</p>

<p>
<pre>
# Convert and write to a file
perl -Mencoding=koi8r,STDOUT,utf8 -pe0 &lt; file.koi8r &gt; file.utf8
# Or just display it in a pager:
perl -Mencoding=koi8r,STDOUT,utf8 -pe0 &lt; file.koi8r|less
</pre>
It is literally that simple. Pretty much every encoding you can imagine
is available (see 'perldoc Encode::Supported' for the naming conventions
and charsets). The conversion does not have to be to UTF-8 - it'll do
any of the listed charsets - but why would you care? <img src="../gx/smile.png" alt=":)">
</p>

<p>
<pre>
# Print the Kanji for 'Rakuda' (Camel) from multibyte strings:
perl -Mencoding=euc-jp,STDOUT,utf-8 -wle'print "Follow the
\xF1\xD1\xF1\xCC!"'
Follow the 駱駝!
# Or you can do it in Hiragana, but using Unicode values instead:
perl -Mencoding=shift-jis,STDOUT,utf8 -wle'print "Follow the
\x{3089}\x{304F}\x{3060}!"'
Follow the らくだ!
</pre>
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</p>


<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_tips.html#mb-2_cent_Tip:_Unicode_conversion">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b>Kapil Hari Paranjape [kapil at imsc.res.in]</b><br />
<b>Tue, 5 Sep 2006 08:36:25 +0530</b>
</p>

<p>
On Mon, 04 Sep 2006, Benjamin A. Okopnik wrote:
</p>

<p>
<pre>
&gt; For some reason, though, good converters appear to be something of a
&gt; black art - and finding one that works, as opposed to all those that
&gt; <strong>claim</strong> to work, was rather frustrating.
</pre>
</p>

<p>
Doesn't 'tcs' work properly? This program is in a Debian package by
the same name. I don't use multiple languages on the computer (which
may be surprising given where I am) so I haven't had a chance to
check.
</p>

<p>
Regards,
</p>

<p>
Kapil.
--
</p>



<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_tips.html#mb-2_cent_Tip:_Unicode_conversion">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b>Benjamin A. Okopnik [ben at linuxgazette.net]</b><br />
<b>Mon, 4 Sep 2006 23:46:53 -0400</b>
</p>

<p>
On Tue, Sep 05, 2006 at 08:36:25AM +0530, Kapil Hari Paranjape wrote:
</p>

<p>
<pre>
&gt; 
&gt; On Mon, 04 Sep 2006, Benjamin A. Okopnik wrote:
&gt; &gt; For some reason, though, good converters appear to be something of a
&gt; &gt; black art - and finding one that works, as opposed to all those that
&gt; &gt; <strong>claim</strong> to work, was rather frustrating.
&gt; 
&gt; Doesn't 'tcs' work properly? This program is in a Debian package by
&gt; the same name. I don't use multiple languages on the computer (which
&gt; may be surprising given where I am) so I haven't had a chance to
&gt; check.
</pre>
</p>

<p>
I hadn't run across it before, but it seems a bit limited:
</p>

<p>
<pre>
ben at Fenrir:~$ apt-cache show tcs|grep '^ '
 tcs translates character sets from one encoding to another.
 .
 Supported encodings include utf (ISO utf-8), ascii, ISO 8859-[123456789],
 koi8, jis-kanji, ujis, ms-kanji, jis, gb, big5, unicode, tis, msdos, and
 atari.
</pre>
Just looking at the ISO8859* list, it's already missing the Nordic
languages, Thai, the Baltics, the Celtics, and Latin 9 and 10. The
solution that I suggested can handle all of those and a much broader
range besides - it includes 9 variants of just Unicode alone.
</p>


<p>
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</p>


<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_tips.html#mb-2_cent_Tip:_Unicode_conversion">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b>John Karns [johnkarns at gmail.com]</b><br />
<b>Wed, 6 Sep 2006 19:53:12 -0500 (COT)</b>
</p>

<p>
On Mon, 4 Sep 2006, Benjamin A. Okopnik wrote:
</p>


<p>
<pre>
&gt; A couple of years ago, I decided to stop wrestling with what I call
&gt; "encoding craziness" for various bits of non-English text that I have
&gt; scattered around my file system.
</pre>
</p>

<p>
[snipped some entertaining imagery of a drunk Russian throwing darts with
his toes] ...
</p>


<p>
<pre>
&gt; UTF-8 (Unicode) allowed me to, well, unify all of that into one single
&gt; encoding that was readable without scrambling for whichever character
&gt; set I needed (and may or may not have installed.) Better yet, Unicode
&gt; usually displays just fine in HTML browsers - no special entity encoding
&gt; is required.
</pre>
</p>

<p>
This leads me to some questions about character sets.  Sorry about the
over-long text, but I didn't want to omit any important details, and the
subject of fonts / charsets can get rather deep very quickly.
</p>

<p>
In the earlier thread here, "Copyright Notice", you quoted a phrase in 
German which had an umlated "u" (hope that's the right terminology) in one 
of the words.  My system (Ubuntu 5.10 [1]* with lots of added dev libs and 
other packages) is set up for utf-8, and the following env vars are set:
<pre>
    LESSCHARSET=utf-8
    LANG=en_US.UTF-8
    LANGUAGE=en_US.UTF-8
</pre>
---
</p>

<p>
Running locale on the system yields:
<pre>
    LANG=en_US.UTF-8
    LC_CTYPE="en_US.UTF-8"
    LC_NUMERIC="en_US.UTF-8"
    LC_TIME="en_US.UTF-8"
    LC_COLLATE="en_US.UTF-8"
    LC_MONETARY="en_US.UTF-8"
    LC_MESSAGES="en_US.UTF-8"
    LC_PAPER="en_US.UTF-8"
    LC_NAME="en_US.UTF-8"
    LC_ADDRESS="en_US.UTF-8"
    LC_TELEPHONE="en_US.UTF-8"
    LC_MEASUREMENT="en_US.UTF-8"
    LC_IDENTIFICATION="en_US.UTF-8"
    LC_ALL=
</pre>
The mutt pager correctly displays the non-ASCII character, as does vim.
But my pager "less" (ver 382), showed it as a blank space.  I thought
perhaps the installed version of "less"  was compiled w/o Unicode support,
so I DL'd the most recent source I could find (ver 394) and compiled it.
I looked for a Unicode related configuration option, but auto-make showed
no special options at all, so nothing there.  However it seems to come a
little closer by displaying the character as "&lt;FC&gt;" =&gt; "FC" in angle
brackets, so it's at least recognizing it as a two byte character.  Also,
looking at the packages source file, "charset.c", utf-8 is listed as an
element of the array "charset", so utf-8 it seems that it should support
it.
</p>

<pre>
Your message contained the following two headers:

    Content-Type: text/plain; charset="iso-8859-1"
    Content-Transfer-Encoding: quoted-printable
</pre>
<p>
So my questions are:
</p>

<p>
1) What's the difference between iso-8859-1 and utf-8?
</p>

<p>
    After Googling for an answer to that, I came across some info that led
    me to understand that the difference is 8-bit vs. 16-bit.  Does utf /
    Unicode then stand alone as 16-bit vs the rest of the character sets
    like 8859, latin[123], etc?
</p>

<p>
1a) What's the main advantage of choosing Unicode over the others?
</p>

<p>
    Intuition says that a 16-bit charset covers more ground than one of
    8-bits, which might lead to being a better bridge of the gap between
    more languages, in many cases alleviating the need to switch between
    different charsets when focusing on various different language
    character sets.  Am I close?  Any other reasons?
</p>

<p>
2) Is the bracketed "FC" an indication that it's attempting to display as
utf-8, and simply not finding the character set?  Or is it indicative of
something else, such as displaying it as some other character set?
</p>

<p>
3) Does it seem more likely that I'm overlooking a step in the compilation
configuration or missing some other needed env var for the run time env?
Or is this a known issue with GNU less?
</p>


<p>
[1]* works wonderfully on this aging Dell I8100, best I've ever had a
laptop run under Linux - at the end of the day I just suspend to RAM -
haven't done a shutdown or reboot now in 14 days!  Probably mostly thanks
to the improvements in the kernel suspend code, but they seem to have the
ACPI scripting functioning very well too.  Hibernate hasn't proven to be
quite as smooth though.
</p>

<p>
<pre>-- 
John Karns
</pre>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_tips.html#mb-2_cent_Tip:_Unicode_conversion">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b>Benjamin A. Okopnik [ben at linuxgazette.net]</b><br />
<b>Fri, 8 Sep 2006 22:47:59 -0400</b>
</p>

<p>
On Wed, Sep 06, 2006 at 07:53:12PM -0500, John Karns wrote:
</p>

<p>
<pre>
&gt; On Mon, 4 Sep 2006, Benjamin A. Okopnik wrote:
&gt; 
&gt; &gt; A couple of years ago, I decided to stop wrestling with what I call
&gt; &gt; "encoding craziness" for various bits of non-English text that I have
&gt; &gt; scattered around my file system.
&gt; 
&gt; [snipped some entertaining imagery of a drunk Russian throwing darts with
&gt; his toes] ...
&gt; 
&gt; &gt; UTF-8 (Unicode) allowed me to, well, unify all of that into one single
&gt; &gt; encoding that was readable without scrambling for whichever character
&gt; &gt; set I needed (and may or may not have installed.) Better yet, Unicode
&gt; &gt; usually displays just fine in HTML browsers - no special entity encoding
&gt; &gt; is required.
&gt; 
&gt; This leads me to some questions about character sets.  Sorry about the
&gt; over-long text, but I didn't want to omit any important details, and the
&gt; subject of fonts / charsets can get rather deep very quickly.
</pre>
</p>

<p>
Yes indeed. Now, I'd like to note right up front that I am anything but
an expert in fonts: I'm much more of a one-trick pony (although it's a
hell of a good trick.) The major reason for my "success" with the font
system is that I kept throwing myself against it until I understood
enough of what was going on to start adjusting the process. I'm not
quite up to that point with console fonts, though... one of these days,
I'll go leave some more forehead prints on those bricks.
</p>


<pre>
&gt; In the earlier thread here, "Copyright Notice", you quoted a phrase in 
&gt; German which had an umlated "u" (hope that's the right terminology) in one 
&gt; of the words.  My system (Ubuntu 5.10 [1]* with lots of added dev libs and 
&gt; other packages) is set up for utf-8, and the following env vars are set:
&gt; 
&gt;     LESSCHARSET=utf-8
&gt;     LANG=en_US.UTF-8
&gt;     LANGUAGE=en_US.UTF-8
&gt; 
&gt; ---
&gt; 
&gt; Running locale on the system yields:
&gt; 
&gt;     LANG=en_US.UTF-8
&gt;     LC_CTYPE="en_US.UTF-8"
&gt;     LC_NUMERIC="en_US.UTF-8"
&gt;     LC_TIME="en_US.UTF-8"
&gt;     LC_COLLATE="en_US.UTF-8"
&gt;     LC_MONETARY="en_US.UTF-8"
&gt;     LC_MESSAGES="en_US.UTF-8"
&gt;     LC_PAPER="en_US.UTF-8"
&gt;     LC_NAME="en_US.UTF-8"
&gt;     LC_ADDRESS="en_US.UTF-8"
&gt;     LC_TELEPHONE="en_US.UTF-8"
&gt;     LC_MEASUREMENT="en_US.UTF-8"
&gt;     LC_IDENTIFICATION="en_US.UTF-8"
&gt;     LC_ALL=
</pre>
 
<p>
That's exactly what mine reports.
</p>

<pre>
&gt; The mutt pager correctly displays the non-ASCII character, as does vim.
&gt; But my pager "less" (ver 382), showed it as a blank space.  I thought
&gt; perhaps the installed version of "less"  was compiled w/o Unicode support,
&gt; so I DL'd the most recent source I could find (ver 394) and compiled it.
&gt; I looked for a Unicode related configuration option, but auto-make showed
&gt; no special options at all, so nothing there.  However it seems to come a
&gt; little closer by displaying the character as "&lt;FC&gt;" =&gt; "FC" in angle
&gt; brackets, so it's at least recognizing it as a two byte character.
</pre>

<p>
Have you checked it against Markus Kuhn's UTF-8 test file? It's
available at <a href="http://www.w3.org/2001/06/utf-8-test/UTF-8-demo.html">http://www.w3.org/2001/06/utf-8-test/UTF-8-demo.html</a> (among
other places), and is my first step in troubleshooting UTF-8 problems.
The second step is to make sure that I have some good UTF-8 (iso10646)
fonts installed (although these days, you get them as part of the
'xfonts-base' kit), and that my xterms use them as the default fonts.
This may still be of use to anyone still using Xfree86, though:
</p>

<pre>
ben at Fenrir:~$ egrep '^xterm.*(utf8|font)' .Xresources 
xterm*font: -misc-fixed-medium-r-normal--18-120-100-100-c-90-iso10646-1
xterm*utf8:1
ben at Fenrir:~$ grep -- '-misc-fixed-medium-r-normal--18-120-100-100-c-90-iso10646-1' /usr/share/fonts/X11/misc/fonts.dir 
9x18.pcf.gz -misc-fixed-medium-r-normal--18-120-100-100-c-90-iso10646-1
ben at Fenrir:~$ dlocate 9x18.pcf.gz
xfonts-base: /usr/share/fonts/X11/misc/9x18.pcf.gz
</pre>

<pre>
&gt;  Also,
&gt; looking at the packages source file, "charset.c", utf-8 is listed as an
&gt; element of the array "charset", so utf-8 it seems that it should support
&gt; it.
&gt; 
&gt; Your message contained the following two headers:
&gt; 
&gt;     Content-Type: text/plain; charset="iso-8859-1"
&gt;     Content-Transfer-Encoding: quoted-printable
&gt; 
&gt; 
&gt; So my questions are:
&gt; 
&gt; 1) What's the difference between iso-8859-1 and utf-8?
</pre>

<p>
The first one is the default charset for Latin-1; the interesting bit in
comparing the two is that the first 256 chars of both ("codepoints", in
Unicode parlance) are identical.
</p>

<p>
<pre>
&gt;     After Googling for an answer to that, I came across some info that led
&gt;     me to understand that the difference is 8-bit vs. 16-bit.  Does utf /
&gt;     Unicode then stand alone as 16-bit vs the rest of the character sets
&gt;     like 8859, latin[123], etc?
</pre>
</p>

<p>
There's a lot more to it; it's a big, complex issue. You can find a good
explanation of it under 'perldoc perlunicode', in the "Unicode Regular
Expression Support Level" section. Just as a starting point, UTF-8 is
variable-length (1 to 6 bytes), with currently-allocated characters
requiring a 4-byte space. Also, take a look at 'perldoc
Encode::Supported', which delineates the differences between the UTF*
charsets.
</p>

<pre>
&gt; 1a) What's the main advantage of choosing Unicode over the others?
&gt; 
&gt;     Intuition says that a 16-bit charset covers more ground than one of
&gt;     8-bits, which might lead to being a better bridge of the gap between
&gt;     more languages, in many cases alleviating the need to switch between
&gt;     different charsets when focusing on various different language
&gt;     character sets.  Am I close?  Any other reasons?
</pre>

<p>
That's the main reason I use it. There are others - e.g., one single
charset that everybody can use for a reference would eliminate all the
conversion errors and related communication problems in one swell foop
(note that this does not mean <em>all</em> communication problems by far - but
certainly a large class of them.) That, all by itself, is an invaluable
benefit.
</p>


<pre>
&gt; 2) Is the bracketed "FC" an indication that it's attempting to display as
&gt; utf-8, and simply not finding the character set?  Or is it indicative of
&gt; something else, such as displaying it as some other character set?
</pre>

<p>
I suspect it means that you're not actually using UTF-8. 2-byte Unicode
characters, when read by something that's not Unicode-aware, look like -
surprise! - two characters. Just to check:
</p>

<pre>
ben at Fenrir:~$ perl -Mencoding=latin2,STDOUT,utf8 -wle 'print "\xFC"'
?
</pre>

<p>
Yep, that's the dude. At least in UTF-8. <img src="../gx/smile.png" alt=":)"> In ASCII, he's just plain
old 'FC'.
 
</p>

<pre>
&gt; 3) Does it seem more likely that I'm overlooking a step in the compilation
&gt; configuration or missing some other needed env var for the run time env?
&gt; Or is this a known issue with GNU less?
</pre>
 
<p>
Not in my case; it displays the above umlaut just fine when I hang it
off the right side of a pipe.
</p>

<pre>
ben at Fenrir:~$ less --version
less 394
Copyright (C) 1984-2005 Mark Nudelman
</pre>
<p>
As I recall from way back when I started this "font adventure", there
was some oddity in X that, despite the proper settings for LC_CTYPE,
etc., it just would not do the right thing - until I put
</p>

<pre>
export LANG=en_US.UTF-8
</pre>
<p>
into my '~/.xinitrc'. I don't know if that's still applicable, but I've
still got it in there.
</p>

<pre>
&gt; [1]* works wonderfully on this aging Dell I8100, best I've ever had a
&gt; laptop run under Linux - at the end of the day I just suspend to RAM -
&gt; haven't done a shutdown or reboot now in 14 days!  Probably mostly thanks
&gt; to the improvements in the kernel suspend code, but they seem to have the
&gt; ACPI scripting functioning very well too.  Hibernate hasn't proven to be
&gt; quite as smooth though.
</pre>

<p>
Mine just don't work, period. Bleagh. :((( To the best of my ability to
figure it out, the ACPI on this Acer 2012 is <em>so</em> horrendously broken
that it's not even worth trying to fix (although I'd downloaded Intel's
ACPI compiler/decompiler, dutifully fixed all the errors, and shoved it
all back in, it didn't seem to make any difference.) Well, this laptop
is getting toward the end of its useful life... we'll see how the next
one goes.
</p>


<p>
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</p>


<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_tips.html#mb-2_cent_Tip:_Unicode_conversion">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b>Francis Daly [francis at daoine.org]</b><br />
<b>Sat, 9 Sep 2006 15:57:56 +0100</b>
</p>

<p>
On Fri, Sep 08, 2006 at 10:47:59PM -0400, Benjamin A. Okopnik wrote:
</p>

<p>
<pre>
&gt; On Wed, Sep 06, 2006 at 07:53:12PM -0500, John Karns wrote:
&gt; &gt; On Mon, 4 Sep 2006, Benjamin A. Okopnik wrote:
</pre>
</p>

<p>
Hi there,
</p>

<p>
Minor correction, or possibly "enhanced explanation"...
</p>


<p>
<pre>
&gt; Now, I'd like to note right up front that I am anything but
&gt; an expert in fonts: 
</pre>
</p>

<p>
...from someone in a similar position.
</p>


<p>
<pre>
&gt; &gt; Running locale on the system yields:
&gt; &gt; 
&gt; &gt;     LANG=en_US.UTF-8
&gt; &gt;     LC_CTYPE="en_US.UTF-8"
&gt; &gt;     LC_NUMERIC="en_US.UTF-8"
&gt; &gt;     LC_TIME="en_US.UTF-8"
&gt; &gt;     LC_COLLATE="en_US.UTF-8"
&gt; &gt;     LC_MONETARY="en_US.UTF-8"
&gt; &gt;     LC_MESSAGES="en_US.UTF-8"
&gt; &gt;     LC_PAPER="en_US.UTF-8"
&gt; &gt;     LC_NAME="en_US.UTF-8"
&gt; &gt;     LC_ADDRESS="en_US.UTF-8"
&gt; &gt;     LC_TELEPHONE="en_US.UTF-8"
&gt; &gt;     LC_MEASUREMENT="en_US.UTF-8"
&gt; &gt;     LC_IDENTIFICATION="en_US.UTF-8"
&gt; &gt;     LC_ALL=
&gt;  
&gt; That's exactly what mine reports.
</pre>
</p>

<p>
I have something analogous, except that I set LC_COLLATE to C because I
think ABC looks better than AaB, and I like [a-z] to mean what I think
it means.
</p>


<p>
<pre>
&gt; &gt; The mutt pager correctly displays the non-ASCII character, as does vim.
&gt; &gt; But my pager "less" (ver 382), showed it as a blank space.  I thought
</pre>
</p>

<p>
The mutt pager has (in theory at least -- I haven't checked for real)
access to the Content-Type: of the text, so it can know to interpret it
as iso-8859-1. vim probably auto-guesses the content encoding (which is
not unreasonable for it to do, absent other instructions). less heeds
the other instructions from the environment, and prints what it sees --
the single octet 0xFC. That's invalid UTF-8, and so it may have chosen
to display it as broken or unknown -- "od -xc" or "cat -v" might have
shown an alternate view of the same character.
</p>


<p>
<pre>
&gt; &gt; no special options at all, so nothing there.  However it seems to come a
&gt; &gt; little closer by displaying the character as "&lt;FC&gt;" =&gt; "FC" in angle
&gt; &gt; brackets, so it's at least recognizing it as a two byte character.
</pre>
</p>

<p>
The single octet 0xFC is not a valid UTF-8 character -- it is part of a
multibyte sequence, which is almost certainly not correctly completed in
your test file. The single octet 0xFC is a valid iso-8859-1 character,
and it is u-umlaut. "man ascii" for the first-bit-zero characters (which,
by not-quite-luck, are the same in ASCII, iso-8859-1, and UTF-8); "man
iso_8859_1" for (most of) the first-bit-one characters in that encoding;
and "man utf-8" for a description of how the first-bit-one characters
are encoded in two octets in that encoding.
</p>

<p>
Your test above doesn't show it reading a two-byte character, by the way
-- it read one byte, and wasn't able to display it properly, so showed
you the hex-encoding of it.
</p>

<p>
("wasn't able to display" is probably because it was invalid UTF-8. But
it could have been because your less didn't have access to a font that
had the right thing for character 252. Unlikely, I'd guess.)
</p>


<p>
<pre>
&gt; &gt; So my questions are:
&gt; &gt; 
&gt; &gt; 1) What's the difference between iso-8859-1 and utf-8?
&gt; 
&gt; The first one is the default charset for Latin-1; the interesting bit in
&gt; comparing the two is that the first 256 chars of both ("codepoints", in
&gt; Unicode parlance) are identical.
</pre>
</p>

<p>
Just to clarify (hopefully): the characters 0 - 127 are the single octets
0x00 - 0x7F and are identical in each; the characters 128 - 255 are the
same in each (not worrying about the ones below 160), but they are a
single octet in iso-8859-1 and two octets in UTF-8.
</p>

<p>
So: Latin-1 says "character number 252 is u-umlaut". Unicode says
"character number 252 is u-umlaut". iso-8859-1 says "character number
252 is the octet with decimal value 252, or 0xfc". UTF-8 says "character
number 252 is the two octets 0xc3 0xbc".
</p>

<p>
If you're given an octet stream comprising 0xc3 0xbc, and you know by
some other means that the stream is UTF-8, you can print u-umlaut. If
you know by some other means that the stream is iso-8859-1, you can
print A-tilde one-quarter (which is how those two characters would be
encoded). If you don't know how the stream is encoded, you can flag that
fact, or you can try guessing.
</p>

<p>
This is kind of the reverse of what you reported, where the octet stream
contained 0xfc -- them that knew it was the right encoding could display
it; them that guessed right could display it; them that knew it was the
wrong encoding, or guessed wrong, or didn't guess, couldn't display it
"correctly", and so made do.
</p>

<p>
Hopefully that explains why things looked the way they did.
</p>


<p>
<pre>
&gt; There's a lot more to it; it's a big, complex issue. 
</pre>
</p>

<p>
Oh so true.
</p>


<p>
<pre>
&gt; &gt; 2) Is the bracketed "FC" an indication that it's attempting to display as
&gt; &gt; utf-8, and simply not finding the character set?  Or is it indicative of
&gt; &gt; something else, such as displaying it as some other character set?
&gt; 
&gt; I suspect it means that you're not actually using UTF-8. 2-byte Unicode
&gt; characters, when read by something that's not Unicode-aware, look like -
&gt; surprise! - two characters. Just to check:
</pre>
</p>

<p>
My guess, as above, is that the content was encoded as iso-8859-1,
but that less was told that it had been encoded as UTF-8.
</p>


<p>
<pre>
&gt; ``
&gt; ben at Fenrir:~$ perl -Mencoding=latin2,STDOUT,utf8 -wle 'print "\xFC"'
&gt; ?
&gt; ''
&gt; 
&gt; Yep, that's the dude. At least in UTF-8. <img src="../gx/smile.png" alt=":)"> In ASCII, he's just plain
&gt; old 'FC'.
</pre>
</p>

<p>
ASCII stops at 7F. In iso-8859-1 (and probably some others too) he's FC.
</p>


<p>
<pre>
&gt; &gt; 3) Does it seem more likely that I'm overlooking a step in the compilation
&gt; &gt; configuration or missing some other needed env var for the run time env?
&gt; &gt; Or is this a known issue with GNU less?
</pre>
</p>

<p>
od can show you what the octets are.
</p>

<p>
Visual examination of the octets can make a fair guess at what the
encoding really is. Tell less that that is the charset (man less for
the various ways of doing that; LESSCHARSET is probably the easiest),
and it should display it correctly.
</p>

<p>
Good luck,
</p>

<p>
	f
<pre>-- 
Francis Daly        francis at daoine.org
</pre>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_tips.html#mb-2_cent_Tip:_Unicode_conversion">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b>John Karns [johnkarns at gmail.com]</b><br />
<b>Sun, 10 Sep 2006 22:30:32 -0500 (COT)</b>
</p>

<p>
On Sat, 9 Sep 2006, Francis Daly wrote:
</p>


<p>
<pre>
&gt; On Fri, Sep 08, 2006 at 10:47:59PM -0400, Benjamin A. Okopnik wrote:
&gt;&gt; On Wed, Sep 06, 2006 at 07:53:12PM -0500, John Karns wrote:
&gt;&gt;&gt; On Mon, 4 Sep 2006, Benjamin A. Okopnik wrote:
&gt;
&gt; Hi there,
</pre>
</p>

<p>
Hello!
</p>



<p>
<pre>
&gt; I have something analogous, except that I set LC_COLLATE to C because I
&gt; think ABC looks better than AaB, and I like [a-z] to mean what I think
&gt; it means.
</pre>
</p>

<p>
Nice tip!
</p>



<p>
<pre>
&gt;&gt;&gt; The mutt pager correctly displays the non-ASCII character, as does vim.
&gt;&gt;&gt; But my pager "less" (ver 382), showed it as a blank space.  I thought
&gt;
&gt; The mutt pager has (in theory at least -- I haven't checked for real)
&gt; access to the Content-Type: of the text, so it can know to interpret it
&gt; as iso-8859-1. vim probably auto-guesses the content encoding (which is
&gt; not unreasonable for it to do, absent other instructions). less heeds
&gt; the other instructions from the environment, and prints what it sees --
&gt; the single octet 0xFC. That's invalid UTF-8, and so it may have chosen
&gt; to display it as broken or unknown -- "od -xc" or "cat -v" might have
&gt; shown an alternate view of the same character.
&gt;
&gt;&gt;&gt; no special options at all, so nothing there.  However it seems to come a
&gt;&gt;&gt; little closer by displaying the character as "&lt;FC&gt;" =&gt; "FC" in angle
&gt;&gt;&gt; brackets, so it's at least recognizing it as a two byte character.
&gt;
&gt; The single octet 0xFC is not a valid UTF-8 character -- it is part of a
&gt; multibyte sequence, which is almost certainly not correctly completed in
&gt; your test file. The single octet 0xFC is a valid iso-8859-1 character,
&gt; and it is u-umlaut. "man ascii" for the first-bit-zero characters (which,
&gt; by not-quite-luck, are the same in ASCII, iso-8859-1, and UTF-8); "man
&gt; iso_8859_1" for (most of) the first-bit-one characters in that encoding;
</pre>
</p>

<p>
This does indeed seem to be the case.
</p>



<p>
<pre>
&gt; and "man utf-8" for a description of how the first-bit-one characters
&gt; are encoded in two octets in that encoding.
</pre>
</p>

<p>
So in essence you're saying that text of the original message was in fact
not utf-8, but 8859-1, as was specified in the "content type" header of
that message.  Makes sense.
</p>



<p>
<pre>
&gt; Your test above doesn't show it reading a two-byte character, by the way
&gt; -- it read one byte, and wasn't able to display it properly, so showed
&gt; you the hex-encoding of it.
</pre>
</p>

<p>
Yeah, I was getting laid up on thinking in terms of a sequence of two
ascii chars rather than a hex
</p>



<p>
<pre>
&gt; ("wasn't able to display" is probably because it was invalid UTF-8. But
&gt; it could have been because your less didn't have access to a font that
&gt; had the right thing for character 252. Unlikely, I'd guess.)
</pre>
</p>

<p>
As you suggest, it appears not to be the case, since the same less binary
can display the character correctly in the man page.
</p>



<p>
<pre>
&gt; So: Latin-1 says "character number 252 is u-umlaut". Unicode says
&gt; "character number 252 is u-umlaut". iso-8859-1 says "character number
&gt; 252 is the octet with decimal value 252, or 0xfc". UTF-8 says "character
&gt; number 252 is the two octets 0xc3 0xbc".
&gt;
&gt; If you're given an octet stream comprising 0xc3 0xbc, and you know by
&gt; some other means that the stream is UTF-8, you can print u-umlaut. If
&gt; you know by some other means that the stream is iso-8859-1, you can
&gt; print A-tilde one-quarter (which is how those two characters would be
&gt; encoded). If you don't know how the stream is encoded, you can flag that
&gt; fact, or you can try guessing.
&gt;
&gt; This is kind of the reverse of what you reported, where the octet stream
&gt; contained 0xfc -- them that knew it was the right encoding could display
&gt; it; them that guessed right could display it; them that knew it was the
&gt; wrong encoding, or guessed wrong, or didn't guess, couldn't display it
&gt; "correctly", and so made do.
&gt;
&gt; Hopefully that explains why things looked the way they did.
</pre>
</p>

<p>
[snip]
</p>

<p>
Your explanation seems to be accurate, at least as far as agreeing with
the results of my experiments.  I'll take your word for it about the text
arriving in my mailbox as 8859 encoding,
</p>

<p>
I tried different methods of "exporting" the text from both pine and mutt.
Pine offers an "export" option, as well as piping the text "raw text" or
"free output".  Depending on how you do it, the results vary for what I'll
call the "extended character".  The 1st method translates the char to "FC"
as I noted in my my original post.  The 2nd ("raw text") translates it to
a byte sequence, "ox3d 0x46 0x43" - as viewed within the hex display mode
of the mc viewer.
</p>

<p>
Viewing the resultant files with less does not render the character as
u-umlaut in any instance, including piping the text to "cat -v filename"
(written as the same three octet sequence as the 2nd case above.  Piping
to "od -xc" agrees with the cat -v output.  However after editting /
re-writing the text file created from Pine's export option using vim, the
character became "0xc3 0xbc", which "less" correctly renders as u-umlaut,
and oddly enough (to me), irrespective of the presence of the var
LESSCHARSET.  And if set, the result is the same whether it's set to
iso8859 or utf-8.
</p>

<p>
Thanks to both Ben and Francis for helping to make this issue a little
bit clearer.
</p>

<p>
<pre>-- 
John Karns
</pre>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_tips.html#mb-2_cent_Tip:_Unicode_conversion">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b>Benjamin A. Okopnik [ben at linuxgazette.net]</b><br />
<b>Mon, 11 Sep 2006 00:26:28 -0400</b>
</p>

<p>
On Sun, Sep 10, 2006 at 10:30:32PM -0500, John Karns wrote:
</p>

<p>
<pre>
&gt; 
&gt; Your explanation seems to be accurate, at least as far as agreeing with
&gt; the results of my experiments.  I'll take your word for it about the text
&gt; arriving in my mailbox as 8859 encoding,
&gt; 
&gt; I tried different methods of "exporting" the text from both pine and mutt.
&gt; Pine offers an "export" option, as well as piping the text "raw text" or
&gt; "free output".  Depending on how you do it, the results vary for what I'll
&gt; call the "extended character".  The 1st method translates the char to "FC"
&gt; as I noted in my my original post.  The 2nd ("raw text") translates it to
&gt; a byte sequence, "ox3d 0x46 0x43" - as viewed within the hex display mode
&gt; of the mc viewer.
</pre>
</p>

<p>
In Mutt, you can edit the Content-Type with '^E'. In my case, when I
look at that email, I see the '?' just fine - but when I hit the 'v'
key, the listing for that attachment looks like this:
</p>

<p>
<pre>
 I     1 &lt;no description&gt;  			[text/plain, 8bit, iso-8859-1, 1.4K]
</pre>
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</p>


<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_tips.html#mb-2_cent_Tip:_Unicode_conversion">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b>Francis Daly [francis at daoine.org]</b><br />
<b>Mon, 11 Sep 2006 11:34:38 +0100</b>
</p>

<p>
On Sun, Sep 10, 2006 at 10:30:32PM -0500, John Karns wrote:
</p>


<p>
<pre>
&gt; I tried different methods of "exporting" the text from both pine and mutt.
&gt; Pine offers an "export" option, as well as piping the text "raw text" or
&gt; "free output".  Depending on how you do it, the results vary for what I'll
&gt; call the "extended character".  The 1st method translates the char to "FC"
&gt; as I noted in my my original post.  The 2nd ("raw text") translates it to
&gt; a byte sequence, "ox3d 0x46 0x43" - as viewed within the hex display mode
&gt; of the mc viewer.
</pre>
</p>

<p>
The difference between those two is related to the joy of SMTP.
</p>

<p>
RFC 821 from 1982 said "SMTP data is 7-bit ASCII characters". Although
that RFC has been obsoleted, SMTP cannot reliably be considered 8-bit
clean.
</p>

<p>
RFC 2821 from 2001 adds "Service extensions may modify this rule to permit
transmission of full 8-bit data bytes as part of the message body",
and the 8BITMIME extension seem to be from 1995 or earlier, but all of
that requires someone else's machine to be sane.
</p>

<p>
So rather than ship high-bit-set data and trust the SMTP servers to do
the right[*] thing, the mail creator can encode the content to 7-bit data
using one of a variety of methods.
</p>

<p>
[*] FSVO.
</p>

<p>
uuencode is a handy way -- I often use it for transferring small files
from my machine -- "uuencode filename filename | xsel" on my machine,
"uudecode" in a shell on remote and x-paste the selection. (C-a [ and
C-a ] within screen can be used too, of course.)
</p>

<p>
base64-encoding is also used, and shares with uuencode the fact that
all of the content is unreadable without decoding.
</p>

<p>
A more common current way is officially called quoted-printable, but
is also known by less complimentary terms. That was the one used here,
according to headers sent earlier:
</p>

<pre>
    Content-Type: text/plain; charset="iso-8859-1"
    Content-Transfer-Encoding: quoted-printable
</pre>

<p>
The details aren't too important, but it turns one high-bit-set octet
into three high-bit-unset octets, the first one of which is "=", which
happens to be the 0x3d that you saw.
</p>

<p>
Url-encoding is another way of turning 8-bit into 7-bit characters,
which is similar to quoted-printable, except that it uses % not = as its
"here comes something different" character.
</p>

<p>
Those two share the feature that all ASCII content can be read without
decoding, and if you only expect a handful of accented characters,
you can probably get used to recognising them in their encoded form.
</p>

<p>
So apparently "raw text" is "the octets that were sent via SMTP",
and "free output" is "the content-transfer-decoded version of same"
(possibly with some extra MIME-decoding too -- but you'll only see that
clearly if more than one attachment was included originally).
</p>


<p>
<pre>
&gt; Viewing the resultant files with less does not render the character as
&gt; u-umlaut in any instance, including piping the text to "cat -v filename"
&gt; (written as the same three octet sequence as the 2nd case above.  
</pre>
</p>

<p>
The "raw text" should contain three characters "=FC", while "free output"
should contain a single octet 0xfc which is invalid UTF-8 unless followed
by five octets with the high bits 10 (and that's beyond the range of
Unicode). So that's to be expected -- there's no u-umlaut there, unless
the viewer knows that it is iso-8859-1 encoded.
</p>


<p>
<pre>
&gt; Piping
&gt; to "od -xc" agrees with the cat -v output.  However after editting /
&gt; re-writing the text file created from Pine's export option using vim, the
&gt; character became "0xc3 0xbc", which "less" correctly renders as u-umlaut,
</pre>
</p>

<p>
So far so good -- vim has written the output as UTF-8 whatever is input was.
</p>


<p>
<pre>
&gt; and oddly enough (to me), irrespective of the presence of the var
&gt; LESSCHARSET.  And if set, the result is the same whether it's set to
&gt; iso8859 or utf-8.
</pre>
</p>

<p>
That bit is a bit odd. With LESSCHARSET unset, the manpage says it will
check some other envariables including LANG, which for you makes it
UTF-8. But setting LESSCHARSET to iso8859 <strong>should</strong> cause it to display
as A-tilde one-quarter, if I read the manpage right.
</p>

<p>
Doesn't work for me either, by the way. Perhaps I'm reading the manpage
wrong, or have odd fonts here...
</p>

<p>
	f
<pre>-- 
Francis Daly        francis at daoine.org
</pre>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_tips.html#mb-2_cent_Tip:_Unicode_conversion">Back</a><hr width="50%" align="left" /><p><br /></p></div>
</body>
</html>
