<!DOCTYPE html
	PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
	 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
<title>Backup software/strategies</title>
<link rel="stylesheet" type="text/css" href="../../../lg.css" />
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
</head>
<body>
<a href="../../../"><img alt="Linux Gazette" src="../../../gx/2003/newlogo-blank-200-gold2.jpg" id="logo" /></a><img alt="Tux" src="../../../gx/tux_86x95_indexed.png" id="tux" /><p id="fun">...making Linux just a little more fun!</p><div class='content articlecontent'><a name="top"></a><h3>Backup software/strategies</h3>
<p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Wed, 11 Jul 2007 09:24:40 -0400</b>
</p>

<p>
All of us know - at least I hope we do - that we should all be doing
regular backups; that's a given. Chances are, though, that we've all
skipped one or two (or more) on the schedule ("heck, nothin's happened
so far; it shouldn't make too much of a difference...") - that is, if
you even have a schedule, rather than relying on some vague sense of
"it's probably about time..." I have to admit that, as much as I advise
my customers to have a solid backup plan, I'm less than stellar about
having a polished, perfect plan myself.
</p>

<p>
In part, this is because backups are much easier with a desktop than a
laptop - or even with a desktop to which you synch the laptop once in a
while. Operating purely from a laptop, as I do, means that I don't have
an always-connected tape drive (or whatever) - and doing a backup is
always a hassle, involving digging out the external HD, hooking it up,
and synchronizing. Moreover, since I do a lot of travelling, setting an
alarm doesn't seem to be very useful; it usually goes off while I'm on
the road, at which point I can only glare at it in frustration.
</p>

<p>
As with so many things, what I <strong>really</strong> need is a copy of "at" installed
in my brain... but lacking that, well, I decided to dump the problem on
you folks. <img src="../gx/smile.png" alt=":)">
</p>

<p>
Can anyone here think of a sensible backup plan for the situation that
I've described - laptop, external backup, arbitrary schedule - and some
way to set up a schedule that work with that? Also, does anyone have a
favorite WRT backup software? I really miss the ability to do incremental
backups; that would be awf'lly nice (I don't mind carrying a few DVDs
with me, and using the external HD for a monthly full backup.)
</p>

<p>
Good ideas in this regard - whether they competely answer the question
or not - are highly welcome.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ren&eacute; Pfeiffer [lynx at luchs.at]
</p>
</b><br />
<b>Wed, 11 Jul 2007 17:44:15 +0200</b>
</p>

<p>
On Jul 11, 2007 at 0924 -0400, Ben Okopnik appeared and said:
</p>

<pre>
&gt; All of us know - at least I hope we do - that we should all be doing
&gt; regular backups; that's a given. Chances are, though, that we've all
&gt; skipped one or two (or more) on the schedule [...]
</pre>

<p>
Most people have cronjobs that automatically skip backups, so they don't
need to worry. ;)
</p>

<p>
I've given backup strategies a lot of thinking lately and I am still not
happy with some solutions and tools.
</p>


<pre>
&gt; [...] Can anyone here think of a sensible backup plan for the
&gt; situation that I've described - laptop, external backup, arbitrary
&gt; schedule - and some way to set up a schedule that work with that?
</pre>

<p>
You could use a combination of rsync, OpenSSH and Perl in order to set
up a rsyncd on your laptop, have a backup machine look for your laptop
on the network and whenever the server sees it create an SSH tunnel and
grab the latest deltas. <a href="http://backuppc.sourceforge.net/">http://backuppc.sourceforge.net/</a> has something
like this according to its feature list:
</p>

<pre>
"Supports mobile environments where laptops are only intermittently
connected to the network and have dynamic IP addresses (DHCP)."
</pre>
BackupPC uses hard links for identical files thus saving a lot of disk
space when backuping multiple servers and clients.
</p>


<pre>
&gt; Also, does anyone have a favorite WRT backup software? I really miss
&gt; the ability to do incremental backups; that would be awf'lly nice (I
&gt; don't mind carrying a few DVDs with me, and using the external HD for
&gt; a monthly full backup.)
</pre>

<p>
For me the usual suspects are rsync and rdiff-backup when it comes to
moderate amounts of storage. Indexing would be nice and in theory every
filesystem should offer something like that but most don't. I think I'll
explain what I mean in a seperate posting to TAG.
</p>

<p>
I have yet to walk through the complete list at
<a href="http://linuxmafia.com/pub/linux/backup/00index.txt">http://linuxmafia.com/pub/linux/backup/00index.txt</a> since I do
incremental backups by parsing the rsync logs of the backup server or
looking for timestamps. Mostly I stick to mirrored repositories because
I don't want to go through several incrementals when restoring
something. Most of the time I need to restore everything and don't need
incrementals. <img src="../gx/smile.png" alt=":)">
</p>

<p>
Best,
Ren&eacute;.
</p>


<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Wed, 11 Jul 2007 12:44:25 -0400</b>
</p>

<p>
On Wed, Jul 11, 2007 at 05:44:15PM +0200, Ren&eacute; Pfeiffer wrote:
</p>

<pre>
&gt; On Jul 11, 2007 at 0924 -0400, Ben Okopnik appeared and said:
&gt; &gt; All of us know - at least I hope we do - that we should all be doing
&gt; &gt; regular backups; that's a given. Chances are, though, that we've all
&gt; &gt; skipped one or two (or more) on the schedule [...]
&gt; 
&gt; Most people have cronjobs that automatically skip backups, so they don't
&gt; need to worry. ;)
&gt; 
&gt; I've given backup strategies a lot of thinking lately and I am still not
&gt; happy with some solutions and tools.
</pre>

<p>
[nod] That's where I am as well. I've been doing this iterative "pick a
backup strategy, wrestle with it for a while, give up in frustration,
let time pass until frustration level decreases to manageable level,
repeat" thing for years now, and haven't come up with anything truly
solid or positive - so it's been a "whenever memory and capability
coincide" method by default -and I'm extremely dissatisfied with that.
</p>


<pre>
&gt; &gt; [...] Can anyone here think of a sensible backup plan for the
&gt; &gt; situation that I've described - laptop, external backup, arbitrary
&gt; &gt; schedule - and some way to set up a schedule that work with that?
&gt; 
&gt; You could use a combination of rsync, OpenSSH and Perl in order to set
&gt; up a rsyncd on your laptop, have a backup machine look for your laptop
&gt; on the network and whenever the server sees it create an SSH tunnel and
&gt; grab the latest deltas. <a href="http://backuppc.sourceforge.net/">http://backuppc.sourceforge.net/</a> has something
&gt; like this according to its feature list:
&gt; 
&gt; "Supports mobile environments where laptops are only intermittently
&gt; connected to the network and have dynamic IP addresses (DHCP)."
</pre>

<p>
The problem with that is that I don't have a "home base" - it's just not
feasible to have an always-on connection on the boat - and using space
on someone else's machine isn't really feasible. At that point, my ideas
run off in the direction of buying rack space, and all the hassle that
entails - especially since St. Augustine, for all its charm, is the
technological backwoods where the hoot owls trod the chickens.
</p>

<p>
As a result, it never comes to pass. And I'm still searching for a
workable strategy. :/
 
</p>

<pre>
&gt; Mostly I stick to mirrored repositories because
&gt; I don't want to go through several incrementals when restoring
&gt; something. Most of the time I need to restore everything and don't need
&gt; incrementals. <img src="../gx/smile.png" alt=":)">
</pre>

<p>
In my case, that's not the usual situation; in fact, restoring a
complete backup would only be necessary in case of catastrophic failure
or a new laptop. 99% of the time, I'd want to restore a specific file or
two.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Karl-Heinz Herrmann [khh at khherrmann.de]
</p>
</b><br />
<b>Wed, 11 Jul 2007 21:17:34 +0200</b>
</p>

<p>
Hi Ben,
</p>


<p>
On Wed, 11 Jul 2007 09:24:40 -0400
Ben Okopnik &lt;ben@linuxgazette.net&gt; wrote:
</p>


<pre>
&gt; In part, this is because backups are much easier with a desktop than a
&gt; laptop - or even with a desktop to which you synch the laptop once in a
&gt; while. Operating purely from a laptop, as I do, means that I don't have
&gt; an always-connected tape drive (or whatever) - and doing a backup is
&gt; always a hassle, involving digging out the external HD, hooking it up,
&gt; and synchronizing. Moreover, since I do a lot of travelling, setting an
&gt; alarm doesn't seem to be very useful; it usually goes off while I'm on
&gt; the road, at which point I can only glare at it in frustration.
</pre>

<p>
Hmm... keeping the schedule might be the tougher part in this. 
</p>


<pre>
&gt; Can anyone here think of a sensible backup plan for the situation that
&gt; I've described - laptop, external backup, arbitrary schedule - and some
&gt; way to set up a schedule that work with that? Also, does anyone have a
&gt; favorite WRT backup software? I really miss the ability to do incremental
</pre>

<p>
I use backuppc at work and since a while now at home. This is a suite
of perl scripts using rsync/tar/... as file transfer backends, ssh
tunnels included. Web frontend allows to trigger backups, get restore
files, browse the archives. The concept of incremental/full backups is
also builtin. Usually it would run backups when scheduled -- but with a
sinlge drive laptop and external drive a possible way to do this would
be: Leave the laptop on during night and let it do its backup (would
require connecting the drive, /etc/backuppc/start and a few clicks in
the web-frontend -- or just waiting for the automatic backup kicking
in) 7 incremental, once a week a full one. Drive space permitting
backuppc allows you to keep several full backups as archive. removed
files will be recoverable. 
</p>

<p>
disadvantage: After copying the files backuppc will spend lots of time
figuring out wihch are identical and hardlinking (optionally
compressing) them. This can take longer than the original transfer :-/
</p>

<p>
It could handle a laptop only occasionally connected to a backup PC
somewhere. In that case the backuppc "server2 would run on the PC, just
fetching the files from the laptop -- then the laptop could already be
diconnected again. incremental backups of /home/user could be pretty
quick that way -- but you would need that running server. 
</p>


<pre>
&gt; backups; that would be awf'lly nice (I don't mind carrying a few DVDs
&gt; with me, and using the external HD for a monthly full backup.)
</pre>

<p>
backuppc would need a drive. As far as I know DVD backups (especially
multi-DVD's)  are not possible. All the history features and
hardlinking needs not just a drive but some decent file system as well.
</p>

<p>
Maybe not quite the solution you were looking for -- but maybe it
starts you thinking? I like the features of backuppc for local networks
-- it is perfect for nightly daily backups. But it does handle floating
laptops, including a reminder mail every week (or whatever you set it
to).
</p>


<p>
But then -- the last mail I got was:
Your PC (khhlap) has not been successfully backed up for 155.0 days.
Your PC has been correctly backed up 2 times from 155.1 to 155.0
[...]
</p>

<p>
Not that anything much happened to that lap during that time .-)
</p>

<p>
(My much used 24/7 PC at home is backing itself up to an external drive
daily.... )
</p>



<p>
K.-H.
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Kapil Hari Paranjape [kapil at imsc.res.in]
</p>
</b><br />
<b>Thu, 12 Jul 2007 09:17:28 +0530</b>
</p>

<p>
Hello,
</p>

<p>
On Wed, 11 Jul 2007, Ben Okopnik wrote:
</p>

<pre>
&gt; All of us know - at least I hope we do - that we should all be doing
&gt; regular backups; that's a given.
</pre>

<p>
Like one of the "existence" proofs in mathematics --- this gives no
hint of how one arrives at a solution <img src="../gx/smile.png" alt=":-)">
</p>


<pre>
&gt; Can anyone here think of a sensible backup plan for the situation that
&gt; I've described - laptop, external backup, arbitrary schedule - and some
&gt; way to set up a schedule that work with that?
</pre>

<p>
There are (at least) two different reasons why one (who uses a
laptop or desktop) might create a backup:
</p>

<pre>
	1. Protection against catastrophic break down or lack of access to the laptop's disk.
	2. Protection against accidental deletion of important work files.
</pre>
As far as (1) is concerned, I have already written out a kind of
strategy in LG #140. To that one can add the fact the laptops are not
"always on" so that "cron" type scheduling is irrelevant. So one
approach is to periodically schedule "laptop maintenance mornings":
</p>

<pre>
	a. Do a backup.
	b. Run all those pending cron jobs with "anacron -s".
	c. Send all those pending "popularity-contest" and bug report mailings, run "cruft" and do house-keeping.
	d. Possibly clean the real cruft from the screen and keyboard.
</pre>
For (a), I would use a simple script that invokes LVM snapshots and
rsync. I would not bother with incremental backups for (1).
</p>

<p>
One solution for (2) is to use version control for <strong>everything</strong>
important. With a typical system like "git" this means all that
is backed-up uses about double (or more[*]) space than it would
otherwise. However, it also means that recovery of deleted files is
quick and easy.
</p>

<p>
Since (1) takes care of backing up the repository as well, you have
the necessary incremental backup too.
</p>

<p>
Having said this, I have only partially implemented (2) and (1) is
often postponed. Note that (1) tends to run into lunch and the late
evening as well <img src="../gx/smile.png" alt=":-)">
</p>

<p>
Obviously, the strategy for a network of computers that share files
and are always on would be quite different.
</p>

<p>
Regards,
</p>

<p>
Kapil.
[*] When is an old version to be dumped? Look at Rene's posting for
some thoughts on this.
--
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Thu, 12 Jul 2007 12:57:06 -0400</b>
</p>

<p>
On Wed, Jul 11, 2007 at 09:17:34PM +0200, Karl-Heinz Herrmann wrote:
</p>

<pre>
&gt; Hi Ben,
&gt; 
&gt; 
&gt; On Wed, 11 Jul 2007 09:24:40 -0400
&gt; Ben Okopnik &lt;ben@linuxgazette.net&gt; wrote:
&gt; 
&gt; &gt; In part, this is because backups are much easier with a desktop than a
&gt; &gt; laptop - or even with a desktop to which you synch the laptop once in a
&gt; &gt; while. Operating purely from a laptop, as I do, means that I don't have
&gt; &gt; an always-connected tape drive (or whatever) - and doing a backup is
&gt; &gt; always a hassle, involving digging out the external HD, hooking it up,
&gt; &gt; and synchronizing. Moreover, since I do a lot of travelling, setting an
&gt; &gt; alarm doesn't seem to be very useful; it usually goes off while I'm on
&gt; &gt; the road, at which point I can only glare at it in frustration.
&gt; 
&gt; Hmm... keeping the schedule might be the tougher part in this. 
</pre>

<p>
In theory, that would be the job of something like "at"; however, that's
triggered off by a) time markers and b) booting the computer (at least
as far as I understand it.) The question is, how do I get an alarm to go
off when a) a backup is due or overdue <strong>and</strong> b) I'm where I can actually
do something about it?
</p>

<p>
(Perl's Telepathy module is still in development, I'm afraid. The docs
actually say so. <img src="../gx/smile.png" alt=":)">
 
</p>

<pre>
&gt; &gt; Can anyone here think of a sensible backup plan for the situation that
&gt; &gt; I've described - laptop, external backup, arbitrary schedule - and some
&gt; &gt; way to set up a schedule that work with that? Also, does anyone have a
&gt; &gt; favorite WRT backup software? I really miss the ability to do incremental
&gt; 
&gt; I use backuppc at work and since a while now at home. This is a suite
&gt; of perl scripts using rsync/tar/... as file transfer backends, ssh
&gt; tunnels included. Web frontend allows to trigger backups, get restore
&gt; files, browse the archives. The concept of incremental/full backups is
&gt; also builtin. 
</pre>

<p>
I've installed it; it's a very nifty looking piece of software, and very
much the kind of thing that I was thinking about. That's probably what
I'll end up using, although I'll be triggering it manually. Thanks!
</p>


<pre>
&gt; Usually it would run backups when scheduled -- but with a
&gt; sinlge drive laptop and external drive a possible way to do this would
&gt; be: Leave the laptop on during night and let it do its backup (would
&gt; require connecting the drive, /etc/backuppc/start and a few clicks in
&gt; the web-frontend -- or just waiting for the automatic backup kicking
&gt; in) 7 incremental, once a week a full one. Drive space permitting
&gt; backuppc allows you to keep several full backups as archive. removed
&gt; files will be recoverable. 
</pre>

<p>
The problem there would be that I can't really leave the 'top on every
night. I have to make every erg of power on board - whether via
gasoline-powered generator, solar panels, or wind generator (I have all
three) - and leaving a big power drain like a laptop on all night is
just not doable.
</p>

<p>
I neglected to mention that because... well, fish aren't aware of water.
It's just <strong>there</strong>. For me, that's how budgeting power works.
 
</p>

<pre>
&gt; backuppc would need a drive. As far as I know DVD backups (especially
&gt; multi-DVD's) are not possible. All the history features and
&gt; hardlinking needs not just a drive but some decent file system as well.
</pre>

<p>
I could probably fake it out by creating a virtual FS on the laptop,
doing a 'backup' to that, and copying the VFS to the DVD when done. In
fact, that's probably a really good technique for when I'm on the road;
heck, I probably wouldn't even need a DVD (a single CDROM would do.)
 
</p>

<pre>
&gt; Maybe not quite the solution you were looking for -- but maybe it
&gt; starts you thinking? 
</pre>

<p>
It does - and that's exactly what I was looking for, since my own ideas
had mostly hit dead ends and blank walls. Thanks, Karl-Heinz!
</p>


<pre>
&gt; I like the features of backuppc for local networks
&gt; -- it is perfect for nightly daily backups. But it does handle floating
&gt; laptops, including a reminder mail every week (or whatever you set it
&gt; to).
</pre>
 
And <em>that</em> would be a key feature of whatever scheduling scheme I came
up with. In a lot of ways, I use my inbox to remind myself of pending
and ongoing tasks.
 
</p>

<pre>
&gt; (My much used 24/7 PC at home is backing itself up to an external drive
&gt; daily.... )
</pre>

<p>
There are times I envy people with desktops - but the ratio of what I
gain by having a laptop to what I'd gain by having a desktop is <strong>very</strong>
strongly tilted toward the latter. Which doesn't stop me from bitching
about what I'm missing, of course - or trying to find ways of getting it
anyway.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Fri, 13 Jul 2007 14:20:41 -0400</b>
</p>

<p>
On Thu, Jul 12, 2007 at 09:17:28AM +0530, Kapil Hari Paranjape wrote:
</p>

<pre>
&gt; Hello,
&gt; 
&gt; On Wed, 11 Jul 2007, Ben Okopnik wrote:
&gt; &gt; All of us know - at least I hope we do - that we should all be doing
&gt; &gt; regular backups; that's a given.
&gt; 
&gt; Like one of the "existence" proofs in mathematics --- this gives no
&gt; hint of how one arrives at a solution <img src="../gx/smile.png" alt=":-)">
</pre>

<p>
Heh. That's why mathematics professors' favorite phrase tends to be
"...the solution is trivial, and therefore left to the student."
 
</p>

<pre>
&gt; &gt; Can anyone here think of a sensible backup plan for the situation that
&gt; &gt; I've described - laptop, external backup, arbitrary schedule - and some
&gt; &gt; way to set up a schedule that work with that?
&gt; 
&gt; There are (at least) two different reasons why one (who uses a
&gt; laptop or desktop) might create a backup:
&gt; 
&gt; 	1. Protection against catastrophic break down or lack of
&gt; 	access to the laptop's disk.
</pre>

<p>
That carries about 75% of the weight in my situation...
</p>


<pre>
&gt; 	2. Protection against accidental deletion of important work
&gt; 	files.
</pre>

<p>
...and this is the rest of it.
</p>


<pre>
&gt; As far as (1) is concerned, I have already written out a kind of
&gt; strategy in LG #140. 
</pre>

<p>
What, the 'back it up if you haven't already" part? I've got that one
handled.
</p>


<pre>
&gt; To that one can add the fact the laptops are not
&gt; "always on" so that "cron" type scheduling is irrelevant. 
</pre>

<p>
This, of course, is why I mentioned "at" - which will either run at the
specified time, or keep retrying if it misses the original schedule.
</p>


<pre>
&gt; So one
&gt; approach is to periodically schedule "laptop maintenance mornings":
</pre>

<p>
Which brings us back to the original problem. "Every other Tuesday where
the date is not divisible by 3 or 7 and is not within 4 days of the end
of month" can be scheduled; "7 days after last backup assuming that I'm
at home, otherwise as soon as I get home and have had a chance to unpack
and catch up on sleep" cannot.
 
</p>

<pre>
&gt; 	a. Do a backup.
&gt; 	b. Run all those pending cron jobs with "anacron -s".
&gt; 	c. Send all those pending "popularity-contest" and bug report
&gt; 	mailings, run "cruft" and do house-keeping.
&gt; 	d. Possibly clean the real cruft from the screen and
&gt; 	keyboard.
&gt; 
&gt; For (a), I would use a simple script that invokes LVM snapshots and
&gt; rsync. I would not bother with incremental backups for (1).
&gt; 
&gt; One solution for (2) is to use version control for <strong>everything</strong>
&gt; important. With a typical system like "git" this means all that
&gt; is backed-up uses about double (or more[*]) space than it would
&gt; otherwise. However, it also means that recovery of deleted files is
&gt; quick and easy.
&gt; 
&gt; Since (1) takes care of backing up the repository as well, you have
&gt; the necessary incremental backup too.
&gt; 
&gt; Having said this, I have only partially implemented (2) and (1) is
&gt; often postponed. Note that (1) tends to run into lunch and the late
&gt; evening as well <img src="../gx/smile.png" alt=":-)">
</pre>

<p>
[laugh] Which creates another scheduling problem if you can't do it all
in one swell foop. I'm not worried about that one at all.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ren&eacute; Pfeiffer [lynx at luchs.at]
</p>
</b><br />
<b>Sat, 14 Jul 2007 13:18:58 +0200</b>
</p>

<p>
On Jul 13, 2007 at 1420 -0400, Ben Okopnik appeared and said:
</p>

<pre>
&gt; On Thu, Jul 12, 2007 at 09:17:28AM +0530, Kapil Hari Paranjape wrote:
&gt; [...]
&gt; &gt; To that one can add the fact the laptops are not
&gt; &gt; "always on" so that "cron" type scheduling is irrelevant.
&gt; This, of course, is why I mentioned "at" - which will either run at the
&gt; specified time, or keep retrying if it misses the original schedule.
</pre>

<p>
Shouldn't anacron take care of this? AFAIK anacron checks for missed
cron job since the last boot and reschedules them. Provided that the
cron jobs check for the presence of a suitable backup device/server (and
don't offer all data to the frist device in range with the right IP
address) this could work, don't you think?
</p>


<pre>
&gt; &gt; So one approach is to periodically schedule "laptop maintenance
&gt; &gt; mornings":
&gt; Which brings us back to the original problem. "Every other Tuesday
&gt; where the date is not divisible by 3 or 7 and is not within 4 days of
&gt; the end of month" can be scheduled; "7 days after last backup assuming
&gt; that I'm at home, otherwise as soon as I get home and have had a
&gt; chance to unpack and catch up on sleep" cannot.
</pre>

<p>
Sounds as if your cron jobs need some serious plugins. ;)
</p>


<pre>
&gt; &gt; 	a. Do a backup.
&gt; &gt; 	b. Run all those pending cron jobs with "anacron -s".
</pre>

<p>
Ah, I missed anacron at the first reading of Kapil's posting.
</p>


<pre>
&gt; &gt; Having said this, I have only partially implemented (2) and (1) is
&gt; &gt; often postponed. Note that (1) tends to run into lunch and the late
&gt; &gt; evening as well <img src="../gx/smile.png" alt=":-)">
&gt; [laugh] Which creates another scheduling problem if you can't do it all
&gt; in one swell foop. I'm not worried about that one at all.
</pre>

<p>
About how much data per time period between backups are we talking? I
just want to get a rough estimate.
</p>

<p>
Best wishes,
Ren&eacute;.
</p>


<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Sat, 14 Jul 2007 13:36:07 -0400</b>
</p>

<p>
On Sat, Jul 14, 2007 at 01:18:58PM +0200, Ren&eacute; Pfeiffer wrote:
</p>

<pre>
&gt; On Jul 13, 2007 at 1420 -0400, Ben Okopnik appeared and said:
&gt; &gt; On Thu, Jul 12, 2007 at 09:17:28AM +0530, Kapil Hari Paranjape wrote:
&gt; &gt; [...]
&gt; &gt; &gt; To that one can add the fact the laptops are not
&gt; &gt; &gt; "always on" so that "cron" type scheduling is irrelevant. 
&gt; &gt; 
&gt; &gt; This, of course, is why I mentioned "at" - which will either run at the
&gt; &gt; specified time, or keep retrying if it misses the original schedule.
&gt; 
&gt; Shouldn't anacron take care of this? AFAIK anacron checks for missed
&gt; cron job since the last boot and reschedules them. Provided that the
&gt; cron jobs check for the presence of a suitable backup device/server (and
&gt; don't offer all data to the frist device in range with the right IP
&gt; address) this could work, don't you think?
</pre>

<p>
Ah. I was thinking of "anacron" when I said "at". Right.
 
</p>

<pre>
&gt; &gt; &gt; So one approach is to periodically schedule "laptop maintenance
&gt; &gt; &gt; mornings":
&gt; &gt; 
&gt; &gt; Which brings us back to the original problem. "Every other Tuesday
&gt; &gt; where the date is not divisible by 3 or 7 and is not within 4 days of
&gt; &gt; the end of month" can be scheduled; "7 days after last backup assuming
&gt; &gt; that I'm at home, otherwise as soon as I get home and have had a
&gt; &gt; chance to unpack and catch up on sleep" cannot.
&gt; 
&gt; Sounds as if your cron jobs need some serious plugins. ;)
</pre>

<p>
That Telepathy module would come in handy, yes. <img src="../gx/smile.png" alt=":)">
 
</p>

<pre>
&gt; &gt; &gt; 	a. Do a backup.
&gt; &gt; &gt; 	b. Run all those pending cron jobs with "anacron -s".
&gt; 
&gt; Ah, I missed anacron at the first reading of Kapil's posting.
&gt; 
&gt; &gt; &gt; Having said this, I have only partially implemented (2) and (1) is
&gt; &gt; &gt; often postponed. Note that (1) tends to run into lunch and the late
&gt; &gt; &gt; evening as well <img src="../gx/smile.png" alt=":-)">
&gt; &gt; 
&gt; &gt; [laugh] Which creates another scheduling problem if you can't do it all
&gt; &gt; in one swell foop. I'm not worried about that one at all.
&gt; 
&gt; About how much data per time period between backups are we talking? I
&gt; just want to get a rough estimate.
</pre>

<p>
Relatively minor, I'd say. Assuming that I want to back up at least once
per week, here are the factors that go into it:
</p>

<p>
1) New packages: I'm not at a point yet where I have all the packages I
want installed, but I'm certainly past the half-way point; I'm probably
installing one or two a week now, maximum. In any case, even if I lose
those, it's just not a huge factor - I can always reinstall. Worst-case
scenario: maybe 20MB worth of changes.
</p>

<p>
2) User data: This is <strong>the</strong> important stuff, of course. LG-related work
in /var, changes in /home and /usr/local/... that's pretty much it. I
seriously doubt that this would exceed 10MB in any given week.
</p>

<p>
So, really, we're looking at a max of 10MB that's absolutely critical,
with another 20MB that would be nice to have but no major loss if it
doesn't happen.
</p>

<p>
Hmm. Having said that, it's clear that I can do incrementals using a
flash drive, and schedule the full backups when I'm home (say, an alarm
every Monday, ignoring those that happen on the road.) Now the question
becomes, what do I use? "dump" is... crude. "backuppc" requires a hard
drive (well, maybe. I'll have to play with it and find out.) Any other
suggestions?
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Sat, 14 Jul 2007 11:28:27 -0700</b>
</p>

<p>
Quoting Ben Okopnik (ben@linuxgazette.net):
</p>


<pre>
&gt; 1) New packages: I'm not at a point yet where I have all the packages I
&gt; want installed, but I'm certainly past the half-way point; I'm probably
&gt; installing one or two a week now, maximum. In any case, even if I lose
&gt; those, it's just not a huge factor - I can always reinstall. Worst-case
&gt; scenario: maybe 20MB worth of changes.
</pre>

<p>
I would urge <em>not</em> including (not-locally-modified) upstream packages in
one's backups -- because they are available at need directly from the
distro's Internet package mirrors.  Instead, just back up a <em>list</em> of
installed package that can later be used during a semi-automated restore
/ rebuild.  E.g., on a Debian or similar system, include the following
command's output in your backup sets:
</p>

<pre>
$ sudo dpkg --get-selections "*" &gt; /root/selections-$(date +%F)
</pre>
Also include maps of your partition tables:
</p>

<pre>
$ sudo /sbin/fdisk -l /dev/sda &gt; /root/partitions-sda-$(date +%F)
$ sudo /sbin/fdisk -l /dev/sdb &gt; /root/partitions-sdb-$(date +%F)
[etc.]
</pre>
If there's a need for quick restore / rebuild, install a minimal system
from installation media, then do:
</p>

<pre>
# dpkg --set-selections &lt; selections-[date string]
# apt-get dselect-upgrade
</pre>
The above is taken from my own recipe for casual backups:
<a href="http://linuxmafia.com/faq/Admin/linuxmafia.com-backup.html">http://linuxmafia.com/faq/Admin/linuxmafia.com-backup.html</a>
</p>

<p>
I realise that you might be saying "Wait, I don't want to have to
re-fetch all of my packages from Internet package mirrors over my very
slow Net connection."  OK, but having a full CD set of your preferred
distro handy for reinstallations is still a better solution than
including unaltered upstream package contents in your system backups.
</p>



<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Sat, 14 Jul 2007 15:31:20 -0400</b>
</p>

<p>
On Sat, Jul 14, 2007 at 11:28:27AM -0700, Rick Moen wrote:
</p>

<pre>
&gt; Quoting Ben Okopnik (ben@linuxgazette.net):
&gt; 
&gt; &gt; 1) New packages: I'm not at a point yet where I have all the packages I
&gt; &gt; want installed, but I'm certainly past the half-way point; I'm probably
&gt; &gt; installing one or two a week now, maximum. In any case, even if I lose
&gt; &gt; those, it's just not a huge factor - I can always reinstall. Worst-case
&gt; &gt; scenario: maybe 20MB worth of changes.
&gt; 
&gt; I would urge <em>not</em> including (not-locally-modified) upstream packages in
&gt; one's backups -- because they are available at need directly from the
&gt; distro's Internet package mirrors.
</pre>

<p>
[ snip ]
 
</p>

<pre>
&gt; I realise that you might be saying "Wait, I don't want to have to
&gt; re-fetch all of my packages from Internet package mirrors over my very
&gt; slow Net connection."  OK, but having a full CD set of your preferred
&gt; distro handy for reinstallations is still a better solution than
&gt; including unaltered upstream package contents in your system backups.
</pre>

<p>
I failed to make myself clear, obviously. I'm not interested in having
backups of the packages themselves; what I meant was that I'm
<em>installing</em> a couple of packages a week on my system, and a full backup
will inevitably include their contents. Assuming that my most recent
backup is within a couple of years (!) of the current "state of the
state", a full restore plus an "apt-get update &amp;&amp; apt-get upgrade"
should bring things up to spec.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Sat, 14 Jul 2007 13:50:24 -0700</b>
</p>

<p>
Quoting Ben Okopnik (ben@linuxgazette.net):
</p>


<pre>
&gt; what I meant was that I'm <em>installing</em> a couple of packages a week on
&gt; my system, and a full backup will inevitably include their contents.
</pre>

<p>
I don't mean to be difficult, but that's not inevitable -- nor, I would
maintain, desirable.  It's my view that one should carefully avoid
backing up any packages that can be provided easily from distro-packaged 
archives, in the event of rebuild / restore.  Therefore, one should
exclude /bin, /sbin, /lib, most of /usr (except /usr/local, and other
odd exceptions that one may find such as /usr/lib/cgi-bin  that may have
locally installed files.
</p>

<p>
If you make sure every backup set has a catalougue of currently
installed package names, then that plus locally generated files are
literaly all that need be backed up.  Here is a complete list of
directories that <em>do</em> need backing up, on my server:
</p>

<pre>
/root                        Root user's home directory (includes above files)
/etc                         System configuration files
/usr/lib/cgi-bin             CGI scripts
/var/lib/mysql               MySQL database files (dump if not quiescent)
/boot/grub/menu.lst          GRUB bootloader configuration
/var/spool/exim4             Exim and SA-Exim internal files
/var/spool/news              NNTP news spool for Leafnode
/var/spool/mail              SMTP mail spool
/var/lib/mailman/lists       Mailing list definitions for Mailman
/var/lib/mailman/archives    Mailing list archives for Mailman
/usr/local                   Locally installed files and records
/var/www                     Public http, ftp, rsync tree
/home                        Non-root users' home trees
</pre>
Making sure that list is complete for a given system requires some study
of one's system.  What I'm saying is that such study, as an alternative
to just copying everything, is worth the trouble.
</p>


<pre>
&gt; Assuming that my most recent backup is within a couple of years (!) of
&gt; the current "state of the state", a full restore plus an "apt-get
&gt; update &amp;&amp; apt-get upgrade" should bring things up to spec.
</pre>

<p>
Yes, but so would my approach, with significantly smaller backup sets.
</p>



<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Sat, 14 Jul 2007 19:01:02 -0400</b>
</p>

<p>
On Sat, Jul 14, 2007 at 01:50:24PM -0700, Rick Moen wrote:
</p>

<pre>
&gt; 
&gt; If you make sure every backup set has a catalougue of currently
&gt; installed package names, then that plus locally generated files are
&gt; literaly all that need be backed up.  Here is a complete list of
&gt; directories that <em>do</em> need backing up, on my server:
&gt; 
&gt; /root                        Root user's home directory (includes above files)
&gt; /etc                         System configuration files
&gt; /usr/lib/cgi-bin             CGI scripts
&gt; /var/lib/mysql               MySQL database files (dump if not quiescent)
&gt; /boot/grub/menu.lst          GRUB bootloader configuration
&gt; /var/spool/exim4             Exim and SA-Exim internal files
&gt; /var/spool/news              NNTP news spool for Leafnode
&gt; /var/spool/mail              SMTP mail spool
&gt; /var/lib/mailman/lists       Mailing list definitions for Mailman
&gt; /var/lib/mailman/archives    Mailing list archives for Mailman
&gt; /usr/local                   Locally installed files and records
&gt; /var/www                     Public http, ftp, rsync tree
&gt; /home                        Non-root users' home trees
&gt; 
&gt; Making sure that list is complete for a given system requires some study
&gt; of one's system.  What I'm saying is that such study, as an alternative
&gt; to just copying everything, is worth the trouble.
</pre>

<p>
I see your point, and I agree that it can be a valuable approach - in
fact, it's well worth considering in my own case. The problem is that a
solution like this can't be generalized to the average user; most people
have no idea of what on their system has been "custom-modified" vs.
being tweaked by package installation, and chances are good that they've
also forgotten anything unusual that they have tweaked. E.g., as I sit
here racking my brain for anything that falls outside the parameters,
I've just recalled my travails with "ndiswrapper" to get my WiFi working
- which required putting the driver files into a directory somewhere in
"/usr/lib" (or was it "/usr/share"?) There was also broken behavior in
several X apps - I've reported it to Ubuntu already, but the problem
dates back to the changeover from X11 to Xorg - that required symlinking
"/usr/lib/X11/fonts/" to "/usr/share/X11/fonts/".
</p>

<p>
In other words: in my experience, it's not nearly as easy to separate
"system" files from "user" files as it should be. Now that I'm thinking
about it, I've tried doing something like this before (although
admittedly not in as nearly organized a fashion as you suggest), and
ended up with a whole lot of pain and suffering - i.e., reconstructing
all the customization that I'd applied to my old system over the years.
</p>

<p>
In theory, I should be keeping a log of all the changes of that sort
that I apply. In reality, some of the tweaks were done under time
pressure and as emergency fixes, and aren't "retrievable" other than
going through the same PITA that caused them to come into being in the
first place.
</p>


<pre>
&gt; &gt; Assuming that my most recent backup is within a couple of years (!) of
&gt; &gt; the current "state of the state", a full restore plus an "apt-get
&gt; &gt; update &amp;&amp; apt-get upgrade" should bring things up to spec.
&gt; 
&gt; Yes, but so would my approach, with significantly smaller backup sets.
</pre>

<p>
That's what makes it worth considering, of course. Decreasing the size
of the "full" backup would be a very useful thing.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Sat, 14 Jul 2007 19:22:22 -0400</b>
</p>

<p>
Oh, and just as a comparison: Rick's approach, for my system, results in
a backup set that's just a hair under 25GB; a full backup is a bit over
53GB. Pretty significant.
 
</p>

<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Kapil Hari Paranjape [kapil at imsc.res.in]
</p>
</b><br />
<b>Sun, 15 Jul 2007 09:27:36 +0530</b>
</p>

<p>
Hello,
</p>

<p>
On Sat, 14 Jul 2007, Rick Moen wrote:
</p>

<pre>
&gt; If you make sure every backup set has a catalougue of currently
&gt; installed package names, then that plus locally generated files are
&gt; literaly all that need be backed up.  Here is a complete list of
&gt; directories that <em>do</em> need backing up, on my server:
</pre>

<p>
I would add the following to Rick's list:
<pre>
	/var/lib/dpkg		Saves a lot of config info (alternatives, diverts, ...)
				which "dpkg --get-selections" misses out on.
	-/var/lib/dpkg/info	Leave out the really large subdirectory
				which contains no information not in the packages
	/var/cache/debconf	Keep all the debconf answered questions
	/var/lib/aptitude	Saves the package dependency choices
</pre>
One also needs to consider the (usually minor) problems that may occur
if a package on the crashed system was out-of-date. When restoring
from the upstream Debian archive or CD, this package would be replaced
by a newer version.
</p>

<p>
On the whole I prefer to have at least one "mondo" style backup of
the system which gets me to a familiar working environment even while
one is trying to bring the dead back to life. With external bootable
USB this is quite feasible.
(This is a shameless plug for my article in LG #140!)
</p>

<p>
Regards,
</p>

<p>
Kapil.
--
</p>



<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Mulyadi Santosa [mulyadi.santosa at gmail.com]
</p>
</b><br />
<b>Sun, 15 Jul 2007 22:50:48 +0700</b>
</p>

<p>
hi...
</p>

<p>
joining a bit late, but better late than never... <img src="../gx/smile.png" alt=":)">
</p>


<pre>
&gt; Oh, and just as a comparison: Rick's approach, for my system, results in
&gt; a backup set that's just a hair under 25GB; a full backup is a bit over
&gt; 53GB. Pretty significant.
</pre>

<p>
It reminds me about a simple but useful practice related to make
backup easier. If you compile a program from the source, make sure
it's installed under certain directory, let's say /usr/local. Later,
we could just make a symlink from /usr/bin (or any other common binary
places) towards these binaries. Or, you could make yourself
RPM/deb/whatever of these applications.
</p>

<p>
It brings two advantage IMHO:
</p>

<p>
1. you don't need to recompile after doing restore (probably after bad accident)
</p>

<p>
2. you know which binaries are brought by the default distribution
installation/upgrade. Of course, you could do the same by using your
package manager, but it takes time.
</p>

<p>
regards,
</p>

<p>
Mulyadi
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Sun, 15 Jul 2007 10:36:31 -0700</b>
</p>

<p>
Quoting Ben Okopnik (ben@linuxgazette.net):
</p>


<pre>
&gt; I see your point, and I agree that it can be a valuable approach - in
&gt; fact, it's well worth considering in my own case. The problem is that a
&gt; solution like this can't be generalized to the average user; most people
&gt; have no idea of what on their system has been "custom-modified" vs.
&gt; being tweaked by package installation, and chances are good that they've
&gt; also forgotten anything unusual that they have tweaked. 
</pre>

<p>
My own is experience is that there turned out to be only a few
"surprise" locations -- the Mailman tree under /var/lib, MySQL's
database files under /var/lib, and the CGIs under /usr/lib -- that only
gradually occurred to me over about a week of occasionally pondering the
problem and wandering the directory tree, looking around.
</p>

<p>
It helped that I always carefully leave "system" directories to distro
control, with the exception of /etc, system CGIs, and Mailman
configuration, and carefully make sure locally installed software went
under /usr/local and not in the regular system tree.
</p>


<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Thomas Adam [thomas at edulinux.homeunix.org]
</p>
</b><br />
<b>Sun, 15 Jul 2007 20:55:16 +0100</b>
</p>

<p>
On Sun, Jul 15, 2007 at 10:50:48PM +0700, Mulyadi Santosa wrote:
</p>

<pre>
&gt; It reminds me about a simple but useful practice related to make
&gt; backup easier. If you compile a program from the source, make sure
&gt; it's installed under certain directory, let's say /usr/local. Later,
&gt; we could just make a symlink from /usr/bin (or any other common binary
</pre>

<p>
This is precisely what stow used to do before it was deprecated.  This isn't
really about making backups easier -- that location is where you <strong>should</strong> be
installing programs you yourself have compiled.
</p>


<pre>
&gt; places) towards these binaries. Or, you could make yourself
&gt; RPM/deb/whatever of these applications.
</pre>

<p>
Better off just using debhelper with dh_make for that -- then it will
install to /usr, where you can then use equivs and package holding to
further that aim.
</p>

<p>
(And since you seem to be a Debian user, this is where deb-repack also
helps.)
</p>

<pre>-- 
Thomas Adam
"He wants you back, he screams into the night air, like a fireman going
through a window that has no fire." -- Mike Myers, "This Poem Sucks".
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Sun, 15 Jul 2007 14:06:11 -0700</b>
</p>

<p>
Quoting Thomas Adam (thomas@edulinux.homeunix.org):
</p>


<pre>
&gt; "He wants you back, he screams into the night air, like a fireman going
&gt; through a window that has no fire." -- Mike Myers, "This Poem Sucks".
</pre>

<p>
You hard-hearted harbinger of haggis, you.
</p>

<pre>-- 
Cheers,                Re-elect Gore in '08.
Rick Moen              <a href="http://www.hyperorg.com/blogger/misc/gorespeech.html">http://www.hyperorg.com/blogger/misc/gorespeech.html</a>
rick@linuxmafia.com
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Thomas Adam [thomas at edulinux.homeunix.org]
</p>
</b><br />
<b>Sun, 15 Jul 2007 22:16:38 +0100</b>
</p>

<p>
On Sun, Jul 15, 2007 at 02:06:11PM -0700, Rick Moen wrote:
</p>

<pre>
&gt; Quoting Thomas Adam (thomas@edulinux.homeunix.org):
&gt; 
&gt; &gt; "He wants you back, he screams into the night air, like a fireman going
&gt; &gt; through a window that has no fire." -- Mike Myers, "This Poem Sucks".
&gt; 
&gt; You hard-hearted harbinger of haggis, you.
</pre>

<p>
That's the one.  <img src="../gx/smile.png" alt=":)">  "So I Married an Axe Murderer" is a terrible film,
as was the poem; although I believe that was the point.  <img src="../gx/smile.png" alt=":)">
</p>

<pre>-- 
Thomas Adam
"He wants you back, he screams into the night air, like a fireman going
through a window that has no fire." -- Mike Myers, "This Poem Sucks".
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Sun, 15 Jul 2007 18:42:23 -0400</b>
</p>

<p>
On Sun, Jul 15, 2007 at 09:27:36AM +0530, Kapil Hari Paranjape wrote:
</p>

<pre>
&gt; Hello,
&gt; 
&gt; On Sat, 14 Jul 2007, Rick Moen wrote:
&gt; &gt; If you make sure every backup set has a catalougue of currently
&gt; &gt; installed package names, then that plus locally generated files are
&gt; &gt; literaly all that need be backed up.  Here is a complete list of
&gt; &gt; directories that <em>do</em> need backing up, on my server:
&gt; 
&gt; I would add the following to Rick's list:
&gt; 
&gt; 	/var/lib/dpkg		Saves a lot of config info (alternatives, diverts, ...)
&gt; 				which "dpkg --get-selections" misses out on.
&gt; 	-/var/lib/dpkg/info	Leave out the really large subdirectory
&gt; 				which contains no information not in the packages
&gt; 	/var/cache/debconf	Keep all the debconf answered questions
&gt; 	/var/lib/aptitude	Saves the package dependency choices
</pre>

<p>
Wouldn't this create problems by "misinforming" the package system about
the current package state? Rick is saying "don't bring the packages
along, reinstall them" - while the above (with the possible exception of
/var/cache/debconf) will tell the system that they're already installed.
It sounds to me like "apt-get" would just tell you "$blah is already the
latest version" and exit for every single package that you had on the
old system (while leaving those that hadn't been installed back then
installable now.)
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Thomas Adam [thomas at edulinux.homeunix.org]
</p>
</b><br />
<b>Sun, 15 Jul 2007 23:46:14 +0100</b>
</p>

<p>
On Sun, Jul 15, 2007 at 06:42:23PM -0400, Ben Okopnik wrote:
</p>

<pre>
&gt; It sounds to me like "apt-get" would just tell you "$blah is already the
&gt; latest version" and exit for every single package that you had on the
&gt; old system (while leaving those that hadn't been installed back then
&gt; installable now.)
</pre>

<p>
Yes, which is why you always:
</p>

<pre>
dpkg --get-selections &gt; ./some_file # Old machine.
dpkg --set-selections &lt; ./some_file # New machine.
apt-get dselect-upgrade
</pre>
Then under the hood those files in /var/lib/* get updated accordingly.
</p>

<pre>-- 
Thomas Adam
"He wants you back, he screams into the night air, like a fireman going
through a window that has no fire." -- Mike Myers, "This Poem Sucks".
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Sun, 15 Jul 2007 18:56:07 -0400</b>
</p>

<p>
On Sun, Jul 15, 2007 at 11:46:14PM +0100, Thomas Adam wrote:
</p>

<pre>
&gt; On Sun, Jul 15, 2007 at 06:42:23PM -0400, Ben Okopnik wrote:
&gt; &gt; It sounds to me like "apt-get" would just tell you "$blah is already the
&gt; &gt; latest version" and exit for every single package that you had on the
&gt; &gt; old system (while leaving those that hadn't been installed back then
&gt; &gt; installable now.)
&gt; 
&gt; Yes, which is why you always:
&gt; 
&gt; ``
&gt; dpkg --get-selections &gt; ./some_file # Old machine.
&gt; dpkg --set-selections &lt; ./some_file # New machine.
</pre>

<p>
Ah.
</p>


<pre>
&gt; apt-get dselect-upgrade
&gt; ''
&gt; 
&gt; Then under the hood those files in /var/lib/* get updated accordingly.
</pre>

<p>
Wouldn't the first two lines take care of that already? I'm missing the
purpose of copying /var/lib/ here.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Sun, 15 Jul 2007 16:02:52 -0700</b>
</p>

<p>
Quoting Ben Okopnik (ben@linuxgazette.net):
</p>


<pre>
&gt; Wouldn't the first two lines take care of that already? I'm missing the
&gt; purpose of copying /var/lib/ here.
</pre>

<p>
As am I.
</p>



<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Thomas Adam [thomas at edulinux.homeunix.org]
</p>
</b><br />
<b>Mon, 16 Jul 2007 00:09:55 +0100</b>
</p>

<p>
On Sun, Jul 15, 2007 at 06:56:07PM -0400, Ben Okopnik wrote:
</p>

<pre>
&gt; Wouldn't the first two lines take care of that already? I'm missing the
&gt; purpose of copying /var/lib/ here.
</pre>

<p>
Yep, and as for copying /var/lib -- it wasn't I who suggested it, so I can't
say.  <img src="../gx/smile.png" alt=":)">
</p>

<pre>-- 
Thomas Adam
"He wants you back, he screams into the night air, like a fireman going
through a window that has no fire." -- Mike Myers, "This Poem Sucks".
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Kapil Hari Paranjape [kapil at imsc.res.in]
</p>
</b><br />
<b>Mon, 16 Jul 2007 07:00:58 +0530</b>
</p>

<p>
Hello,
</p>

<p>
Since I was the one who suggested copying /var/lib/dpkg (but leaving
out /var/lib/dpkg/info) and /var/lib/aptitude, and since a number of
people are wondering why ...
</p>

<p>
On Sun, 15 Jul 2007, Ben Okopnik wrote:
</p>

<pre>
&gt; Wouldn't the first two lines take care of that already? I'm missing the
&gt; purpose of copying /var/lib/ here.
</pre>

<p>
On Sun, 15 Jul 2007, Rick Moen wrote:
</p>

<pre>
&gt; As am I.
</pre>

<p>
On Mon, 16 Jul 2007, Thomas Adam wrote:
</p>

<pre>
&gt; Yep, and as for copying /var/lib -- it wasn't I who suggested it, so I can't
&gt; say.  <img src="../gx/smile.png" alt=":)">
</pre>

<p>
... let me explain.
</p>

<p>
The directory /var/lib/dpkg/alternatives contains the alternatives
that you have chosen for the system by running update-alternatives.
</p>

<p>
The file /var/lib/dpkg/diversions lists all the diversions that you
have created for your system by running dpkg-divert.
</p>

<p>
The file /var/lib/aptitude/pkgstates lists which packages were pulled
in by automatic dependency checks.
</p>

<p>
So one way to restore your system if you <strong>have</strong> backed-up the
directories /var/lib/dpkg and /var/lib/aptitude would be:
</p>

<pre>
	dpkg --get-selections ....
	rm /var/lib/dpkg/status
	dpkg --set-selects ...
</pre>
An alternate approach would be to backup /var/lib/dpkg/alternatives
and /var/lib/dpkg/diversions. You can use "debfoster" to keep
track of package dependency and backup its (much smaller) file
/var/lib/debfoster/keepers.
</p>

<p>
Regards,
</p>

<p>
Kapil.
--
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Rick Moen [rick at linuxmafia.com]
</p>
</b><br />
<b>Sun, 15 Jul 2007 19:24:31 -0700</b>
</p>

<p>
Quoting Kapil Hari Paranjape (kapil@imsc.res.in):
</p>


<pre>
&gt; An alternate approach would be to backup /var/lib/dpkg/alternatives
&gt; and /var/lib/dpkg/diversions. 
</pre>

<p>
Kapil, thanks for reminding us of where data for the alternatives
and diversions mechanisms reside -- which mechanisms I was aware of, but
hadn't yet had occasion to use diversions on my systems, and I've made
only relatively minimal use of alternatives.  That's why I wasn't taking
measures to collect and preserve any of that data.
</p>

<p>
Note:  It wouldn't be sufficient to merely back up and restore
/var/lib/dpkg/alternatives/* :  That's a record kept by the
update-alternatives utility as as result of its management of the
/etc/alternatives/ symlink tree.  You want to also get the contents of
that tree back.
</p>

<p>
When I dealt with this matter before, I did <em>not</em> try to preserve
any "alternatives" data as such, except through having a reference tarball of 
the prior /etc/ tree, on-hand for eyeball reference while reconstructing the
prior machine's local configuration manually.  Which approach has the
advantage of always working and offering no possibility of unpleasant
surprises -- and, honestly, the /etc/alternatives symlink tree is going
to end up almost 100% correct if not totally so <em>without</em> any help:  E.g.,
including vim in your package shopping list will result in vim replacing
nvi as the system devault "vi" utility (/etc/alternatives/vi symlink),
without your needing to take any other steps at all.
</p>

<p>
Anyway, given a preserved copy of /var/lib/dpkg/alternatives/* from the
prior system, I believe running "update-alternatives --set" will correctly
rebuild one's /etc/alternatives/ symlinks as desired, if I'm reading
the manpage correctly.
</p>

<p>
By contrast, I believe it is perfectly sufficient to back up, preserve,
and carry forward the /var/lib/dpkg/diversions file, which record 
appears to be self-contained.
</p>



<pre>
&gt; The file /var/lib/aptitude/pkgstates lists which packages were pulled
&gt; in by automatic dependency checks.
</pre>
[...]
</p>

<pre>
&gt; You can use "debfoster" to keep
&gt; track of package dependency and backup its (much smaller) file
&gt; /var/lib/debfoster/keepers.
</pre>

<p>
I'm not sure that backing up and preserving <em>dependency</em> data is a
useful thing to do.  Do you think it is?
</p>

<p>
My working assumption -- and it seemed to prove valid when I used my 
aforementioned procedure to migrate my entire system -- is that your
best tactic is to just build a minimal system, then hand the package
tool your list of packages (carried over from a backup set) as a
shopping list, and let <em>it</em> work out the dependencies on its own, fetching
whatever are the <em>current</em> requirments, which may be quite different
from what they used to be, in your machine's prior incarnation.
</p>

<p>
Strictly speaking, actually, what I ended up doing was building a minimal
system, fetching a copy of the prior installed-packages list, manually 
snipping out all <em>library</em> packages using a text editor, and only <em>then</em>
feeding that list to apt as a shopping list.  Doing so avoided a large
number of messy and time-wasting error messages resulting from the fact
that requested package foo no longer required library bar but instead
differently-named library baz.
</p>

<p>
My general approach, then, is:  Tell the package tool what I want, and 
let it work out the details without my needing to micromanage.  On a
system with good tools and a strong package policy (Debian, Ubuntu,
etc.), this works well.
</p>



<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Kapil Hari Paranjape [kapil at imsc.res.in]
</p>
</b><br />
<b>Mon, 16 Jul 2007 09:10:57 +0530</b>
</p>

<p>
Hello,
</p>

<p>
On Sun, 15 Jul 2007, Rick Moen wrote:
</p>

<pre>
&gt; Anyway, given a preserved copy of /var/lib/dpkg/alternatives/* from the
&gt; prior system, I believe running "update-alternatives --set" will correctly
&gt; rebuild one's /etc/alternatives/ symlinks as desired, if I'm reading
&gt; the manpage correctly.
</pre>

<p>
That is right. I didn't say anything about this since /etc was also
being backed up. Your analysis of what needs to be backed up from
/var/lib/dpkg is on the nose as far as I can see.
</p>


<pre>
&gt; I'm not sure that backing up and preserving <em>dependency</em> data is a
&gt; useful thing to do.  Do you think it is?
</pre>

<p>
(After some cogitation I now realise that my reasons are as given
below. Thanks for making me think about this!)
</p>

<p>
I preserve this <strong>not</strong> as a way to re-install the system but as a way
to preserve my usual way of managing packages.
</p>

<p>
To re-install the system, I would just use "dpkg --get-selections"
and if the upstream has evolved considerably, then resolve the
dependencies that arise.
</p>

<p>
However, as I test/tryout a number of packages, I need a good package
management tool (p-m-t) for a running system. This tool needs to have
dependency information regarding which alternative out of the various
packages providing a dependency I prefer. Depending on the p-m-t the
data is stored in different places in /var/lib and I would want to
back that up.
</p>

<p>
This is not very different from backing up /var/lib/mysql and other
data which is stored by the system for shared use by different users.
(Perhaps that is close to the definition of /var/lib in FHS).
</p>

<p>
So perhaps the correct instruction for backup would be:
<pre>
	Look closely at /var/lib for data created by users
	that is stored there instead of in their home
	directories because it is shared.
 
	If such data has been generated automatically by the system
	using content available elsewhere (such as most contents of
	/var/lib/dpkg and /var/lib/texmf) then you should/could skip it. 
</pre>
I believe that the latter class should usually be in /var/cache but
this is often a somewhat fine distinction (and hence a source of
flame-wars <img src="../gx/frown.png" alt=":-(">) .
</p>

<p>
Regards,
</p>

<p>
Kapil.
--
</p>


<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Karl-Heinz Herrmann [khh at khherrmann.de]
</p>
</b><br />
<b>Wed, 18 Jul 2007 21:23:45 +0200</b>
</p>

<p>
On Sat, 14 Jul 2007 13:36:07 -0400
Ben Okopnik &lt;ben@linuxgazette.net&gt; wrote:
</p>


<pre>
&gt; "backuppc" requires a hard
&gt; drive (well, maybe. I'll have to play with it and find out.) Any other
</pre>

<p>
FLASH drive should do <img src="../gx/smile.png" alt=":-)"> just make it ext2/3 not VFAT
</p>

<p>
K.-H.
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]
</p>
</b><br />
<b>Fri, 20 Jul 2007 21:44:48 -0400</b>
</p>

<p>
On Wed, Jul 18, 2007 at 09:23:45PM +0200, Karl-Heinz Herrmann wrote:
</p>

<pre>
&gt; On Sat, 14 Jul 2007 13:36:07 -0400
&gt; Ben Okopnik &lt;ben@linuxgazette.net&gt; wrote:
&gt; 
&gt; &gt; "backuppc" requires a hard
&gt; &gt; drive (well, maybe. I'll have to play with it and find out.) Any other
&gt; 
&gt; FLASH drive should do <img src="../gx/smile.png" alt=":-)"> just make it ext2/3 not VFAT
</pre>

<pre>
tar cvjSpf - -T file_list &gt; /mnt/flash/backup-`date +%s`.tbz
</pre>
<img src="../gx/smile.png" alt=":)">
</p>

<p>
That would take care of it even if it was VFAT - but yeah, that's too
much of a pain. You're right; it's much better to stick with a <em>real</em>
filesystem.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href="http://LinuxGazette.NET">http://LinuxGazette.NET</a> *
</pre>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-backup_software_strategies">Back</a><hr width="50%" align="left" /><p><br /></p></div>
</body>
</html>