<!DOCTYPE html
	PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
	 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="utf-8" xml:lang="utf-8">
<head>
<title>Searching for Text (Part I) LG #149</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" href="../lg.css" type="text/css" media="screen, projection"  />
<link rel="alternate" type="application/rss+xml" title="LG RSS" href="lg.rss" />
<link rel="alternate" type="application/rdf+xml" title="LG RDF" href="lg.rdf" />
<!-- link rel="alternate" type="application/atom+xml" title="LG Atom" href="lg.atom.xml" / -->
<link rel="shortcut icon" href="../favicon.ico" />

<style type="text/css" media="screen, projection">
<!--

-->
</style>

</head>
<body>

<a href="../">
<img src="../gx/2003/newlogo-blank-200-gold2.jpg" id="logo" alt="Linux Gazette"/>
</a>
<p id="fun">...making Linux just a little more fun!</p>

<div id="navigation">
<table summary="masthead" width="100%">
<tr>
<td align="center" colspan="3" style="font-size: 10px; font-weight: bold">
<a href="../index.html">Home</a>
<a href="http://linuxgazette.net">Main Site</a>
<a href="../faq/index.html">FAQ</a>

<a href="../lg_index.html">Site Map</a>
<a href="../mirrors.html">Mirrors</a>
<a href="../mirrors.html">Translations</a>
<a href="../search.html">Search</a>
<a href="../archives.html">Archives</a>
<a href="../authors/index.html">Authors</a>
<a href="http://lists.linuxgazette.net/mailman/listinfo/">Mailing Lists</a>
<a href="../jobs.html">Join Us!</a>
<a href="../contact.html">Contact Us</a>

<hr width="99%" style="border: 1px inset #000033">
</td>
</tr>
<tr>
<td width="40%" align="left" style="font-size: 10px; font-weight: bold">
The Free International Online Linux Monthly
</td>
<td width="20%" align="center" style="font-size: 10px; font-weight: bold">
ISSN: 1934-371X
</td>
<td width="40%" align="right" style="font-size: 10px; font-weight: bold">
Main site: <a href="http://linuxgazette.net">http://linuxgazette.net</a> 
</td>
</table>
</div>


<div id="breadcrumbs1">

<a href="../index.html">Home</a> &gt; 
<a href="index.html">April 2008 (#149)</a> &gt; 
Article

</div>

<div class="articlecontent1">
<div class="content">

<div id="previousnexttop">
<A HREF="melinte.html" >&lt;-- prev</A> | <A HREF="prestia.html" >next --&gt;</A>
</div>

<h1>Searching for Text (Part I)</h1>
<p id="by"><b>By <a href="../authors/pfeiffer.html">Ren&eacute; Pfeiffer</a></b></p>

<p>
Do you deal a lot with reading or writing text? Do you often use search
tools? Do you have a pile of data sitting on your web and file
server(s)? Many of us do. How do you organise your collection of text
data? Do you use a directory, an index, or a database? In case you
haven't decided yet, let me suggest a few options.
</p>

<h2>
Documents and Dealing with Text
</h2>

<p>
I will focus on organising, indexing, and searching text data. This is
sufficient, since a lot of search queries can be transformed to text. In
addition, processing text is harder than it seems, so it's good to have
a focus. You may note that I make a distinction between documents
and text data; the reason is the sheer volume of different document
formats. Some of them are well-defined, some aren't. Some have open
specifications readily available to developers. Proprietary document
formats are always a barrier for data processing. Unfortunately, these
formats cannot be avoided.
</p>

<p>
The first thing you have to do is to organise your data in some way. It
doesn't matter if you populate a file server with a directory structure
and start copying data or if you keep a list of bookmarks in your
browser. The most important aspect is to have a kind of unique
identifier or reference to every single document. Uniform Resource
Locators (URLs) work well; a path to a file along with its name will
also be perfect. It's best if you manage to group your documents by a
list of categories. The next thing you have to consider is the document
formats. Most indexing and search tools can only handle text, so if your
document format allows for conversions, then it is useful for processing.
Here are some examples for conversions done in shell scripts.

<ol>
<li> PDF: <tt>pdftotext -q -eol unix -enc UTF-8 $IN - &gt; $OUT</tt></li>
<li> Postscript: <tt>pstotext $IN | iconv -f ISO-8859-1 -t UTF-8 -o $OUT -</tt></li>
<li> MS Word: <tt>antiword $IN &gt; $OUT</tt></li>
<li> HTML: <tt>html2text -nobs -o $OUT $IN</tt></li>
<li> RTF: <tt>unrtf --nopict --text $IN &gt; $OUT</tt></li>
<li> MS Excel: <tt>py_xls2txt $IN &gt; $OUT</tt></li>
<li> any OpenOffice document: <tt>ooo_as_text $IN &gt; $OUT</tt></li>
</ol>

The variable <tt>$IN</tt> denotes the source document and <tt>$OUT</tt>
is the name and location of the converted content in plain text. In
order to capture all possible character encodings, it is always useful 
to convert to a suitable Unicode encoding. I usually use UTF-8 for this
purpose. Converting to UTF-8 from any other encoding works well;
converting from UTF-8 to an encoding having fewer representations of
characters is "lossy" and is usually not precise enough to be useful.
</p>

<p>
Keep in mind that although some converters can deal with MS Office
documents, it is not the best format for storing information. The format is
still proprietary and you may not use Microsoft's "free" document
specification for any purpose (commercial use is explicitly excluded,
therefore the specs are not free to use). Storing information in these
formats will cause a lot of trouble - especially if the vendor disables old
versions of the format by software updates (this has already happened).
That's a clear and obvious warning, and if you have any word in how to
organise document collections you can avoid a lot of trouble at the
beginning.
</p>

<p>
Having thought about organising the data, we can now consider how to best
index it. This doesn't mean that you are done with thinking about the
organisation of the data - it really is the most important step.
</p>

<h2>
MySQL Natural Language Full-Text Searches
</h2>

<p>
MySQL offers the creation of full text indices; this is described in the
manual in the "Natural Language Full-Text Searches" section. It is an
easy way of indexing text data. Let's say you have the following table:

<pre>
CREATE DATABASE textsearch;
USE textsearch;
CREATE TABLE documents (
    id INT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY,
    filename VARCHAR(255) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL,
    path VARCHAR(255) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL,
    type VARCHAR(255) CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL,
    mtime TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    content TEXT CHARACTER SET utf8 COLLATE utf8_unicode_ci NOT NULL,
    FULLTEXT (filename),
    FULLTEXT (content)
);
</pre>

We'll store the filename, the path information, the file type, and its
converted content in a database table. The <tt>VARCHAR</tt> data type might
be too small if you have big directory trees, but it's more than enough for a
simple example. Every document has a unique ID consisting of the field
<tt>id</tt>. The option <tt>FULLTEXT()</tt> advises MySQL to create a full
text search index over the columns <tt>filename</tt> and <tt>content</tt>.
You can add more columns if you like, but you need to be careful not to
index everything. Adding the <tt>type</tt> column might also be a
reasonable option.
</p>

<p>
Now we need some content - so let's insert a few records for testing.

<pre>
INSERT INTO documents ( filename, path, type, content )
   VALUES ( 'gpl.txt', '/home/pfeiffer', 'Text', 'This program is free software; 
   you can redistribute it and/or modify it under the terms of the GNU General 
   Public License as published by the Free Software Foundation;' );
INSERT INTO documents ( filename, path, type, content )
   VALUES ( 'fortune.txt', '/home/pfeiffer', 'Text', 'It was all so different 
   before everything changed.' );
INSERT INTO documents ( filename, path, type, content )
   VALUES ( 'lorem.txt', '/home/pfeiffer', 'Text', 'Lorem ipsum dolor sit amet,
   consectetuer adipiscin...' );
</pre>

<p>
Now you can do a full text search.
</p>

<pre>
mysql&gt; SELECT id,filename FROM documents WHERE MATCH(content) AGAINST('lorem');
+----+----------+
| id | filename |
+----+----------+
|  6 | test.txt |
+----+----------+
1 row in set (0.01 sec)

mysql&gt;
</pre>
<p>
The construct <tt>MATCH() AGAINST()</tt> does the full text search for you. MySQL uses
a number to indicate the relevance of the table record. You can show all these rankings
by querying for <tt>MATCH() AGAINST()</tt>.
</p>
<pre>
mysql&gt; SELECT id, filename, MATCH(content) AGAINST('lorem') FROM documents;
+----+-------------+---------------------------------+
| id | filename    | MATCH(content) AGAINST('lorem') |
+----+-------------+---------------------------------+
|  1 | gpl.txt     |                               0 |
|  2 | fortune.txt |                               0 |
|  3 | s3.txt      |                               0 |
|  4 | s4.txt      |                               0 |
|  5 | miranda.txt |                               0 |
|  6 | test.txt    |                0.75862580537796 |
+----+-------------+---------------------------------+
6 rows in set (0.00 sec)

mysql&gt;
</pre>

<p>
Obviously, I added a few more rows than described originally. The right
column displays the ranking. Only record 6 has a number greater than 0
because all the other texts lack the word <em>lorem</em>. Now you can add
more texts and see what their rating is like. Note that MySQL uses a
specific strategy when performing full text indexing:
</p>

<ul>
<li> It does not index words with less than 4 characters.
<li> It also does not index any word that appears in more than 50% of the
text lines.
<li> Hyphenated words count as two words.
<li> Any partial words are ignored.
<li> The database engine also maintains a list of common English words (<em>stop words</em>); they are ignored,
too.
</ul>

<p>
Be careful - if your search query consists solely of stop words, you'll
never get any results. If you need a full text search in languages other
than English you can provide your own set of stop words. The documentation
will tell you how to do this.
</p>

<p>
It is also possible to search for more than one word. You can add
multiple words separated by commas.
<pre>
SELECT id, filename, MATCH(content) AGAINST('lorem,ipsum') FROM documents;
</pre>
</p>

<h2>
Full Text Search with PostgreSQL
</h2>

<p>
Of course PostgreSQL can also deal with full text searches - a plugin
called <em>Tsearch2</em> is available for PostgreSQL database servers prior
to version 8.3.0 (it's integrated into 8.3.0). Just like for the MySQL
functions, you can fine-tune these according to the language your texts are
written in. The content has to be transformed into tokens, and PostgreSQL
offers new database objects that deal with these operations. The Tsearch2
engine provides text parsers for tokenisation, dictionaries for
normalisation of tokens (and lists of stop words), templates for switching
between parsers or dictionaries, and configurations to use whatever
language you need to. Creating new database objects requires knowledge of C
programming.
</p>
<p>
Let's recreate the example table in PostgreSQL (I use version 8.3.0; if
you have an older version, please install Tsearch2):
<pre>
CREATE TABLE documents (
 id_documents serial,
 filename character varying(254),
 path character varying(254),
 type character varying(254),
 mtime timestamp with time zone,
 content text );
CREATE INDEX documents_idx ON documents USING gin(to_tsvector('english',content));
</pre>

<p>
First we create the table, then we create the text GIN (Generalized
Inverted Index); this type of index consists of distinct <a
href="http://en.wikipedia.org/wiki/Lexeme"><em>lexemes</em></a>. The
function <tt>to_tsvector()</tt> converts the text stored in the content
column into these normalised words. It uses the English parser and
dictionary. Search queries look like this:
</p>

<pre>
lynx=&gt; SELECT filename,mtime FROM documents WHERE to_tsvector(content) @@ to_tsquery('lorem');
 filename  |            mtime             
-----------+------------------------------
 lorem.txt | 2008-02-26 12:15:16.34584+01
(1 row)

lynx=&gt;
</pre>

<p>
You'd use a normal <tt>SELECT</tt> and the <tt>@@</tt> text match operator. This operator 
compares arguments and the search string converted to lexemes by use of <tt>to_tsvector()</tt>
and <tt>to_tsquery()</tt> functions. The results are returned by the <tt>SELECT</tt> statement.
You can also use ranking in order to sort the results.
</p>

<pre>
lynx=&gt; SELECT filename,mtime,ts_rank(to_tsvector(content),to_tsquery('lorem'))
          FROM documents WHERE to_tsvector(content) @@ to_tsquery('lorem');
 filename  |            mtime             |  ts_rank  
-----------+------------------------------+-----------
 lorem.txt | 2008-02-26 12:15:16.34584+01 | 0.0607927
(1 row)

lynx=&gt;
</pre>
</p>

<p>
The tokenisation is one of the crucial parts of a text search, and it's
important to understand the algorithms that Postgres uses to decompose a
string. Consider the following example:
</p>

<pre>
lynx=&gt; SELECT alias, description, token FROM ts_debug('copy a complete database');
   alias   |   description   |  token   
-----------+-----------------+----------
 asciiword | Word, all ASCII | copy
 blank     | Space symbols   |  
 asciiword | Word, all ASCII | a
 blank     | Space symbols   |  
 asciiword | Word, all ASCII | complete
 blank     | Space symbols   |  
 asciiword | Word, all ASCII | database
(7 rows)

lynx=&gt;
</pre>

<p>
The example uses the <tt>ts_debug()</tt> function and shows every token
with its classification.  The text search module understands most of the
common text constructs; it can also decode URLs.
</p>

<pre>
lynx=&gt; SELECT alias, description, token FROM ts_debug('http://linuxgazette.net/145/lg_tips.html');
  alias   |  description  |               token               
----------+---------------+-----------------------------------
 protocol | Protocol head | http://
 url      | URL           | linuxgazette.net/145/lg_tips.html
 host     | Host          | linuxgazette.net
 url_path | URL path      | /145/lg_tips.html
(4 rows)

lynx=&gt;
</pre>

<p>
Now the parser displays the tokens as part of the URL and identifies
them. The tokens allow for better search query processing, and this is
the reason why you have to filter your query string. The text search
compares tokens and not the strings themselves.
</p>

<h2>
Conclusion
</h2>

<p>
I presented only two ways to index text data. This is really only the tip
of the iceberg - there's a lot more to learn about full text searches. Both
MySQL and PostgreSQL have convenient algorithms ready for use that
facilitate finding documents. You can use a simple Perl script with either
one of these database engines, feed them your browser bookmarks and build
an index with the content of the web pages, ready for search queries. There
are many other tools available, and I'll present another way of indexing in
the next part of this series. If you use something different or interesting
to accomplish these tasks, please write and let us know about it!
</p>

<h2>
Useful resources
</h2>

<p>
<ul>
<li> <a href="http://www.winfield.demon.nl/">antiword</a> - a free MS Word document reader</li>
<li> <a href="http://www.lipsum.com/">Lorem Ipsum generator</a></li>
<li> <a href="http://dev.mysql.com/doc/refman/5.0/en/fulltext-natural-language.html">MySQL Natural Language Full-Text Searches</a></li>
<li> <a href="http://ooopy.sourceforge.net/">OOoPy: Modify OpenOffice.org documents in Python</a></li>
<li> <a href="http://www.postgresql.org/docs/8.3/static/textsearch.html">PostgreSQL documentation - Full Text Search</a></li>
<li> <a href="http://sourceforge.net/projects/pyexcelerator">pyexcelerator</a> - MS Excel reader written in Python</li>
<li> <a href="http://www.sai.msu.su/~megera/postgres/gist/tsearch/V2/">Tsearch2 - full text extension for PostgreSQL</a></li>
<li> <a href="http://www.gnu.org/software/unrtf/unrtf.html">UnRTF</a></li>
<li> <a href="http://www.databasejournal.com/features/mysql/article.php/1578331">Using Fulltext Indexes in MySQL</a></li>
<li> <a href="http://www.joelonsoftware.com/items/2008/02/19.html">Why are the Microsoft Office file formats so complicated?</a></li>
<li> <a href="http://wvware.sourceforge.net/">wvWare</a> - Microsoft&reg; Word document converter</li>
<li> <a href="http://www.foolabs.com/xpdf/home.html">Xpdf</a> tool for PDF viewing and conversion</li>
</ul>


<script type='text/javascript'>
digg_url = 'http://linuxgazette.net/149/pfeiffer.html';
digg_title = 'Searching for Text (Part I)';
digg_bodytext = '<p> Do you deal a lot with reading or writing text? Do you often use search tools? Do you have a pile of data sitting on your web and file server(s)? Many of us do. How do you organise your collection of text data? Do you use a directory, an index, or a database? In case you haven\'t decided yet, let me suggest a few options. </p> ';
digg_topic = 'linux_unix';
</script>
<script src="http://digg.com/tools/diggthis.js" type="text/javascript"></script>


<p class="talkback">
Talkback: <a
href="mailto:tag@lists.linuxgazette.net?subject=Talkback:149/pfeiffer.html">Discuss this article with The Answer Gang</a>
</p>

<!-- *** BEGIN author bio *** -->
<!-- *** BEGIN bio *** -->
<hr>
<p>

<img align="left" alt="Bio picture" src="../gx/authors/pfeiffer.jpg" class="bio">

</p>
<em>

<p>
Ren&eacute; was born in the year of Atari's founding and the release of the game Pong.
Since his early youth he started taking things apart to see how they work. He
couldn't even pass construction sites without looking for electrical wires that
might seem interesting. The interest in computing began when his grandfather
bought him a 4-bit microcontroller with 256 byte RAM and a 4096 byte operating
system, forcing him to learn assembler before any other language.
</p>

<p>
After finishing school he went to university in order to study physics. He then
collected experiences with a C64, a C128, two Amigas, DEC's Ultrix, OpenVMS and
finally GNU/Linux on a PC in 1997. He is using Linux since this day and still
likes to take things apart und put them together again. Freedom of tinkering
brought him close to the Free Software movement, where he puts some effort into
the right to understand how things work. He is also involved with civil liberty
groups focusing on digital rights.
</p>

<p>
Since 1999 he is offering his skills as a freelancer. His main activities
include system/network administration, scripting and consulting. In 2001 he
started to give lectures on computer security at the Technikum Wien. Apart from
staring into computer monitors, inspecting hardware and talking to network
equipment he is fond of scuba diving, writing, or photographing with his digital
camera. He would like to have a go at storytelling and roleplaying again as soon
as he finds some more spare time on his backup devices.
</p>

</em>
</p>
<br clear="all">
<!-- *** END bio *** -->

<!-- *** END author bio *** -->

<div id="articlefooter">

<p>
Copyright &copy; 2008, Ren&eacute; Pfeiffer. Released under the <a
href="http://linuxgazette.net/copying.html">Open Publication License</a>
unless otherwise noted in the body of the article. Linux Gazette is not
produced, sponsored, or endorsed by its prior host, SSC, Inc.
</p>

<p>
Published in Issue 149 of Linux Gazette, April 2008
</p>

</div>

<div id="previousnextbottom">
<A HREF="melinte.html" >&lt;-- prev</A> | <A HREF="prestia.html" >next --&gt;</A>
</div>

</div>
</div>

<script src="http://www.google-analytics.com/urchin.js"
type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-1204316-1";
urchinTracker();
</script>







<img src="../gx/tux_86x95_indexed.png" id="tux" alt="Tux"/>

</body>
</html>

