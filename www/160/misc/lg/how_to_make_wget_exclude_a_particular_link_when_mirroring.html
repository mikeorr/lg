<!DOCTYPE html PUBLIC '-//W3C//DTD XHTML 1.0 Transitional//EN'
		 'http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd'>
<html xmlns='http://www.w3.org/1999/xhtml' lang='utf-8' xml:lang='utf-8'>
<head>
<title>How to make wget exclude a particular link when mirroring</title>
<meta http-equiv='Content-Type; charset=utf-8' />
<link rel='stylesheet' type='text/css' href='../../../lg.css' />
</head>
<body>
<a href="../../../"><img alt="Linux Gazette" src="../../../gx/2003/newlogo-blank-200-gold2.jpg" id="logo" /></a><img alt="Tux" src="../../../gx/tux_86x95_indexed.png" id="tux" /><p id="fun">...making Linux just a little more fun!</p><div class='content articlecontent'><a name="top"></a><h3>How to make wget exclude a particular link when mirroring</h3>
<p>
<b><p>
Suramya Tomar [security at suramya.com]

</p>
</b><br />
<b>Wed, 04 Feb 2009 18:36:50 +0530</b>
</p>

<p>
Hey Everyone,
</p>

<p>
I am trying to mirror an Invision Powerboard forum locally on my system 
(With permission from the admin) using wget and I am having issues.
</p>

<p>
When I start downloading wget visits each and every link and makes a 
local copy (like its supposed to) but in this process it also visits the 
"Log out" link which logs me out from the site and then I am unable to 
download the remaining links.
</p>

<p>
So I need to figure out how to exclude the Logout link from the process. 
The logout link looks like: www.website.com/index.php?act=Login&amp;CODE=03
So I tried the following:
</p>

<pre>
wget -X "*CODE*" --mirror --load-cookies=/var/www/cookiefile.txt 
<a href='http://www.website.com'>http://www.website.com</a>
</pre>

<p>
but it didn't work.
</p>

<p>
I can't exclude the index.php itself because all the links are based off 
the index.php with parameters.
</p>

<p>
I have tried searching the web but didn't find anything relevant.
</p>

<p>
Any ideas on how to do it?
</p>

<p>
Thanks,
</p>

<p>
Suramya
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]

</p>
</b><br />
<b>Wed, 4 Feb 2009 08:54:07 -0500</b>
</p>

<p>
On Wed, Feb 04, 2009 at 06:36:50PM +0530, Suramya Tomar wrote:
</p>

<pre>
&gt; 
&gt; When I start downloading wget visits each and every link and makes a 
&gt; local copy (like its supposed to) but in this process it also visits the 
&gt; "Log out" link which logs me out from the site and then I am unable to 
&gt; download the remaining links.
&gt; 
&gt; So I need to figure out how to exclude the Logout link from the process. 
&gt; The logout link looks like: www.website.com/index.php?act=Login&amp;CODE=03
</pre>

<p>
Seems like the '-R' should do it. From the "wget" man page:
</p>

<pre>
  -R rejlist --reject rejlist
	  Specify comma-separated lists of file name suffixes or patterns to
      accept or reject (@pxref{Types of Files} for more details).
</pre>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Suramya Tomar [security at suramya.com]

</p>
</b><br />
<b>Wed, 04 Feb 2009 19:38:11 +0530</b>
</p>

<p>
Hi Ben,
</p>


<pre>
&gt;&gt; So I need to figure out how to exclude the Logout link from the process. 
&gt;&gt; The logout link looks like: www.website.com/index.php?act=Login&amp;CODE=03
&gt; 
&gt; Seems like the '-R' should do it. From the "wget" man page:
&gt; 
&gt; ``
&gt;   -R rejlist --reject rejlist
&gt; 	  Specify comma-separated lists of file name suffixes or patterns to
&gt;       accept or reject (@pxref{Types of Files} for more details).
&gt; ''
</pre>

<p>
Unfortunately that didn't work. It still logged me out.
</p>

<p>
According to: <a href='http://www.mail-archive.com/wget@sunsite.dk/msg10956.html'>http://www.mail-archive.com/wget@sunsite.dk/msg10956.html</a>
</p>

<pre>
-------
As I currently understand it from the code, at least for Wget 1.11,
matching is against the _URL_'s filename portion (and only that portion:
no query strings, no directories) when deciding whether it should
download something through a recursive descent (the relevant spot in the
code is in recur.c, marked by a comment starting "6. Check for
acceptance/rejection rules.").
-------
</pre>

<p>
Is there any other way to do this? Maybe some other tool?
</p>

<p>
- Suramya
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]

</p>
</b><br />
<b>Wed, 4 Feb 2009 09:28:46 -0500</b>
</p>

<p>
On Wed, Feb 04, 2009 at 07:38:11PM +0530, Suramya Tomar wrote:
</p>

<pre>
&gt; Hi Ben,
&gt; 
&gt; &gt;&gt; So I need to figure out how to exclude the Logout link from the process. 
&gt; &gt;&gt; The logout link looks like: www.website.com/index.php?act=Login&amp;CODE=03
&gt; &gt; 
&gt; &gt; Seems like the '-R' should do it. From the "wget" man page:
&gt; &gt; 
&gt; &gt; ``
&gt; &gt;   -R rejlist --reject rejlist
&gt; &gt; 	  Specify comma-separated lists of file name suffixes or patterns to
&gt; &gt;       accept or reject (@pxref{Types of Files} for more details).
&gt; &gt; ''
&gt; 
&gt; Unfortunately that didn't work. It still logged me out.
</pre>

<p>
What didn't work? What, exactly, did you try?
 
</p>

<pre>
&gt; According to: <a href='http://www.mail-archive.com/wget@sunsite.dk/msg10956.html'>http://www.mail-archive.com/wget@sunsite.dk/msg10956.html</a>
&gt; 
&gt; -------
&gt; As I currently understand it from the code, at least for Wget 1.11,
&gt; matching is against the _URL_'s filename portion (and only that portion:
&gt; no query strings, no directories) when deciding whether it should
&gt; download something through a recursive descent (the relevant spot in the
&gt; code is in recur.c, marked by a comment starting "6. Check for
&gt; acceptance/rejection rules.").
&gt; -------
</pre>

<p>
I've just looked at the source, and it seems to me that the rule
immediately above that one contradicts this.
</p>

<pre>
  /* 5. If the file does not match the acceptance list, or is on the
     rejection list, chuck it out.  The same goes for the directory
     exclusion and inclusion lists.  */
</pre>

<p>
I didn't dig into the code (I've forgotten C to such an extent that even
reading it is very difficult for me), but this seems like a rejection
mechanism that runs in addition to the one in #6.
</p>


<pre>
&gt; Is there any other way to do this? Maybe some other tool?
</pre>

<p>
No, you can't use another tool. Since this is a decision that is made
internally by "wget", you need to instruct "wget" itself to make that
decision.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Kapil Hari Paranjape [kapil at imsc.res.in]

</p>
</b><br />
<b>Wed, 4 Feb 2009 20:12:41 +0530</b>
</p>

<p>
Hello,
</p>

<p>
On Wed, 04 Feb 2009, Suramya Tomar wrote:
</p>

<pre>
&gt; &gt; ``
&gt; &gt;   -R rejlist --reject rejlist
&gt; &gt; 	  Specify comma-separated lists of file name suffixes or patterns to
&gt; &gt;       accept or reject (@pxref{Types of Files} for more details).
&gt; &gt; ''
&gt; 
&gt; Unfortunately that didn't work. It still logged me out.
</pre>

<p>
1. The "info" pages have more information than the man pages (I
think). 
</p>

<p>
2. In particular, note that -R something is treated as a pattern if
it contains the ? character so you will need to escape that
character. Did you?
</p>

<p>
Kapil.
--
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Suramya Tomar [security at suramya.com]

</p>
</b><br />
<b>Wed, 04 Feb 2009 20:39:23 +0530</b>
</p>

<p>
Hi Ben,
</p>


<pre>
&gt;&gt; Unfortunately that didn't work. It still logged me out.
&gt; 
&gt; What didn't work? What, exactly, did you try?
</pre>

<p>
The command I tried was:
</p>

<pre>
wget -R "*CODE*" --mirror --load-cookies=/var/www/cookiefile.txt 
<a href='http://www.website.com/index.php'>http://www.website.com/index.php</a>
</pre>

<p>
The download started fine, but as soon as it hit the logout link, I got 
logged out and the remaining pages downloaded kept showing me the login 
page instead of the content.
</p>


<pre>
&gt; No, you can't use another tool. Since this is a decision that is made
&gt; internally by "wget", you need to instruct "wget" itself to make that
&gt; decision.
</pre>

<p>
What I meant was, if wget doesn't support this then is there some other 
program that I can use to mirror the site?
</p>

<p>
Thanks,
  Suramya
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]

</p>
</b><br />
<b>Wed, 4 Feb 2009 10:10:08 -0500</b>
</p>

<p>
On Wed, Feb 04, 2009 at 08:12:41PM +0530, Kapil Hari Paranjape wrote:
</p>

<pre>
&gt; Hello,
&gt; 
&gt; On Wed, 04 Feb 2009, Suramya Tomar wrote:
&gt; &gt; &gt; ``
&gt; &gt; &gt;   -R rejlist --reject rejlist
&gt; &gt; &gt; 	  Specify comma-separated lists of file name suffixes or patterns to
&gt; &gt; &gt;       accept or reject (@pxref{Types of Files} for more details).
&gt; &gt; &gt; ''
&gt; &gt; 
&gt; &gt; Unfortunately that didn't work. It still logged me out.
&gt; 
&gt; 1. The "info" pages have more information than the man pages (I
&gt; think). 
&gt; 
&gt; 2. In particular, note that -R something is treated as a pattern if
&gt; it contains the ? character so you will need to escape that
&gt; character. Did you?
</pre>

<p>
Actually, except in odd cases, that specific one won't matter - since
'?' in the shell means 'any single character'... which would include
'?'. It can, however, give you false positives: i.e., "foo?bar" will
match the literal string specified, but it will also match "foo=bar" and
so on.
</p>

<p>
In any case - I've just tested this out, and '-R' does indeed work as it
should, at least on recursive retrievals. First, I made up a little CGI
proglet, something that would return actual output when given
parameters, and placed it in my WEBROOT/test directory:
</p>

<p>
<pre class='code'>
#!/usr/bin/perl -w
# Created by Ben Okopnik on Wed Feb  4 09:32:13 EST 2009
use CGI qw/:standard :cgi-lib/;
 
my %params = %{Vars()};
 
print header, start_html,
    map({"$_ =&gt; $params{$_}&lt;br&gt;"} keys %params), end_html;
</pre>

<p>
Then, I created a file containing a list of URLs to download:
</p>

<pre>
<a href='http://localhost/test/foo.cgi?a=b'>http://localhost/test/foo.cgi?a=b</a>
<a href='http://localhost/test/foo.cgi?c=d'>http://localhost/test/foo.cgi?c=d</a>
<a href='http://localhost/test/foo.cgi?foo=bar'>http://localhost/test/foo.cgi?foo=bar</a>
</pre>

<p>
Last, I ran "wget" with the appropriate options: '-nd' for "no
directories" - I want to see the downloaded files in the current dir;
'-r' for recursive - accept and reject lists only work with recursive
retrievals; '-R foo=bar' - to ignore all URLs containing that string;
and '-i input_file' to read in the above URLs.
</p>

<pre>
 wget -nd -rR 'foo=bar' -i input_file
</pre>

<p>
Result:
</p>

<p>
<pre class='code'>
ben@Tyr:/tmp/testwget$ ls -l
total 12
-rw-r--r-- 1 ben ben 355 2009-02-04 10:01 foo.cgi?a=b
-rw-r--r-- 1 ben ben 355 2009-02-04 10:01 foo.cgi?c=d
-rw-r--r-- 1 ben ben 106 2009-02-04 09:46 input_file
</pre>

<p>
Three URLs, two downloaded files.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]

</p>
</b><br />
<b>Wed, 4 Feb 2009 10:20:35 -0500</b>
</p>

<p>
On Wed, Feb 04, 2009 at 08:39:23PM +0530, Suramya Tomar wrote:
</p>

<pre>
&gt; Hi Ben,
&gt; 
&gt; &gt;&gt; Unfortunately that didn't work. It still logged me out.
&gt; &gt; 
&gt; &gt; What didn't work? What, exactly, did you try?
&gt; 
&gt; The command I tried was:
&gt; 
&gt; wget -R "*CODE*" --mirror --load-cookies=/var/www/cookiefile.txt 
&gt; <a href='http://www.website.com/index.php'>http://www.website.com/index.php</a>
</pre>

<p>
Oh... dear. Suramya, you're not supposed to retype what you entered; you
should always copy and paste your original entry. After being here in
TAG for so long, I figured you'd know that; we ding people for doing
that regularly.
</p>

<p>
Nobody wants to troubleshoot retyping errors or deal with the poster
skipping the "non-important" parts - and that's exactly what happened
here. I'm sure that you didn't actually use the string '*CODE*' in your
original test - and the one thing that I actually needed to know was
what you <em>did</em> type there. Since you retyped, I have to ask again.
 
</p>

<pre>
&gt; &gt; No, you can't use another tool. Since this is a decision that is made
&gt; &gt; internally by "wget", you need to instruct "wget" itself to make that
&gt; &gt; decision.
&gt; 
&gt; What I meant was, if wget doesn't support this then is there some other 
&gt; program that I can use to mirror the site?
</pre>

<p>
FTP or 'rsync', since you have the admin's permission? Those wouldn't be
doing any interpretation; they'd just download the files.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Suramya Tomar [security at suramya.com]

</p>
</b><br />
<b>Wed, 04 Feb 2009 20:54:35 +0530</b>
</p>

<p>
Hi Ben,
</p>


<pre>
&gt; Last, I ran "wget" with the appropriate options: '-nd' for "no
&gt; directories" - I want to see the downloaded files in the current dir;
&gt; '-r' for recursive - accept and reject lists only work with recursive
&gt; retrievals; '-R foo=bar' - to ignore all URLs containing that string;
&gt; and '-i input_file' to read in the above URLs.
</pre>

<p>
It worked when I used it -r option instead of --mirror. I thought that 
the -R option would override the --mirror option but I guess thats not 
the case.
</p>

<p>
The command I used to start the download was:
</p>

<pre>
wget -R "*CODE*" -r --load-cookies=/var/www/cookiefile.txt 
<a href='http://www.website.com/index.php'>http://www.website.com/index.php</a>
</pre>

<p>
It correctly rejected all URL's with 'CODE' in them.
</p>

<p>
Thanks for the help.
</p>

<p>
- Suramya
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Suramya Tomar [security at suramya.com]

</p>
</b><br />
<b>Wed, 04 Feb 2009 21:01:43 +0530</b>
</p>

<p>
Hi Ben,
</p>


<pre>
&gt; Oh... dear. Suramya, you're not supposed to retype what you entered; you
&gt; should always copy and paste your original entry. After being here in
</pre>

<p>
I did copy and paste. All I changed was the name of the website.
</p>


<pre>
&gt; Nobody wants to troubleshoot retyping errors or deal with the poster
&gt; skipping the "non-important" parts - and that's exactly what happened
&gt; here. I'm sure that you didn't actually use the string '*CODE*' in your
&gt; original test - and the one thing that I actually needed to know was
&gt; what you <em>did</em> type there. Since you retyped, I have to ask again.
</pre>

<p>
Actually, thats exactly what I typed because I wanted it to skip links 
like:
</p>

<pre>
index.php?act=Msg&amp;CODE=01
</pre>

<p>
and keep links like:
</p>

<pre>
index.php?showforum=1
</pre>

<p>
so I used <strong>CODE</strong> as my skip term.
</p>

<p>
Thanks,
</p>

<p>
Suramya
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Suramya Tomar [security at suramya.com]

</p>
</b><br />
<b>Wed, 04 Feb 2009 21:08:18 +0530</b>
</p>

<p>
Hey,
</p>


<pre>
&gt; It worked when I used it -r option instead of --mirror. I thought that 
&gt; the -R option would override the --mirror option but I guess thats not 
&gt; the case.
</pre>

<p>
I spoke a bit too soon. It downloaded the link to my system and then 
removed it. So in the process it logged me out of the system. <img src="../gx/frown.png" alt=":(">
</p>

<p>
<pre class='code'>
-------
Saving to: `www.website.com/index.php?act=Search&amp;CODE=getnew'
 
     [   &lt;=&gt;                                          ] 19,492 
26.9K/s   in 0.7s
 
2009-02-04 21:01:19 (26.9 KB/s) - 
`www.website.com/index.php?act=Search&amp;CODE=getnew' saved [19492]
 
Removing www.website.com/index.php?act=Search&amp;CODE=getnew since it 
should be rejected.
--------
</pre>


<p>
I am going to try using httrack and see if that works better.
</p>

<p>
Thanks,
</p>

<p>
Suramya
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]

</p>
</b><br />
<b>Wed, 4 Feb 2009 22:20:25 -0500</b>
</p>

<p>
On Wed, Feb 04, 2009 at 08:54:35PM +0530, Suramya Tomar wrote:
</p>

<pre>
&gt; Hi Ben,
&gt; 
&gt; &gt; Last, I ran "wget" with the appropriate options: '-nd' for "no
&gt; &gt; directories" - I want to see the downloaded files in the current dir;
&gt; &gt; '-r' for recursive - accept and reject lists only work with recursive
&gt; &gt; retrievals; '-R foo=bar' - to ignore all URLs containing that string;
&gt; &gt; and '-i input_file' to read in the above URLs.
&gt; 
&gt; It worked when I used it -r option instead of --mirror. I thought that 
&gt; the -R option would override the --mirror option but I guess thats not 
&gt; the case.
</pre>

<p>
Odd, since '--mirror' includes '-r' - at least according to the man
page.
 
</p>

<pre>
&gt; The command I used to start the download was:
&gt; 
&gt; wget -R "*CODE*" -r --load-cookies=/var/www/cookiefile.txt 
&gt; <a href='http://www.website.com/index.php'>http://www.website.com/index.php</a>
&gt; 
&gt; It correctly rejected all URL's with 'CODE' in them.
</pre>

<p>
Ah - my confusion, then. In theory, typing 'CODE' would have worked just
fine; the pattern match succeeds if the pattern is anywhere within the
URL. This, of course, means that it's better to be overly specific;
otherwise, you end up ignoring more than you expected.
 
</p>

<pre>
&gt; Thanks for the help.
</pre>

<p>
You're welcome - glad it worked for you!
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]

</p>
</b><br />
<b>Wed, 4 Feb 2009 22:21:12 -0500</b>
</p>

<p>
On Wed, Feb 04, 2009 at 09:01:43PM +0530, Suramya Tomar wrote:
</p>

<pre>
&gt; Hi Ben,
&gt; 
&gt; &gt; Oh... dear. Suramya, you're not supposed to retype what you entered; you
&gt; &gt; should always copy and paste your original entry. After being here in
&gt; 
&gt; I did copy and paste. All I changed was the name of the website.
</pre>

<p>
Ah. That was what confused me.
 
</p>

<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Francis Daly [francis at daoine.org]

</p>
</b><br />
<b>Thu, 5 Feb 2009 13:56:08 +0000</b>
</p>

<p>
On Wed, Feb 04, 2009 at 06:36:50PM +0530, Suramya Tomar wrote:
</p>

<p>
Hi there,
</p>


<pre>
&gt; I am trying to mirror an Invision Powerboard forum locally on my system 
&gt; (With permission from the admin) using wget and I am having issues.
</pre>


<pre>
&gt; So I need to figure out how to exclude the Logout link from the process. 
&gt; The logout link looks like: www.website.com/index.php?act=Login&amp;CODE=03
&gt; So I tried the following:
&gt; 
&gt; wget -X "*CODE*" --mirror --load-cookies=/var/www/cookiefile.txt 
&gt; <a href='http://www.website.com'>http://www.website.com</a>
&gt; 
&gt; but it didn't work.
</pre>

<p>
As the rest of the thread has shown, wget doesn't let you do this.
</p>

<p>
(The short version is: wget considers the url to be
</p>

<pre>
scheme://host/directory/file?query#fragment
</pre>

<p>
-X filters on "directory", -R filters on "file", nothing filters on
"query", which is where you need it.)
</p>

<p>
So, the choices are: 
</p>

<p>
* use something instead of wget. You mentioned you'll try httrack. I
don't have a better suggestion.
</p>

<p>
* use something as well as wget. You could use a proxy server which you
configure to prevent access to the specific "logout" url, so all other
requests go to the origin server.
</p>

<p>
* reconsider the original question. What is it you want to achieve? The
best local version of the website is probably a similarly-configured
web server with the same content on the backend. "Access to the useful
information" could just be a dump of the backend content in a suitable
file-based format. Straight http access to the front-end web server will
probably not give you that content easily.
</p>

<p>
Trying to mirror a dynamic website into local files is not always
easy. You end up with weird filenames and potentially duplicated
content. If you can get it to work for your case, go for it; but I'd be
slow to try it (again <img src="../gx/frown.png" alt=":-(">) unless I was sure it was the best method.
</p>

<p>
Good luck,
</p>

<p>
	f
<pre>-- 
Francis Daly        francis@daoine.org
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]

</p>
</b><br />
<b>Thu, 5 Feb 2009 09:50:01 -0500</b>
</p>

<p>
On Thu, Feb 05, 2009 at 01:56:08PM +0000, Francis Daly wrote:
</p>

<pre>
&gt; 
&gt; (The short version is: wget considers the url to be
&gt; 
&gt; scheme://host/directory/file?query#fragment
&gt; 
&gt; -X filters on "directory", -R filters on "file", nothing filters on
&gt; "query", which is where you need it.)
</pre>

<p>
'-R' definitely filters on both "file" and "query". Again, using the CGI
script that I put together earlier, and a list of URLs that looks like
this ('Tyr' is my local hostname):
</p>

<pre>
<a href='http://localhost/test/foo.cgi?a=b'>http://localhost/test/foo.cgi?a=b</a>
<a href='http://Tyr/test/foo.cgi?c=d'>http://Tyr/test/foo.cgi?c=d</a>
<a href='http://localhost/test/foo.cgi?xyz=zap'>http://localhost/test/foo.cgi?xyz=zap</a>
</pre>

<p>
<pre class='code'>
ben@Tyr:/tmp/test_wget$ wget -q -r -nd -nv -R 'foo.cgi*' -i list; ls -1; rm -f foo*
list
</pre>

<p>
Filtering on the filename succeeds - since 'foo.cgi*' matches all the
URLs, none are retrieved.
</p>

<p>
<pre class='code'>
ben@Tyr:/tmp/test_wget$ wget -q -r -nd -nv -R 'xyz=zap' -i list; ls -1; rm -f foo*
foo.cgi?a=b
foo.cgi?c=d
list
</pre>

<p>
So does filtering on the query; since "xyz=zap" matches the last URL,
only the first two are retrieved.
</p>

<p>
I think your suggestion of setting up a proxy server is excellent,
though. If there are no tools that will do this kind of precise
filtering, that would be the right answer.
</p>

<p>
On a slightly different topic, given Suramya's experience (i.e., "wget"
still retrieves the '-R' excluded file but deletes it afterwards), it
would make sense to file a bug report with the 'wget' maintainers. That
method definitely fails the "least surprise" test.
</p>

<p>
<a href='http://www.faqs.org/docs/artu/ch11s01.html'>http://www.faqs.org/docs/artu/ch11s01.html</a>
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Francis Daly [francis at daoine.org]

</p>
</b><br />
<b>Thu, 5 Feb 2009 16:13:10 +0000</b>
</p>

<p>
On Thu, Feb 05, 2009 at 09:50:01AM -0500, Ben Okopnik wrote:
</p>

<pre>
&gt; On Thu, Feb 05, 2009 at 01:56:08PM +0000, Francis Daly wrote:
</pre>

<p>
Hi there,
</p>


<pre>
&gt; &gt; (The short version is: wget considers the url to be
&gt; &gt; 
&gt; &gt; scheme://host/directory/file?query#fragment
&gt; &gt; 
&gt; &gt; -X filters on "directory", -R filters on "file", nothing filters on
&gt; &gt; "query", which is where you need it.)
&gt; 
&gt; '-R' definitely filters on both "file" and "query". 
</pre>

<p>
No, it doesn't.
</p>

<p>
Or at least: it doesn't before deciding whether or not to get the url.
</p>

<p>
Check your web server logs.
</p>


<pre>
&gt; ```
&gt; ben@Tyr:/tmp/test_wget$ wget -q -r -nd -nv -R 'xyz=zap' -i list; ls -1; rm -f foo*
&gt; foo.cgi?a=b
&gt; foo.cgi?c=d
&gt; list
&gt; '''
&gt; 
&gt; So does filtering on the query; since "xyz=zap" matches the last URL,
&gt; only the first two are retrieved.
</pre>

<p>
Or all are retrieved, and then some of the stored files are deleted. Which
is what seems to happen when I try the same test. According to access.log,
and "grep unlink" in the strace output.
</p>

<p>
And in the original case, it is the GET to the server which induces the
"logout" (presumably the invalidation of the current cookie).
</p>

<p>
Arguably, using a GET for "logout" is unwise, since it effectively changes
state somewhere. But it is idempotent -- make the same request repeatedly
and nothing extra should happen -- so is justifiable on that basis.
</p>

<p>
The combination of GET for logout and the somewhat-unexpected observed
wget behaviour does seem to break this use-case, sadly.
</p>

<p>
	f
<pre>-- 
Francis Daly        francis@daoine.org
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Neil Youngman [Neil.Youngman at youngman.org.uk]

</p>
</b><br />
<b>Thu, 5 Feb 2009 16:22:50 +0000</b>
</p>

<p>
On Thursday 05 February 2009 14:50:01 Ben Okopnik wrote:
</p>

<pre>
&gt; On a slightly different topic, given Suramya's experience (i.e., "wget"
&gt; still retrieves the '-R' excluded file but deletes it afterwards), it
&gt; would make sense to file a bug report with the 'wget' maintainers. That
&gt; method definitely fails the "least surprise" test.
</pre>

<p>
This could have unfortunate consequences. 
</p>

<p>
Consider somebody trying to exclude large files, e.g. .iso files, because they 
have limited bandwidth and/or per MB bandwidth charges.
</p>

<p>
Neil
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]

</p>
</b><br />
<b>Thu, 5 Feb 2009 11:28:55 -0500</b>
</p>

<p>
On Thu, Feb 05, 2009 at 04:13:10PM +0000, Francis Daly wrote:
</p>

<pre>
&gt; On Thu, Feb 05, 2009 at 09:50:01AM -0500, Ben Okopnik wrote:
&gt; &gt; On Thu, Feb 05, 2009 at 01:56:08PM +0000, Francis Daly wrote:
&gt; 
&gt; Hi there,
&gt; 
&gt; &gt; &gt; (The short version is: wget considers the url to be
&gt; &gt; &gt; 
&gt; &gt; &gt; scheme://host/directory/file?query#fragment
&gt; &gt; &gt; 
&gt; &gt; &gt; -X filters on "directory", -R filters on "file", nothing filters on
&gt; &gt; &gt; "query", which is where you need it.)
&gt; &gt; 
&gt; &gt; '-R' definitely filters on both "file" and "query". 
&gt; 
&gt; No, it doesn't.
&gt; 
&gt; Or at least: it doesn't before deciding whether or not to get the url.
</pre>

<p>
I think we can agree that 1) "wget" <em>does</em> apply the filter to the
'file' and 'query' parts of the URL as evidenced by the results - but 2)
does the wrong thing when processing those filter results.
</p>

<p>
It "doesn't", in that it fails to stop the retrieval. It "does", in that
the retrieved file is not present on your system after "wget" is done.
The real answer here is that the method is broken; "does" and "doesn't"
are not nuanced enough to fully describe the actual problem.
 
</p>

<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]

</p>
</b><br />
<b>Thu, 5 Feb 2009 11:32:23 -0500</b>
</p>

<p>
On Thu, Feb 05, 2009 at 04:22:50PM +0000, Neil Youngman wrote:
</p>

<pre>
&gt; On Thursday 05 February 2009 14:50:01 Ben Okopnik wrote:
&gt; &gt; On a slightly different topic, given Suramya's experience (i.e., "wget"
&gt; &gt; still retrieves the '-R' excluded file but deletes it afterwards), it
&gt; &gt; would make sense to file a bug report with the 'wget' maintainers. That
&gt; &gt; method definitely fails the "least surprise" test.
&gt; 
&gt; This could have unfortunate consequences. 
</pre>

<p>
I assume you mean "this bug", not "filing this bug report". <img src="../gx/smile.png" alt=":)">
 
</p>

<pre>
&gt; Consider somebody trying to exclude large files, e.g. .iso files, because they 
&gt; have limited bandwidth and/or per MB bandwidth charges.
</pre>

<p>
Good point!
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Francis Daly [francis at daoine.org]

</p>
</b><br />
<b>Thu, 5 Feb 2009 17:19:58 +0000</b>
</p>

<p>
On Thu, Feb 05, 2009 at 11:28:55AM -0500, Ben Okopnik wrote:
</p>

<pre>
&gt; On Thu, Feb 05, 2009 at 04:13:10PM +0000, Francis Daly wrote:
&gt; &gt; On Thu, Feb 05, 2009 at 09:50:01AM -0500, Ben Okopnik wrote:
&gt; &gt; &gt; On Thu, Feb 05, 2009 at 01:56:08PM +0000, Francis Daly wrote:
</pre>

<p>
Hi there,
</p>


<pre>
&gt; &gt; &gt; &gt; (The short version is: wget considers the url to be
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; scheme://host/directory/file?query#fragment
&gt; &gt; &gt; &gt; 
&gt; &gt; &gt; &gt; -X filters on "directory", -R filters on "file", nothing filters on
&gt; &gt; &gt; &gt; "query", which is where you need it.)
&gt; &gt; &gt; 
&gt; &gt; &gt; '-R' definitely filters on both "file" and "query". 
&gt; &gt; 
&gt; &gt; No, it doesn't.
&gt; &gt; 
&gt; &gt; Or at least: it doesn't before deciding whether or not to get the url.
&gt; 
&gt; I think we can agree that 1) "wget" <em>does</em> apply the filter to the
&gt; 'file' and 'query' parts of the URL as evidenced by the results - but 2)
&gt; does the wrong thing when processing those filter results.
</pre>

<p>
Yes to 1); maybe to 2).
</p>

<p>
I think we're actually testing two different things. The original case was
"fetch this url, and get everything in it recursively". In that case,
-X and -R <strong>should</strong> work the way they are supposed to, but the manual
does say that HTML files will be fetched anyway. (It doesn't
immediately-obviously say what a HTML file is, though.)
</p>

<p>
This case is "here is a list of urls; please fetch them". It is not
unreasonable for wget to believe that request, and possibly apply
the -R/-X things to things fetched recursively from them (or patch up
afterwards, as it appears to do).
</p>

<p>
But the possibilities are numerous, and the "right" behaviour is unclear
to me, so I'll leave it at that and read the fine manual later.
</p>

<p>
All the best,
</p>

<p>
	f
<pre>-- 
Francis Daly        francis@daoine.org
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Francis Daly [francis at daoine.org]

</p>
</b><br />
<b>Thu, 5 Feb 2009 17:31:31 +0000</b>
</p>

<p>
On Thu, Feb 05, 2009 at 04:22:50PM +0000, Neil Youngman wrote:
</p>


<pre>
&gt; This could have unfortunate consequences. 
&gt; 
&gt; Consider somebody trying to exclude large files, e.g. .iso files, because they 
&gt; have limited bandwidth and/or per MB bandwidth charges.
</pre>

<p>
I suspect that it does work fine for someone who says "-R iso" when
recursively fetching a different url (or else surely someone would
have noticed!).
</p>

<p>
I suspect that it may not work as maybe-hoped if you try
</p>

<pre>
  wget -R iso <a href='http://example.com/that.iso'>http://example.com/that.iso</a>
</pre>

<p>
The obvious easy "fix" is "don't do that then".
</p>

<p>
	f
<pre>-- 
Francis Daly        francis@daoine.org
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]

</p>
</b><br />
<b>Thu, 5 Feb 2009 15:10:18 -0500</b>
</p>

<p>
On Thu, Feb 05, 2009 at 05:31:31PM +0000, Francis Daly wrote:
</p>

<pre>
&gt; On Thu, Feb 05, 2009 at 04:22:50PM +0000, Neil Youngman wrote:
&gt; 
&gt; &gt; This could have unfortunate consequences. 
&gt; &gt; 
&gt; &gt; Consider somebody trying to exclude large files, e.g. .iso files, because they 
&gt; &gt; have limited bandwidth and/or per MB bandwidth charges.
&gt; 
&gt; I suspect that it does work fine for someone who says "-R iso" when
&gt; recursively fetching a different url (or else surely someone would
&gt; have noticed!).
</pre>

<p>
Francis, you've lost me completely. The behavior of "wget -rR &lt;pattern&gt;"
is to do exactly as Neil describes (i.e., if "wget" was asked to ignore
some file in a recursive download, it would first fetch it, then delete
it.) Why would you expect it not to do that when that's exactly the
problem that was demonstrated here? 
</p>

<p>
<pre class='code'>
ben@Tyr:/tmp/test_wget$ mkdir /var/www/test/abc
ben@Tyr:/tmp/test_wget$ cd $_
ben@Tyr:/var/www/test/abc$ head -c 100M /dev/full &gt; large.iso
ben@Tyr:/var/www/test/abc$ head -c 10M /dev/full &gt; medium.iso
ben@Tyr:/var/www/test/abc$ head -c 1M /dev/full &gt; small.iso
ben@Tyr:/var/www/test/abc$ cd /tmp/test_wget/
ben@Tyr:/tmp/test_wget$ wget -nd -rR 'large*' <a href='http://Tyr/test/abc/{large,medium,small}.iso'>http://Tyr/test/abc/{large,medium,small}.iso</a>
--15:04:12--  <a href='http://tyr/test/abc/large.iso'>http://tyr/test/abc/large.iso</a>
           =&gt; `large.iso'
Resolving tyr... 127.0.0.1
Connecting to tyr|127.0.0.1|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 104,857,600 (100M) [application/x-iso9660-image]
 
100%[=====================================================================================&gt;] 104,857,600    6.86M/s    ETA 00:00
 
15:04:26 (7.58 MB/s) - `large.iso' saved [104857600/104857600]
 
Removing large.iso since it should be rejected.
--15:04:26--  <a href='http://tyr/test/abc/medium.iso'>http://tyr/test/abc/medium.iso</a>
           =&gt; `medium.iso'
 
[ snipped ]
 
FINISHED --15:04:28--
Downloaded: 116,391,936 bytes in 3 files
ben@Tyr:/tmp/test_wget$ ls
medium.iso  small.iso
</pre>

<p>
The "Removing large.iso since it should be rejected" line makes it
pretty obvious: "wget" does the wrong thing in recursive fetches when
asked to reject a file.
 
</p>

<pre>
&gt; I suspect that it may not work as maybe-hoped if you try
&gt; 
&gt;   wget -R iso <a href='http://example.com/that.iso'>http://example.com/that.iso</a>
&gt; 
&gt; The obvious easy "fix" is "don't do that then".
</pre>

<p>
'-R' only works on recursive fetches anyway.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Francis Daly [francis at daoine.org]

</p>
</b><br />
<b>Thu, 5 Feb 2009 21:15:41 +0000</b>
</p>

<p>
On Thu, Feb 05, 2009 at 03:10:18PM -0500, Ben Okopnik wrote:
</p>

<pre>
&gt; On Thu, Feb 05, 2009 at 05:31:31PM +0000, Francis Daly wrote:
&gt; &gt; On Thu, Feb 05, 2009 at 04:22:50PM +0000, Neil Youngman wrote:
</pre>


<pre>
&gt; &gt; I suspect that it does work fine for someone who says "-R iso" when
&gt; &gt; recursively fetching a different url (or else surely someone would
&gt; &gt; have noticed!).
&gt; 
&gt; Francis, you've lost me completely. The behavior of "wget -rR &lt;pattern&gt;"
&gt; is to do exactly as Neil describes (i.e., if "wget" was asked to ignore
&gt; some file in a recursive download, it would first fetch it, then delete
&gt; it.) Why would you expect it not to do that when that's exactly the
&gt; problem that was demonstrated here? 
</pre>

<p>
I guess that in your examples, I'm not seeing a recursive download.
</p>

<p>
Just sticking in "-r" doesn't make it recurse -- unless the url content
is html and contains links through which wget can recurse.
</p>

<p>
For example...
</p>


<pre>
&gt; ben@Tyr:/tmp/test_wget$ mkdir /var/www/test/abc
&gt; ben@Tyr:/tmp/test_wget$ cd $_
&gt; ben@Tyr:/var/www/test/abc$ head -c 100M /dev/full &gt; large.iso
&gt; ben@Tyr:/var/www/test/abc$ head -c 10M /dev/full &gt; medium.iso
&gt; ben@Tyr:/var/www/test/abc$ head -c 1M /dev/full &gt; small.iso
&gt; ben@Tyr:/var/www/test/abc$ cd /tmp/test_wget/
</pre>

<p>
Here, try
</p>

<pre>
 wget -np -nd -rR 'large*' <a href='http://Tyr/test/abc/'>http://Tyr/test/abc/</a>
</pre>

<p>
(assuming you have directory indexing enabled, so that the content of
that url includes some "a href" links to the .iso files).
</p>

<p>
(-np means "don't include the parent directory", which is worth including
here.)
</p>

<p>
Now wget will fetch the url given on the command line, will check it for
links to which to recurse, will find three, will discard that one where
the filename matches the pattern given, and will only get the other two.
</p>

<p>
(And if you have a typical apache setup, will also save a bunch of files
with names like index.html?C=N;O=D)
</p>


<pre>
&gt; ben@Tyr:/tmp/test_wget$ wget -nd -rR 'large*' <a href='http://Tyr/test/abc/{large,medium,small}.iso'>http://Tyr/test/abc/{large,medium,small}.iso</a>
&gt; --15:04:12--  <a href='http://tyr/test/abc/large.iso'>http://tyr/test/abc/large.iso</a>
</pre>

<p>
The request to wget is "get these three urls, and don't get ones with a
filename like large". It gets the three urls. It then deletes the large
one. Which, since it had explicitly been told to get, I'm not sure is
ideal. But is not unreasonable.
</p>

<p>
What my request above is is "get this url, and get everything it links
to. But don't get ones with a filename like large". And it gets the url,
identifies the links, discards the ones that match the pattern (unless
they also look like html-containing urls), and only fetches the others.
</p>


<pre>
&gt; The "Removing large.iso since it should be rejected" line makes it
&gt; pretty obvious: "wget" does the wrong thing in recursive fetches when
&gt; asked to reject a file.
</pre>

<p>
I think this case is user error. If you don't want wget to fetch
large* files, don't explicitly tell it to fetch large.iso.
</p>

<p>
I'm happy that wget does the right thing in recursive fetches when asked
to reject a file extension or pattern. To me, the later "get this explicit
url" overrides the previous "don't get ones that match this"
instruction. Any recursively-sought urls that match large* are correctly
not requested.
</p>

<p>
I'm also happy to accept that the -R option only refers to the
filename, not the query string, in recursive queries. The behaviour for
explicitly-named urls is a bit confusing, as it looks like it deletes
based on the saved filename.
</p>

<p>
But having looked through the info pages, the observed behaviour matches
my (current) expectations.
</p>

<p>
	f
<pre>-- 
Francis Daly        francis@daoine.org
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Predrag Ivanovic [predivan at nadlanu.com]

</p>
</b><br />
<b>Thu, 5 Feb 2009 22:48:29 +0100</b>
</p>

<p>
On Wed, 04 Feb 2009 19:38:11 +0530
Suramya Tomar wrote:
</p>


<pre>
&gt;Hi Ben,
&gt;
&gt;&gt;&gt; So I need to figure out how to exclude the Logout link from the process. 
&gt;&gt;&gt; The logout link looks like: www.website.com/index.php?act=Login&amp;CODE=03
</pre>


<pre>
&gt;
&gt;Is there any other way to do this? Maybe some other tool?
&gt;
&gt;- Suramya
</pre>

<p>
Have you tried Pavuk, <a href='http://pavuk.sourceforge.net/'>http://pavuk.sourceforge.net/</a>?
</p>

<p>
 From its man page (<a href='http://pavuk.sourceforge.net/man.html'>http://pavuk.sourceforge.net/man.html</a>)
</p>

<pre>
" You can use regular expressions to help pavuk select and filter content[...]"
</pre>

<p>
HTH
</p>

<p>
Pedja
<pre>-- 
 not approved by the FCC
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]

</p>
</b><br />
<b>Fri, 6 Feb 2009 14:19:45 -0500</b>
</p>

<p>
On Thu, Feb 05, 2009 at 09:15:41PM +0000, Francis Daly wrote:
</p>

<pre>
&gt; On Thu, Feb 05, 2009 at 03:10:18PM -0500, Ben Okopnik wrote:
&gt; &gt; On Thu, Feb 05, 2009 at 05:31:31PM +0000, Francis Daly wrote:
&gt; &gt; &gt; On Thu, Feb 05, 2009 at 04:22:50PM +0000, Neil Youngman wrote:
&gt; 
&gt; &gt; &gt; I suspect that it does work fine for someone who says "-R iso" when
&gt; &gt; &gt; recursively fetching a different url (or else surely someone would
&gt; &gt; &gt; have noticed!).
&gt; &gt; 
&gt; &gt; Francis, you've lost me completely. The behavior of "wget -rR &lt;pattern&gt;"
&gt; &gt; is to do exactly as Neil describes (i.e., if "wget" was asked to ignore
&gt; &gt; some file in a recursive download, it would first fetch it, then delete
&gt; &gt; it.) Why would you expect it not to do that when that's exactly the
&gt; &gt; problem that was demonstrated here? 
&gt; 
&gt; I guess that in your examples, I'm not seeing a recursive download.
</pre>

<p>
"wget" certainly sees it as a request for a recursive download; '-R'
does nothing without that '-r'. <strong>That</strong> behavior, as specified, is broken
- and whether it's possible to make it succeed in some <em>other</em> type of
recursive retrieval isn't the issue: Suramya's problem comes from
exactly this misbehavior of "wget".
</p>


<pre>
&gt; Just sticking in "-r" doesn't make it recurse -- unless the url content
&gt; is html and contains links through which wget can recurse.
</pre>

<p>
Assuming that this is right, how would that help? In Suramya's case, 
the "Logout" link <em>is</em> a link - and "wget" does try to traverse it.
The fact that it will remove the results from its blacklist later
doesn't prevent the problem from happening.
</p>


<pre>
&gt; Now wget will fetch the url given on the command line, will check it for
&gt; links to which to recurse, will find three, will discard that one where
&gt; the filename matches the pattern given, and will only get the other two.
</pre>

<p>
In the scenario you proposed, you're right - but it doesn't help
Suramya's problem, as you saw. It still traverses the link, and only
then removes the bits from its blacklist. Which brings us back to the
original problem.
 
</p>

<pre>
&gt; What my request above is is "get this url, and get everything it links
&gt; to. But don't get ones with a filename like large". And it gets the url,
&gt; identifies the links, discards the ones that match the pattern
&gt; (unless they also look like html-containing urls), and only fetches the others.
</pre>
<pre>
   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
</pre>
And <em>that</em> is the problem in a nutshell.
 
</p>

<pre>
&gt; [...] having looked through the info pages, the observed behaviour matches
&gt; my (current) expectations.
</pre>

<p>
Perhaps your reading comprehension is much higher than mine, but I
didn't find either the man page or the info pages at all informative on
this point. In addition to that, and far more damning, is the counter-
intuitive nature of the '-R' operation: it doesn't do the obvious, the
least-surprising thing.
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
clarjon1 [clarjon1 at gmail.com]

</p>
</b><br />
<b>Fri, 6 Feb 2009 14:32:38 -0500</b>
</p>

<p>
On Wed, Feb 4, 2009 at 8:54 AM, Ben Okopnik &lt;ben@linuxgazette.net&gt; wrote:
</p>

<pre>
&gt; On Wed, Feb 04, 2009 at 06:36:50PM +0530, Suramya Tomar wrote:
&gt;&gt;
&gt;&gt; When I start downloading wget visits each and every link and makes a
&gt;&gt; local copy (like its supposed to) but in this process it also visits the
&gt;&gt; "Log out" link which logs me out from the site and then I am unable to
&gt;&gt; download the remaining links.
&gt;&gt;
&gt;&gt; So I need to figure out how to exclude the Logout link from the process.
&gt;&gt; The logout link looks like: www.website.com/index.php?act=Login&amp;CODE=03
</pre>

<p>
-SNIP-
</p>

<p>
Oh, i actually know the answer to this one!
I just had this exact same problem, to be honest.
</p>

<p>
Heres what you do:
</p>

<p>
You export the cookie from the site, stick it in a cookies .txt, and
have it load the cookies from there.
</p>

<p>
Then, you monitor the site's download progress in the terminal, and
wait until you see it hit the logout page. Once that's done, hit
ctrl-z to pause, then login in your browser again.
</p>

<p>
Once you're logged in, go back to the terminal window, and type "fg"
to continue wg.
</p>

<p>
This took me a couple of hours to figure out, but once i figured it
out, boy, was i happy!
</p>

<p>
I hope this helps!
</p>

<pre>-- 
Jon
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
clarjon1 [clarjon1 at gmail.com]

</p>
</b><br />
<b>Fri, 6 Feb 2009 14:34:40 -0500</b>
</p>

<p>
On Fri, Feb 6, 2009 at 2:32 PM, clarjon1 &lt;clarjon1@gmail.com&gt; wrote:
</p>

<pre>
&gt; On Wed, Feb 4, 2009 at 8:54 AM, Ben Okopnik &lt;ben@linuxgazette.net&gt; wrote:
&gt;&gt; On Wed, Feb 04, 2009 at 06:36:50PM +0530, Suramya Tomar wrote:
&gt;&gt;&gt;
&gt;&gt;&gt; When I start downloading wget visits each and every link and makes a
&gt;&gt;&gt; local copy (like its supposed to) but in this process it also visits the
&gt;&gt;&gt; "Log out" link which logs me out from the site and then I am unable to
&gt;&gt;&gt; download the remaining links.
&gt;&gt;&gt;
&gt;&gt;&gt; So I need to figure out how to exclude the Logout link from the process.
&gt;&gt;&gt; The logout link looks like: www.website.com/index.php?act=Login&amp;CODE=03
&gt; -SNIP-
&gt;
&gt; Oh, i actually know the answer to this one!
&gt; I just had this exact same problem, to be honest.
&gt;
&gt; Heres what you do:
&gt; You export the cookie from the site, stick it in a cookies .txt, and
&gt; have it load the cookies from there.
&gt; Then, you monitor the site's download progress in the terminal, and
&gt; wait until you see it hit the logout page. Once that's done, hit
&gt; ctrl-z to pause, then login in your browser again.
&gt; Once you're logged in, go back to the terminal window, and type "fg"
&gt; to continue wg.
&gt;
&gt; This took me a couple of hours to figure out, but once i figured it
&gt; out, boy, was i happy!
&gt;
&gt; I hope this helps!
</pre>

<p>
This is assuming, of course, that the PHPSESSION variable doesn't
change in the cookie... else you may have problems again... I luckily
didn't run into that, but it is a thing to keep in mind.
</p>

<pre>-- 
Jon
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Ben Okopnik [ben at linuxgazette.net]

</p>
</b><br />
<b>Fri, 6 Feb 2009 19:05:27 -0500</b>
</p>

<p>
On Fri, Feb 06, 2009 at 02:32:38PM -0500, clarjon1 wrote:
</p>

<pre>
&gt; On Wed, Feb 4, 2009 at 8:54 AM, Ben Okopnik &lt;ben@linuxgazette.net&gt; wrote:
&gt; &gt; On Wed, Feb 04, 2009 at 06:36:50PM +0530, Suramya Tomar wrote:
&gt; &gt;&gt;
&gt; &gt;&gt; When I start downloading wget visits each and every link and makes a
&gt; &gt;&gt; local copy (like its supposed to) but in this process it also visits the
&gt; &gt;&gt; "Log out" link which logs me out from the site and then I am unable to
&gt; &gt;&gt; download the remaining links.
&gt; &gt;&gt;
&gt; &gt;&gt; So I need to figure out how to exclude the Logout link from the process.
&gt; &gt;&gt; The logout link looks like: www.website.com/index.php?act=Login&amp;CODE=03
&gt; -SNIP-
&gt; 
&gt; Oh, i actually know the answer to this one!
&gt; I just had this exact same problem, to be honest.
&gt; 
&gt; Heres what you do:
&gt; You export the cookie from the site, stick it in a cookies .txt, and
&gt; have it load the cookies from there.
&gt; Then, you monitor the site's download progress in the terminal, and
&gt; wait until you see it hit the logout page. Once that's done, hit
&gt; ctrl-z to pause, then login in your browser again.
&gt; Once you're logged in, go back to the terminal window, and type "fg"
&gt; to continue wg.
&gt; 
&gt; This took me a couple of hours to figure out, but once i figured it
&gt; out, boy, was i happy!
</pre>

<p>
Eeep. I'm imagining a site with several thousand pages, each of which
has a logout link...
</p>


<pre>-- 
* Ben Okopnik * Editor-in-Chief, Linux Gazette * <a href='http://LinuxGazette.NET'>http://LinuxGazette.NET</a> *
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
Francis Daly [francis at daoine.org]

</p>
</b><br />
<b>Sat, 7 Feb 2009 02:13:07 +0000</b>
</p>

<p>
On Fri, Feb 06, 2009 at 02:19:45PM -0500, Ben Okopnik wrote:
</p>

<pre>
&gt; On Thu, Feb 05, 2009 at 09:15:41PM +0000, Francis Daly wrote:
</pre>

<p>
Hi there,
</p>


<pre>
&gt; &gt; I guess that in your examples, I'm not seeing a recursive download.
&gt; 
&gt; "wget" certainly sees it as a request for a recursive download; '-R'
&gt; does nothing without that '-r'. <strong>That</strong> behavior, as specified, is broken
</pre>

<p>
I suspect I'm interpreting the terms differently to you.
</p>

<p>
To me "-r" means "get these starting urls. Then get the links in the
html content fetched so far". "-R" applies to the links, not to the
starting urls[*]. "-R" decides whether the link will not be followed
(unless it looks like it will be html, explained below).
</p>

<pre>
  * the tests here show that "-R" <strong>does</strong> determine which saved files of
  the starting urls are deleted after having been got. It's not obvious
  to me that deleting them is right, but it seems justifiable.
</pre>


<pre>
&gt; - and whether it's possible to make it succeed in some <em>other</em> type of
&gt; recursive retrieval isn't the issue: Suramya's problem comes from
&gt; exactly this misbehavior of "wget".
</pre>

<p>
Suramya's problem is in the recursion -- in following the links, not in
getting the starting url.
</p>

<p>
The original problem is "mirror this website", which can be considered
to be
</p>

<pre>
  wget -r /index.php
</pre>

<p>
The content at /index.php includes links of the
form /index.php?do_get_this and /index.php?logout and
/index.php?do_get_this_too. The hope is to be able to invite wget not to
follow the ?logout link, but to follow all of the others.
</p>

<p>
Everything I see says that you can't do that with wget, because "-R"
does not apply to the query string, and so there is no "-R" argument
that can differentiate between the three included links.
</p>


<pre>
&gt; &gt; Just sticking in "-r" doesn't make it recurse -- unless the url content
&gt; &gt; is html and contains links through which wget can recurse.
&gt; 
&gt; Assuming that this is right, how would that help? In Suramya's case, 
&gt; the "Logout" link <em>is</em> a link - and "wget" does try to traverse it.
</pre>

<p>
wget traverses it because it is a link and "-R" cannot prevent it from
being traversed, because "-R" only considers the "index.php" part of
the url.
</p>

<p>
There is no wget argument that filters on the query string when deciding
what links to follow. Which is the quick answer to the original problem.
</p>

<p>
[ pavuk was mentioned elsewhere in the thread. That does include
skip_url_pattern and skip_url_rpattern options which are documented to
include the query string when matching ]
</p>


<pre>
&gt; &gt; What my request above is is "get this url, and get everything it links
&gt; &gt; to. But don't get ones with a filename like large". And it gets the url,
&gt; &gt; identifies the links, discards the ones that match the pattern
&gt; &gt; (unless they also look like html-containing urls), and only fetches the others.
&gt;    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
&gt; 
&gt; And <em>that</em> is the problem in a nutshell.
</pre>

<p>
Actually, no.
</p>

<p>
To wget, that line just means that if the filename suffix in the url is
"htm", "html", or "Xhtml" (for any single X), then the link will be
followed irrespective of any -R argument.
</p>

<p>
Otherwise, "-R" links are not followed, whatever the content-type of
the response might be.
</p>


<pre>
&gt; &gt; [...] having looked through the info pages, the observed behaviour matches
&gt; &gt; my (current) expectations.
&gt; 
&gt; Perhaps your reading comprehension is much higher than mine, but I
&gt; didn't find either the man page or the info pages at all informative on
&gt; this point. In addition to that, and far more damning, is the counter-
&gt; intuitive nature of the '-R' operation: it doesn't do the obvious, the
&gt; least-surprising thing.
</pre>

<p>
It makes sense to me, once I accept that "-R" applies to links within
content, and not to starting urls. I'm not sure that deleting some saved
files by default is great behaviour, but that's a separate issue.
</p>

<p>
All the best,
</p>

<p>
	f
<pre>-- 
Francis Daly        francis@daoine.org
</pre>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p><p>
<b><p>
clarjon1 [clarjon1 at gmail.com]

</p>
</b><br />
<b>Sat, 7 Feb 2009 09:45:36 -0500</b>
</p>

<p>
On Fri, Feb 6, 2009 at 7:05 PM, Ben Okopnik &lt;ben@linuxgazette.net&gt; wrote:
</p>

<pre>
&gt; On Fri, Feb 06, 2009 at 02:32:38PM -0500, clarjon1 wrote:
&gt;&gt; On Wed, Feb 4, 2009 at 8:54 AM, Ben Okopnik &lt;ben@linuxgazette.net&gt; wrote:
&gt;&gt; &gt; On Wed, Feb 04, 2009 at 06:36:50PM +0530, Suramya Tomar wrote:
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; When I start downloading wget visits each and every link and makes a
&gt;&gt; &gt;&gt; local copy (like its supposed to) but in this process it also visits the
&gt;&gt; &gt;&gt; "Log out" link which logs me out from the site and then I am unable to
&gt;&gt; &gt;&gt; download the remaining links.
&gt;&gt; &gt;&gt;
&gt;&gt; &gt;&gt; So I need to figure out how to exclude the Logout link from the process.
&gt;&gt; &gt;&gt; The logout link looks like: www.website.com/index.php?act=Login&amp;CODE=03
&gt;&gt; -SNIP-
&gt;&gt;
&gt;&gt; Oh, i actually know the answer to this one!
&gt;&gt; I just had this exact same problem, to be honest.
&gt;&gt;
&gt;&gt; Heres what you do:
&gt;&gt; You export the cookie from the site, stick it in a cookies .txt, and
&gt;&gt; have it load the cookies from there.
&gt;&gt; Then, you monitor the site's download progress in the terminal, and
&gt;&gt; wait until you see it hit the logout page. Once that's done, hit
&gt;&gt; ctrl-z to pause, then login in your browser again.
&gt;&gt; Once you're logged in, go back to the terminal window, and type "fg"
&gt;&gt; to continue wg.
&gt;&gt;
&gt;&gt; This took me a couple of hours to figure out, but once i figured it
&gt;&gt; out, boy, was i happy!
&gt;
&gt; Eeep. I'm imagining a site with several thousand pages, each of which
&gt; has a logout link...
</pre>


<p>
Actually, that's not how wget works
</p>

<p>
It notes that it had already downloaded the logout link earlier, so
it'll ignore it after it downloads it the first time.
</p>

<p>

</p>
<br /><a href="#top">Top</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="../../lg_mail.html#mb-how_to_make_wget_exclude_a_particular_link_when_mirroring">Back</a><hr width="50%" align="left" /><p><br /></p></div>
</body>
</html>